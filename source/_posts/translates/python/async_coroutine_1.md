---
title: 비동기 코루틴 번역-1
p: translates/python/async_coroutine_1
date: 2019-06-19 18:11:02
tags: ['python', 'async', 'coroutine', 'translates']
---

> 이 글은 너무 유명한 글이라 출처 링크는 생략합니다.

# A Web crawler with ayncio Coroutines
## A.Jesse Jiryu and guido van Rossum

### intoduction
고전적인 컴퓨터 과학은 강조한다 효율적인 알고리즘을, 컴퓨테이션을 가능한 빠르게 수행하는 알고리즘.
그러나 많은 네트워크 프로그램은 그들의 시간이 아니라, 많은 연결을 유지하는 곳에, 그것은 느리거나 드문 이벤트를 지닌다.
이 프로그램들은 많은 다른 도전을 과시한다.: 많은 거대한 네트워크 이벤트들을 효과적으로 기다리기를.
이 문제에 대한 동시대의 접근은 비동기적 I/O 혹은 Async이다.

이 챕터는 간단한 웹크롤러를 보여준다. 크롤러는 전형적인 비동기 어플리케이션이다. 왜냐하면 이것은 많은 응답들을 기다리고 있기 떄문이다. 
그러나 작은 컴퓨테이션을 수행한다. 많은 페이지를 한번에 송수신 한다면, 더 빠르게 이것은 종료된다.
이것이 만약 쓰레드를 진행중인 각각의 리퀘스트에 힘을 쏟는다면, 그러면 동시성 리퀘스트 수가 느는 것 만큼
이것은, 메모리가 부족해질것 이거나, 다른 쓰레드관련 리소스를 소모하게 될 것이다, 이것이 소켓을 다 소모하기 전에.
이것은 쓰레드에 대한 필요성을 피하게 해준다. 비동기 IO를 사용함으로써.

3가지 스테이지로 나눠서 예를 보여준다.
첫째 스테이지: 우리는 비동기 이벤트루프를 보여주고, 콜백을 사용하는 이벤트루프를 이용하는 크롤러를 그린다.
이것은 매우 효과적일 수 있다. 
그러나 이것을 더 복잡한 문제로 끌고 갈수록 그것은 관리할 수 없는 스파게티코드로 인도할 것이다.

둘째 스테이지: 그러므로 우리는 파이썬 코루팅들이 효과적이면서 동시에 확장가능하다는 것을 보여줄 것이다. 우리는 간단한 코루틴을 실행할것이다. 파이썬에서 제네레이터를 사용함으로써.

셋째 스테이지: 우리는 최대로 능력을 끌어올린 파이썬 코루틴을 사용할 것이다. 파이썬의 표준 "asyncIO"라이브러리, 그리고 그들을 async queue를 이용해 조직화 한다.

### The Task
웹 크롤러는 모든 페이지를 찾고 다운로드한다, 비록 그들을 기록하거나, 번호를 메기려 할 목적이라도 말이다. 루트 URL로 시작해서, 모든 페이지를 fetch하고, 그것을 보여지지 않은 페이지들의 링크로 파싱한다. 그리고 그들을 queue에 올린다. 이것은 모든페이지에 대해서 Fetch가 완료하고, queue가 비었을 때 멈출 것이다.

우리는 이 프로세스를 많은 페이지를 동시적으로 다운로드 함으로써, 가속할 수 있다. 크롤러가 새로운 링크를 찾으면, 이것은 동시Fetch 명령을 실행한다.(개별소켓위의 새로운 페이지들을 얻어 오기 위한)
이것은 응답이 도착하는 데로 파싱하고, 새로운 링크를 큐에 올린다.
거기엔 그런 리턴을 절감하는 포인트들이 함께 올 수 있다.(어딘가 너무 많은 동시성이 퍼포먼스를 저하시키는 곳에)
그래서 우리는 동시성 리퀘스트의 수의 한도를 정한다. 그리고 큐에 남은 링크들을 그냥 둔다.
일부 진행중인 리퀘스트가 끝날 때까지.

### Traditional approach
고전적으로 우리는 쓰레드 풀을 생성해서 크롤러를 동시적으로 만들었다. 각 쓰레드는 소켓을 통해서 한번에 하나의 페이지를 다운받는 역할을 맡고 있었다. 예를 들어 다운로드 xkcd.com을 한다면:
```python
def fetch(url):
	sock = socket.socket()
	sock.connect(("xkcd.com",80))
	req = 'GET {} HTTP/1.0\r\nHost: xkcd.com/r/n/r/n'.format(url)
	sock.send(request.encode('ascii'))
	response = b''
	chunk = sock.recv(4096)
	while chunk:
		response += chunk
		chunk = scok.recv(4096)

	#Page is now downloaded.
	links = parse_links(response)
	q.add(links)
```
기본적으로 소켓의 수행은 블로킹이다. 쓰레드가 connect, recv등을 수행할때 그러하다. 이것은 수행이 끝날때까지 멈춰있다.
동시적으로 많은 페이지를 한번에 받기위해서, 우리는 많은 쓰레드가 필요하다. 정교한 어플리케이션은 쓰레드를 생성하는 비용을 쓰레드풀에 대기중인 쓰레드를 보관함으로써 분할처리한다. 그리고 그들을 동시적인 일에 재사용 할 수 있게 체크해나간다. 이것은 소켓의 커넥션풀과 동일한 것을 하는 것이다.

그러나 쓰레드는 비싸다, OS는 그들을 프로세스당 쓰레드의 수가 많아지는 것을 강력하게 제어한다. 프로세스나 유저, 혹은 머신이 가질 수 있는 쓰레드말이다.

제시의 시스템에서 파이썬 쓰레드는 50k정도 비용의 메모리를 소모한다. 천개중 열개의 쓰레드를 실행하는 것은 실패를 부른다. 그것을 동시적인 명령으로 동시성 소켓으로 확장한다면, 소켓이 다 떨어지기전에, 쓰레드가 다 떨어진다. 쓰레드당 오버헤드 혹은 시스템의 제한은 굉장히 목을 조른다.

영향력있는 기사인 The C10K problem - Dan kegel 에서는 IO동시성을위한 멀티쓰레딩의 제한을 그린다.
그는 이렇게 시작한다.

이제 웹서버가 만명의 클라이언트를 동시에 처리해야 한다. 그렇지 않은가? 어찌됐든, 웹은 이제 커다란 장소가 되었다.

1999년에 작성된 이것은 만개의 연결이라고하면 다소 앙증맞게 들린다. 그러나 그떄와 달라진건 규모일 뿐이지 같은 이야기를 우리는 하고 있다.
그때는 연결당 쓰레드를 생성하는 것은 실용적이지 않았다. 이제 OS의 쓰레드 생성제한수는 높아졌다.
게다가 우리의 장난감 웹 크롤러는 쓰레드만으로 잘 작동할 것이다. 그러나, 거대한 어플리케이션에서,
10만개의 연결과 함께하는 그런 어플리케이션에서는, 제한 수는 이것을 말한다.:
거기에는 제한수를 뛰어넘게 소켓을 생성할 수 있는 시스템이 많은데, 쓰레드는 부족하다. 이걸 우린 어떻게 극복할 것인가?
