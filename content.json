{"meta":{"title":"Junehan's workbook","subtitle":"work flow and descriptions","description":"with python and javascript, could be some descriptions or workflow writtens for project","author":"junehan","url":"https://codenamenadja.github.io","root":"/"},"pages":[],"posts":[{"title":"test_post","slug":"fastcamp/my_post","date":"2021-01-26T01:09:17.000Z","updated":"2021-01-26T01:14:52.644Z","comments":true,"path":"2021/01/26/fastcamp/my_post/","link":"","permalink":"https://codenamenadja.github.io/2021/01/26/fastcamp/my_post/","excerpt":"","text":"My Test Hexo 12345message = \"Hello WOrld!\"print(f\"&#123;message&#125;\") # print가 리턴하는 것이 무엇인가?exit()","categories":[],"tags":[{"name":"tutorial","slug":"tutorial","permalink":"https://codenamenadja.github.io/tags/tutorial/"},{"name":"hexo-deploy","slug":"hexo-deploy","permalink":"https://codenamenadja.github.io/tags/hexo-deploy/"}]},{"title":"http_1.0_sementics","slug":"http/2_http_sementics","date":"2019-09-17T14:09:02.000Z","updated":"2021-01-25T08:36:06.071Z","comments":true,"path":"2019/09/17/http/2_http_sementics/","link":"","permalink":"https://codenamenadja.github.io/2019/09/17/http/2_http_sementics/","excerpt":"","text":"index Simple Form Trasportation: 간단한 폼 전송 File trasnport with form: 폼을 통한 파일 전송 Simple_form_transportation 12345678910111213141516171819# clientcurl --http1.0 -d title=\"art of...\" -d author=\"jono\" http://127.0.0.1:18888/# serverContent-Length: 27Content-Type: application/x-www-form-urlencodedUser-Agent: curl/7.58.0title=art of...&amp;author=jono# httpclienthttp http://127.0.0.1:18888/ username=sample password=asdqrfrg -f# serverContent-Length: 33Content-Type: application/x-www-form-urlencoded; charset=utf-8User-Agent: HTTPie/0.9.8username=sample&amp;password=asdqrfrg 위 커맨드로 처리하는 바디는 브라우저의 웹폼과는 달리 지정된 문자열의 특수문자를 변환없이 그대로 연결한다. RFC 1866에서 책정한 변환 포멧에 따라, 알파벳 숫자 별표 하이픈 마침표 언더스코어 의 종류 외 문자는 변환이 되어 아래와 같은 형식이 된다. title=Head First PHP &amp; MySQL&amp;authre = Lynn Beighley, Micheal Morrison title=Head+First+PHP+%26+MySQL&amp;author=Lynn+Beighley%2C+Micheal+Morrison curl커맨드에서는 --data-urlencode옵션을 통해서 RFC 3986에 정의된 방법으로 변환한다. 이 경우에는 RFC 1866과 달리 공백이 +가 아닌 %20으로 처리한다. File_transport_WITH_form HTML Form에서 multipart라는 폼 형식 인코딩 타입을 통해서, 파일을 전송할 수 있다. 이는 RFC 1867에 정의된다. 12&lt;form action=\"POST\" enctype=\"mulitpart/form-data\"&gt;&lt;/form&gt; HTTP응답은 한번에 한 바디만 반환하므로, 빈줄로부터 Content-Length바이트 만큼 읽어내면 종료된다. 하지만 멀티파트를 통하는 경우 한번의 요청으로 복수의 파일의 경계를 받는쪽에서 처리해야 한다. Content-Type은 모두 multipart/form-data이지만, 또 하나의 속성인 경계 문자열이 있다. 각 따라 독자적인 포멧으로 랜덤하게 생성한다. 1234567891011Content-Type: multipart/form-data; boundary=---WebKitFormBoundaryy0YfbccgoID172j7------WebKitFormBoundaryy0YfbccgoID172j7Content-Disposition: form-data; name=\"title\"The Art of Community------WebKitFormBoundaryy0YfbccgoID172j7Content-Disposition: form-data; name=\"author\"Jono Bacon------WebKitFormBoundaryy0YfbccgoID172j7 1234567891011121314151617181920212223242526272829303132333435POST / HTTP/1.1Host: 127.0.0.1:18888Accept: */*Accept-Encoding: gzip, deflateCache-Control: no-cacheConnection: keep-aliveContent-Length: 2719Content-Type: multipart/form-data; boundary=--------------------------925161045727238542287839Postman-Token: cd5fc7b3-3090-4cc7-921e-223c3defc0a5User-Agent: PostmanRuntime/7.6.0----------------------------925161045727238542287839Content-Disposition: form-data; name=\"\"; filename=\"3_8_buffer_io.c\"Content-Type: text/x-c#include &lt;stdio.h&gt;int main(void)&#123; FILE *in, *out; struct pirate &#123; char name[100]; unsigned long booty; unsigned int beard_len; &#125;; struct pirate p; struct pirate blackbeard = &#123;\"Edward Teach\", 950, 48&#125;;&#125;----------------------------925161045727238542287839Content-Disposition: form-data; name=\"\"; filename=\"sample.txt\"Content-Type: application/javascriptsample file_2 content-data----------------------------925161045727238542287839-- curl에서는 -d 대신 -F를 사용하는 것만으로 enctype=&quot;multipart/form-data&quot;로 설정한다. curl --http1.0 -F &quot;attachment-file@test.txt;filename=sample.txt;type=text/html&quot; http://127.0.0.1:18888/ filename과 type은 메뉴얼하게 지정하며, 내용은 text.txt에서 취득한다. Content-Negotiation 컨텐츠 협상에 사용되는 대상과 헤더 Header Response Negotiation-Target Accept Content-Type 헤더 MIME 타입 Accept-Language Content-Language 헤더 / html 태그 표시 언어 Accept-Charset Content-Type 헤더 문자의 문자셋 Accept-Encoding Content-Encoding 헤더 바디 압축 파일_종류_결정 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8 image/webp */*;q=0.8 q는 우선순위르 나타내는 품질 계수로 0~1사이 수치로 표현되며, 기본값은 1.0으로 이때는 생략된다. 웹서버가 Webp(“구글이 권장하는 PNG보다 20%절약 포멧”)를 지원하면 webp를, 그렇지 않으면 PNG등 다른 포멧(q=0.8)을 서버에 보낼 것을 요구한다. 서버는 Request에서 요구한 형식중에서 파일을 반환. 우선 순위를 해석해, 위에서부터 차례로 지원하는 포멧을 찾고, 그 포멧을 반환한다. 만약 일치하는 형식이 없으면 서버가 406 NOT Acceptable을 반환한다. 표시_언어_결정 Accept-Language: en-US,en;q=0.8,ko;q=0.6 en-US en ko 순의 우선순위로 요청 헤더를 보낸다. 문자셋_결정 Accept-Charset: windows-949,utf-8;q=0.7,*;q=0.3 현재는 어떤 브라우저도 Accept-Charset헤더를 송신하지 않는다. 대부분 브라우저가 문자셋 인코더를 내장하고 있어, 대부분의 문자를 처리할 수 있기 때문이다. 문자셋은 MIME타입과 세트로 Content-Type헤더에 실려서 전달된다. Content-Type: text/html; charset=UTF-8 HTML의 경우 문서 안에 쓰기도 한다. RFC 1866의 HTML/2.0으로 이용할 수 있다. HTML을 로컬에 저장했다가 다시 표시하는 경우도 많으므로, 이용된다. 1&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"&gt; http-equiv태그는 HTTP헤더와 똑같은 지시를 문서 내부에 삽입해서 반환하는 상자. HTML5의 경우, 아래와 같이 표기 가능하다. 1&lt;meta charset=\"UTF-8\"&gt;","categories":[],"tags":[{"name":"http","slug":"http","permalink":"https://codenamenadja.github.io/tags/http/"}]},{"title":"travel_note 서비스 고찰","slug":"project/travel_note","date":"2019-09-09T06:10:16.000Z","updated":"2021-01-25T08:36:06.079Z","comments":true,"path":"2019/09/09/project/travel_note/","link":"","permalink":"https://codenamenadja.github.io/2019/09/09/project/travel_note/","excerpt":"","text":"index main mvp model MVP 여행후기를 공유한다 작성 시기 제주도 (지역, 인원, 필터링) 독자적인 SNS Follow. Follow한 유저가 구독 경험 로그인 지역선택(성향선택최초 1회) 관심사(후에 여행 태그 기반 추천 검색) 메인 페이지 (Favorite Places) 친구들 후기(New from friends) 예산에 맞는 추천 Popular 검색 필터 지역, 금액, 태그, 본문(키워드)&gt;타이틀(찰워드) 3:1 model location location.country id name travel_ids 1 USA 3,5,7[one to many(travels)] 2 France 1,2,4[one to many(travels)] location.spot id name country_id day_ids 1 effel_tower 2[related name] 유저 id email gender likes interests planings travels 1 sample@facebook.com True one to many(tags) one to many(location.country) one to many() 여행 태그 스팟(여정에 대한 기념, 상징적인 장소 개념, 날짜별로 4개정도는 가능하다.) 스팟은 사실 검색용도로 사용되고, 다른 의미는 없다? 금액 날짜, 분류, 디테일(숙박, 1일자, 무슨호텔/ 식음료, 1일자, 무슨음식) 총 교통수단에 대한 요금(Transportation) Accomodation(Hotel) Expenditure(일반 소비), 화폐에 대한 기준 달러화 Post(사진과 글) 마이페이지 팔로잉, 팔로워수 나의 여행들 고찰 190911 가입할때 interests로 등록하는 목록(activity, food, relax, tradition, shopping, festival) day.tags(어떤 날의 주요 액티비티 종류) travel.tag(여행의 주 목표) 지역에 대해서 검색할때, 기본적으로 travel이라는 패키지를 출력하지만, 유저의 취향에 맞춰서 키워드 기반으로 유저의 관심 안에 있는 활동을 한 날이 있다면? 특정 날만 별도 뷰로 전달해줄 필요가 있다. 애초에 매칭률이 높다면, 특정 키워드가 컨텐츠에 등장한다? 텍스트의 키워드 검색을 어플리케이션에 맞기고(정렬), tag가 어떤날에 부합하면서, 키워드가 등장하고, 내가 등록한 정보로 어떤 의미있는 데이터를 가져올 수 있을 것인가? 내 취향과 유사한 사람들이 어떤 선택을 하는지 참고하고 싶어한다, 나와 같은 성향. (1~2명이라면 신경쓰지 않지만, 나와 같은 성향으로 그나라에 대한 여행 정보를 등록해준 동성 100명의 통계가 이러한데 이렇다. 그리고 거기서 만족도가 높았다.) 왜 평가가 높았는지 정확한 디테일을 캐치하는 것은, 표본중에 랜덤하게 선택하여 유저가 직접 검토하는 것이 맞다. 평가가 높았던 날에 키워드에서 장소에 대한 키워드를 수집해서, 처리하는 것. 어느 스팟이 들어간 곳이 통계적으로 평가가 높았다. 통계라고 하면 100명이서 통합 4점과, 30명이서 4.3점이라면, 100명이서 4점을 더 좋게 하지만, 나와 같은 취향을 선별한다면, 좀 더 유저가 원하는 정보에 가까울 수… 나와 같은 성향. 만족도는 어떻게 체크 할 것인가? 사실 디테일하게 처리하기 위해선, 그 날짜에 대한 전반적인 점수가 높다라는 것으로? 만족도를 어떻게 체크해야 간편하면서, 구체적일 수 있을까? 1,2 평하 3 평중 4,5 평상 사람들이 통계적으로 봤을때, 1~5점으로 매긴다고 하면은, 정말 너무 행복했던 기억만을 가졌던 날을 누군가 있었다면, 정말 흔치않지만, 5점일 것이고 대게 좋았거나, 비교적 순탄했다면 4점을 매길것이다 새롭지 않고, 문제 없었다면 3점이고 기분 안 좋은 일이 있었다면, 1~2점 나와 같은 성향은 어떻게 진단 할 것인가? interests와 budgets이 어떤 나라에대해서 맞는 사람들이 다녀온 여행. budgets 항공료를 제외한, 남녀에 맞춰서. 통계정보를 통해서, 사용자가 어떤 선택을 긍정적으로 검토 할 수 있도록 유도하는 것. 190914 첫번쨰 즐거움은 작성하는 즐거움을, 자신이 작성한 것이, 한 눈에 보기 쉽고 작성하기 쉬우면서 작성할때 너무 어렵지 않게, 언제든 세부적으로 검토하는 것도 가능하지만, 모드조정을 통해 간략한 글쓰기, 세부적으로 글쓰기 간단한 여행일지 쓰기.(1장의 사진, 1~2줄의 글) 사진 하나, 타임라인, 엽서에 사진과 쪽지가 붙어있는. 그것을 다시 봤을떄, 누구나 이해할 수 있도록 돌려주려면? 엽서들을 어떤식으로? 여행 중에는 간단히 사진한장을 찍으면, 그곳의 위치와 지역시간을 기록하고. 간단한 메세지(노천카페에서 누구와) 그것들을 모아서 스팟에 등록하면, 시간 별로 스팟에 대한 시퀸스를 만들어준다. 지역이 이전 사진지역과 500m근방이거나, 30분 이하의 기록이라면 자동으로 하나의 스팟으로 자동 처리해주고(예를 들어 길거리 여러 상점을 들린다거나?) 그날 여행의 종료했을떄, 스팟의 최초 등록된 포스트의 위치(~거리 단위로)가제목을 붙여주고, 실제목은 따로 없으나, 유저가 직접 수정해서 스팟내의(시퀸셜한 여정(여행중에는 20분에 한장정도는 자주 찍는 편이라고 한다면)) 그것을 파일로 처리해서 인쇄물로 받아볼 수 있도록. 물리적인 기록물과 연결된 것으로 처리하고, 디지털정보도 같이 보관할 수 있도록 그래도 3~5시간 내로는 보통 스팟을 벗어난다고 가정할 수 있다. 최초 스팟에서 1km이상 벗어났다면, 혹은 스팟을 전환하고 싶을 때 메뉴얼하게 전환하면, 해당 포스트를 기점으로 스팟을 처리해준다. 디테일한 여행일지 쓰기.(소설 형식) 가끔 좀 더 디테일한 기록을 남기는 사람들이 있다. 우리의 대화의 양상이라거나, 그날 분위기가 어땠지만, 난 기분이 좋지 않았다 라거나. 소설 쓰듯이 일지를 작성하는 스타일 디테일한 여행일지 쓰기_v2(여러 사진과 4~5줄의 글이 더 많은경우.) 앞의 사람들에 비하면, 사진에 대해서 중요시 생각하는 타입. 디바이스를 조금 더 생각해보면, 사진기능이 좋아지고, 한장의 사진이 특정 부분만 클립으로 재생되고 나머지는 하나의 프레임으로 가진 GIF파일도 있다. 하나의 포스트에 많은 사진을 저장하지 말고 시퀸스를 풍부하게 구성하세요.(이 정도 규칙은 유저들이 충분히 수용 할 수 있을 만한 범위) 당신이 사진가라는 것을 우리가 존중하지 않는 것은 아니지만, 서버에 RAW파일을 저장할 순 없잖아요? 아직 21세기는 시작했을 뿐입니다. 공통점 하나의 사진에 긴 텍스트를 하는 경우도 있으나, 여정을 기록할 떄는 사진으로 더 설명을 하려고 하는 경향이 많다. 한 스팟의 하나의 시퀸스에 여러장을 게시하는 것은 없앤다. 그 정도의 많은 사진은 직접 디바이스에 저장하고, 우리 서비스를 통해 등록하는 사진은 하나의 스팟을 구성하는 시퀸스중 하나로, 5분간격 20m 간격으로 촬영하는 갭이 짧은 플롯이 될 수 있지만, 여행포스팅을 어떻게 정리해야 많이 공유되고, 네비게이션을 자동으로 생성해줘서, 시간-지역의 드래그가능한 선형 그래프 스팟별로 색깔을 다르게 표시하고, 개별 시퀸스는 작은 글씨로 표시한다. 시간대별로 정렬하지만, 가장 위쪽에 비용이나 빠르게 참고하고 읽은 후에 이 정보를 읽을 만 한지 판별할 수 있는 장치가 필요. 10중 7은 마음에 안드는 것은 정상적인 것. 내가 이 글을 읽어봐야 할지 아닐지 판별 할 수 있는 정보를 빠르게 전달해야 한다. 여행정보 공유하기 좋은 형태로 재가공하기 스팟이동 경로와 시간 표시 비용에 대한 정보는 애초에 신뢰성이 없다. 그것은 그냥 텍스트 데이터에 맞길것인가? 총 비용 혹은 스팟에 대한 비용으로 처리하거나, 가계부처럼 여정을 진행중에 자신이 얼마나 썼는지 작성 할 수 있도록. 작성하면서 시간을 따로 기록 하지 않는다면, 특정 시퀸스에서 일어난 일이 아니라, 하나의 스팟에서 몰아서 처리하도록 자동으로 스팟을 전환해주는 기능 스팟 위치 시간 사진 텍스트 산호세거리 23-바르셀로나 NONE NONE NONE 아래 포스팅이 최초여서 스팟그룹이 묶여진다. 산호세거리 23-바르셀로나 산호세거리 23 13:00(esp) pic 걷다가 너무 힘들었다 산호세거리 23-바르셀로나 산호세거리 15 13:21(esp) pic 가게 조형물들이 너무 아름답다. 한국가면, 유리 세공공방이나 차려볼까… ㅎㅎ 산호세거리 23-바르셀로나 콜리뇨거리 33 13:45(esp) pic 젤라또를 먹으러 왔다. 이것이 본토의 맛인가… 산호세거리 43-바르셀로나 NONE NONE NONE 거리는 비슷하나, 마지막 포스팅으로부터 3시간 이상 지났기 떄문에 스팟그룹이 전환된다. 산호세거리 43-바르셀로나 산호세거리 43 17:55(esp) pic 호텔을 겨우 잡았다. 방이 솔직히 맘에 들지 않지만, 특히 주인장은 동양인을 무시한다. 이렇게 되면 장점이, 비행기내에서 포스팅을 한번 하면, 10분내로 수십 KM를 이동하기 때문에, 스팟을 전환해서 처리해주고, 또 수십킬로 지나면 스팟이 전환된다. 이게 은근히 매력적인 구성. 중국해 영공에서(스팟)-&gt; 발트해 영공에서(스팟 10분경과) 소비를 기록 하는 기능, 그냥 기록하면 알아서 해당 스팟내에서 발생한 것으로 간주하고 시간 순으로 정렬해줘요. 직접 스팟을 지정하거나, 시간을 지정하면 어찌됐건 해당 스팟으로 처리되요. 스팟 혹은, 포스트에 대한 태그 태그가 어떤 기능을 해결해 줄 수 있을까? 어떤 지역 여행을 한 사람들의 족적을 미리 체크해봄으로 간접체험하고, 내가 더 나은 경험을 하기 위함이 큰 목적이라면. 스팟마다 사용한 비용을 포스트:잠, 태그:라인에 맞춰서, 지도상이나 그래픽위에, 시간과 금액을 표시하고, 총액은 스팟별로. 여행을 마감하면, 종료된 여행으로 간주하고, 모든 금액을 산출해서 정확히 어디에 소비했고 이런게 아니라, 그냥 얼마나 썼는지, 하나의 스팟에서 시퀸스가 음식이나, 쇼핑, 여행 물품, 지역교통비, 관광료 등 여러요소가 존재 할 수 있으니 그것은 이름만 적어서 작성할 수 있도록. 어차피 가계부에서도, 종목을 정하지만 세부적인 상품명은 별도로 달아준다. 그리고 사실 한눈에 보면 간편해 보이는 듯 하지만, 식음료비용이 전달보다 2만원 늘었다는 사실이, 그렇게 큰 도움은 되지 않는다. 소비 패턴은 분석해 줄 수 있겠지. 검색할때, 태그가 부연설명으로 도움이 될 수 있을까? 우리는 기본적으로 지역에 대한 정리된 여정을 검색하고 보는 것에 의미를 두고 있다면, 어떤 검색을 통해서, 내가 원하는 정보를 얻을 수 있을까? 유효한 지역을 검색하면, 스팟이 그 지역 부근 1km안에 포함된다면? 모두 출력하고, 1KM밖이라면, 제일 가까운 순위별로 주는 것도 괜찮다. 그러면 거리를 출력해줄 필요가 있다. 해당 스팟의 위도경도정보를 같이 저장해준다면, 구글에 분석을 요청하지 않고도, 바로 처리할 수 있다. 최초 포스트를 등록하거나를 통해서 스팟이 시작되면, 그 시작점을 기준으로 위도경도 정보를, 서버에서 요청하는 것이 아니라, 클라이언트에서 작성해서 넘긴다. 물론 조작의 위험도는 있지만, 크리티컬한 이슈가 아니고, 단순히 정보가 잘못됐다는 것이기 때문에, 그것은 사용자가 이거 엉터리 글이네 하고 넘길 수 있다. 위도경도의 수치계산 알고리즘을 통해서 인접 거리를 구해서? 일단 나라는 같아야 한다? 그러면 스팟마다 위도경도를 해당 스팟 아래 속하는 모든 시퀸셜 포스트가 동일하다고 간주하고 처리하니, 어차피 거리가 일정 이상 벗어나면 스팟을 넘겨버릴 것이다. 모든 스팟에 대해서 풀스캔? 아니다 최소한 나라라는 기준은 매치해야한다. 시와 시를 왔다 갔다 하는 것은 굉장히 애매한 기준이기 때문에, 물론 왠만하면 시의 중심지가 주요 관광지이겠지만, 그냥 나라를 기준을 나눈다면 가장 안전할 수도있다. 인접 지역이라도 같은 나라가 아니라면 여행자가 원하는 정보가 아닐 가능성이 매우 높다. 지역적 키워드를 검색하면 그것의 해당하는 나라와, 위도경도정보를 서버로 전송한다. 나라에 등록된 스팟을 모두 검색한다. 잠깐. 하나의 여정노트를 완성한다고 할떄, 여러 나라를 들릴 수 있다고하면, 하나로 묶어주는게 좋을까 별개로 나누는게 좋을까? 나누는게 백번 나은데, 도저히 나눌 수 없는 상황도 있지 않을까? 잠깐 미국에서 캐나다 하루 들렸다 오는 길이며, 미국에서 1주일 머무는 거라고 하면 얘기가 달라진다. 그렇다면 여정에서 나라는 제외하자. 스팟 기준으로 나라는 바뀔 수 있고. 만약 유저가 해당 여정노트를 마감하고 싶다고하면, 매뉴얼하게 할 수 있다. 포스트를 등록할때. 어떤 나라로, 포스트를 등록하겠다고 하면 스팟과 시퀸셜 포스팅 기준 모델 유저 |이름|여정| 여정 |유저|스팟|주요 테마| 스팟 |나라|위도,경도|시퀸셜 포스팅|지역적 테마(태그 1개)| 시퀸셜 포스트 |시간|텍스트|멀티파트-이미지, 동영상(15)|소비|태그(13개)| 지역적 테마를 통해서, 주 목적과 조금 다른 일시적인 여흥도 커버 할 수 있다. 190917 유저가 초기 입력값인, interests를 설정하면, 그것에 대해서 유효한 추천건을 어떻게 가져올 것인가. 유저가 음식에 관심이 있다고 처음 입력 했다면, 검색할때, 여정에 총 테마가 음식인 것 패스의 테마가 음식인 것. SPOT RENAMED TO PATH |country|pos_x|pos_y|post_ids|theme| POST(의미적으로 SPOT과 동일) |time|text|pos_x|pos_y|media(1~5) auto_layout기능, 업로드 순으로 보이도록 처리를 해야 한다.|payments|tags| TAG는 검색에 연동시키기 위한 장치라고하면 THEME은 순전이 키워드 기반 주제를 골라주도록 유도하는 것. healing, nature, shopping, locality, experience등으로 THEME을 정하면, 그것을 기반으로 작은 조각으로 POST가 아닌 PATH를 검색하면, POST가 여행을 이해하기에는 너무 내러티브가 존재하지 않는다고 하면, PATH는 앞뒤로 어느정도 과정이 있으니, 그것을 충족시켜줄 수 있다. 사진을 로컬앱으로 업로드하는 과정에서, 레이아웃이 적용되서 처리되니까, 유저가 서버에 업로드 전(글 작성 완료)에 알아서 수정할 수 있다. 여행중 유저의 시나리오. 여행을 하고 있고, 시장으로 나가는 길에서부터 앱을 실행해서, 여행시작하기 를 하고 첫번 쨰 포스트를 등록하려 한다. 시장의 과일상들의 사진과 시장의 일부 오브젝트를 촬영하여 4장을 업로드 하겠다. 글은 간단한게 작성했다.(ㅇㅇㅇ시장의 풍경 #ㅇㅇㅇ시장 #푸드) 20미터 쯤 지났을까, 또 포스트작성하기를 하니, 이전촬영지역의 이름의 앨범에 이어서 작성하도록 되어있다. … 마찬가지로 작성한것으로… 그리고 20분쯤뒤에 새로운 포스트를 작성하려고 하는데, 앨범을 전환하도록 했다.(새로운 지역이름) 기존에 진행중이던 앨범이 마감되면서, 해당 앨범이 그래픽으로 뭔가 고정되었음을 알 수 있고, 해당 앨범을 클릭하니, 지도에 지역이 점으로 찍혀서 선으로 나와있고, 밑에 요약이 되어있으면서, 그 루트 안의 총 지출이 요약되어있다. 정확히 어느지점에서 썼는지는 모르겠으나, 어떤 소비가 있었는지는, 해당 앨범이 활성화 되어있을때, 포스트등록하기가 아닌, 지출입력하기 로 큰카테고리와 상품명, 가격을 해당 지역의 화폐단위에 맞춰서 처리했다. 여행 후 유저의 시나리오 앨범 에 사진이 몰려있고, 그것을 순차적으로 조금 나눠서 날짜별로 정리하려 한다. 사진 5개를 등록하고, 태그를 작성했고, 시간과 위치를 메뉴얼하게 입력해 주었다. 다음에 이어서 하다가, 날짜를 전환하도록 한다. 이경우 한날짜에 너무 많은 데이터가 몰려서 들어간다면, 검색할때 그 정보가 그렇게 유효하지 않기 떄문에, 1일차-ㅌㅌㅌ시장 이런식으로 수정을 하도록 권고할 수 있으면 좋겠다. 그러면, 어차피 유저가 정리할 떄는, 대부분 여행기록은 시간적으로 묶어서 처리하는 경우가 많기 때문에, 물론 그렇게 하지 않아도 되지만, 장소적으로는 인근지역을 몰아서 처리하거나, 하는 경우는 분명 대다수 라 할 수 있다. 화폐단위 처리는, 저장할떄, 단위는 해외에서 USD를 안쓰는 곳에서도 USD를 처리하는 경우가 많기 떄문에, 서버에 저장할때는 USD든 뭐든 해당 국가의 화폐단위로 변환해서 저장해주면, 모든 소비가 해당국가의 화폐단위의 수치로 기록이 되고, 그것을 나중에 자국 화폐로 변환해서 볼떄는 매번 바뀌는 식으로. 소수점 2자리까지 허용해서, 300.12정도로 기록?","categories":[],"tags":[{"name":"project","slug":"project","permalink":"https://codenamenadja.github.io/tags/project/"},{"name":"travel_note","slug":"travel-note","permalink":"https://codenamenadja.github.io/tags/travel-note/"}]},{"title":"http/1.0의 신택스: 기본이 되는 네가지 요소","slug":"http/1_http_1.0_basic_four_feature","date":"2019-09-06T11:03:56.000Z","updated":"2021-01-25T08:36:06.071Z","comments":true,"path":"2019/09/06/http/1_http_1.0_basic_four_feature/","link":"","permalink":"https://codenamenadja.github.io/2019/09/06/http/1_http_1.0_basic_four_feature/","excerpt":"","text":"index HTTP 역사 및 대표 기관명칭 HTTP 0.9 HTTP 1.0 전환 Content-Type의 값 MIME타입 메서드와 스테이터스 코드 URL BODY HTTP의_역사와_담당기관 1990년: HTTP/0.9, CREN에서 근무하던 팀 버너스리가 최초 웹서버 CERN HTTPd 개발 1996년: HTTP/1.0 1997년: HTTP/1.1 2005년: HTTP/2 이름 정식 명칭 역할/의미 IETF Internet Engineering Task Force 인터넷의 상호 접속성을 향상시키는 것을 목적으로 만들어진 임의 단체 (통신프로토콜 관리자) RFC Request For Comments IETF가 만든 규약문서 (상호 접속성 유지를 위한 사양서 모음) IANA Internet Assigend Numbers Authority 포트번호와 컨텐츠타입등 웹에 관한 데이터베이스를 관리하는 단체 W3C World Wide Web Consortium 웹 관련 표준화를 하는 비영리 단체 (브라우저에 특화된 기능 책정) WHATWG Web Hypertext Application Technology Working Group 웹 관련 규격을 논의하는 단체, W3C와 겸하는 멤버가 많다. HTTP_0.9 go 에코서버 12345678910111213141516171819202122232425262728293031package mainimport ( \"fmt\" \"log\" \"net/http\" \"net/http/httputil\")func handler(writer http.ResponseWriter, req *http.Request) &#123; dump, err := httputil.DumpRequest(req, true) /* func dumprequst(req *http.request, body bool)([]byte, error) */ if err != nil &#123; http.Error(writer, fmt.Sprint(err), http.StatusInternalServerError) return &#125; fmt.Println(string(dump)) fmt.Fprintf(writer, \"&lt;html&gt;&lt;body&gt;hello&lt;/body&gt;&lt;/html&gt;\")&#125;func main() &#123; // 초기실행부 var httpServer http.Server http.HandleFunc(\"/\", handler) // \"/\"에 접속이 있을 때, handler함수를 호출 log.Println(\"start http listening :18888\") httpServer.Addr = \":18888\" // 18888포트로 설정 log.Println(httpServer.ListenAndServe())&#125; request 전달 1234567891011# client$ curl --http1.0 http://localhost:18888/greeting# response&lt;html&gt;&lt;body&gt;hello&lt;/body&gt;&lt;/html&gt;# server: requestGET / HTTP/1.0Host: 127.0.0.1:18888Connection: closeAccept: */*User-Agent: curl/7.58.0 0.9버전은 1.0과 호환되지 않기 때문에, 0.9요청을 전달하는 것은 현재는 불가능하다. 0.9버전 당시에 content/type은 text/html밖에 존재하지 않음 검색 기능 12345678910# client$ curl --http1.0 --get --data-urlencode \"search word\" http://localhost:18888/greeting# serverGET /?search%20word HTTP/1.0Host: 127.0.0.1:18888Connection: closeAccept: */*User-Agent: curl/7.58.0 http_0.9에서_1.0으로 HTTP/0.9의 프로토콜로는 할 수 없는 일 하나의 문서를 전송하는 기능만 존재(&lt;html&gt; -&gt; &lt;/html&gt;) 컨텐츠 타입은 모두 text/html로 가정. 다운로드할 컨텐츠를 서버가 바꿀 수 없었다. 클라이언트에서 검색이외 요청을 보낼 수 없었다. 새로운 문장을 전송하거나 갱신 또는 삭제 불가?(소켓유지?) 요청이 올바른지 혹은 서버가 올바르게 처리했는지 전달할 수 없음. 123456789101112131415161718192021curl --get -v --http1.0 http://127.0.0.1:18888/* Trying 127.0.0.1...* TCP_NODELAY set* Connected to 127.0.0.1 (127.0.0.1) port 18888 (#0)# &gt;로 시작하는 행이 클라이언트에서 서버로 갈 내용&gt; GET / HTTP/1.0&gt; Host: 127.0.0.1:18888&gt; User-Agent: curl/7.58.0&gt; Accept: */*&gt; # 여기까지* HTTP 1.0, assume close after body# &lt;로 시작하는 행은 서버 응답&lt; HTTP/1.0 200 OK&lt; Date: Fri, 06 Sep 2019 11:53:03 GMT&lt; Content-Length: 31&lt; Content-Type: text/html; charset=utf-8&lt; * Closing connection 0&lt;html&gt;&lt;body&gt;hello&lt;/body&gt;&lt;/html&gt; 1.0으로 버전업 변경점 요청 시 메서드가 추가 요청 시 HTTP버정이 추가(HTTP/1.0) 헤더가 추가(Host, User-Agent, Accept) 응답 시 HTTP버전과 스테이터스 코드가 포함 요청과 같은 형식의 헤더가 포함됨 ARPAnet에서 부터 시작된 전자메일(RFC822)이 HTTP보다 더욱 발달 했었기 때문에, 많은 참조, 승계가 일어남. Req header From Client User-Agent: 클라이언트가 자신의 어플리케이션 이름을 전달 하는 키값. Referer: 서버에서 참고할 수 있는 추가 정보, 요청을 보내는 시점의 URL등을 포함. 보안때문에, 사양이 당초 보다 크게 변경됨. Authorization: 특별한 클라이언트에만 통신을 허가할 떄 인증 정보를 서버에게 전달 Res header From Server Content-Type: 파일 종류. MIME타입 이라는 식별자를 기술.(전자메일을 위해 만들어짐) Content-Length: 바디 크기. 압축이 이루어지는 경우 압축 후의 크기 Content-Encoding: 압축이 이루어진 경우 압축 형식 Date: 문서 날짜 12345678910111213141516171819202122232425262728# 윈도우 7의 익스플로러 10 버전으로 User-Agent설정curl -v --http1.0 -A \"Mozilla/5.0(compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)\" http://127.0.0.1:18888/# server outputGET / HTTP/1.0Host: 127.0.0.1:18888Connection: closeAccept: */*User-Agent: Mozilla/5.0(compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)# client output* Trying 127.0.0.1...* TCP_NODELAY set* Connected to 127.0.0.1 (127.0.0.1) port 18888 (#0)&gt; GET / HTTP/1.0&gt; Host: 127.0.0.1:18888&gt; User-Agent: Mozilla/5.0(compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)&gt; Accept: */*&gt; * HTTP 1.0, assume close after body&lt; HTTP/1.0 200 OK&lt; Date: Fri, 13 Sep 2019 07:41:43 GMT&lt; Content-Length: 31&lt; Content-Type: text/html; charset=utf-8 # MIME타입 문자열&lt; * Closing connection 0 MIME타입 MIME타입_개요 (RFC 1341) RFC1341에 처음 등장하여, 전자메일을 위해 만들어졌으며, 파일의 종류를 구별하는 문자열이다. 인터넷이 보급된 시기는 MS-DOS, 윈도우3.x, 맥 OS7등이 사용되던 시기. OS별 파일의 구별 윈도우: 파일 확장자를 통해 구별 맥: Resource fork라 불리는 메타정보로 파일 종류를 판단 지금도 위 사항은 기본적으로 유지되고 있음. Content-Type의 등장 (RFC 1049) 이 시기(1988년)에는 Content-Type인 MIME_TYPE(아직 MIME타입이 아니라 값)에는 POSTSCRIPT TEX 와 같은 것들이 있었다. 대항목/상세, 같은 식의 표기방식을 채택한 1992년 RFC 1341에서(HTML 미등장) ‘text/plain’ ‘image/jpeg’ ‘multipart/form-data’ 같은 포멧을 위한 표기 방법이 등장하게 되면서 MIME타입이라 공식화 하게된다. 서버에서 클라이언트로 데이터를 전송할 때, 컨텐트 타입을 헤더에 MIME타입값으로 정의하여 send하면, 브라우저 혹은 무엇이든 요청을 보낸 클라이언트 어플리케이션은 그것을 수용할 수 있는지 없는지 판단하고 서버와 소통을 통해 최종 결정하여 body를 받는다. 이 과정을 Negotiation이라 한다. RFC 1590에서 새로운 종류의 MIME타입을 IANA에 등록신청하는 절차를 구성하였다. RFC 3023에서, JSON, XML등의 MIME타입이 공식화 되었다. 이로써, XML을 기반으로 한 SVG이미지를 application/xml이 아니라, Image/svg+xml로 표기 할 수 있게 되었다. Content-Type과_보안 특정 타입으로 BODY를 해석해 달라는 요청을 Content-Type대신 확장자를 사용하려고 해도, 클라이언트에 따라 전송되지 않을 수 있다. 2000년 초기까지는 CGI를 이용한 접속 카운터(숫자가 들어간 이미지를 생성하는 Pearl등의 스크립트 언어 프로그램)로 대체. 이 카운터는 .cgi라는 확장자로 HTML에서 아래와 같은 식으로 CGI를 호출하여 사용하였다. 1&lt;img src=\"cgi-bin/counter.cgi\"&gt; 이 경우, cgi프로그램이 Content-Type:image/gif같은 헤더를 생성하여 처리. 그러나 익스플로러는 인터넷 옵션에 따라, MIME타입이 아니라 내용을 보고 파일 형식을 추측하기도 한다. 이런 동작을 content sniffing이라 한다. 서버에서 잘못된 처리로 타입과 언매치한 바디를 주었을 때, 해결해 줄 수 있는 장점이 있으나, 브라우저가 추측하는 정보가 잘못되어, 예상 외의 분석, 실행을 하는 경우에 보안의 문제가 일어난다. 예를 들어 자바스크립트가 아닌데, 자바스크립트로 해석하면, 엄격한 규칙에 따라 그 응답을 브라우저에서 실행하지 않는 경우등이 있다. 서버에서 다음과 같은 헤더를 통하여, 브라우저의 추측을 금지하도록 지시하는 방식을 주로 사용하고 있다. X-Content-Type-Options: nosniff 메서드와_STATUS_CODE HTTP의_또다른_메타_뉴스그룹 인터넷 이전의 주요 미디어로서, 지금은 사용되지 않는 뉴스그룹은 분산 아키텍쳐로 구성되어 있다. 사용자는 서버에 구독하는 최신기사를 요청하고 기사가 있으면 가져온다. 웹처럼 모든 사용자가 한 곳의 서버에 접속하러 가는 것이 아니라, 복수의 서버가 master/slave구조로 연결. 슬레이브의 서버가 마스터로 접속하고 정보를 가져와 로컬에 저장. 스토리지 용량제한으로 오래된 기사는 삭제한다. 마스터서버를 참조하는 복수의 슬레이브 서버, 그리고 슬레이브에 접근하는 클라이언트 간 통신에 사용된 것이, NNTP(Network News Transfer Protocol)이다. (RFC 977) HTTP0.9보다 5년 빠른 1986년에 정식화 되었으며, 메세지 포멧은 그보다 3년 전인 RFC 850에서 등장하였다. 이 포멧도 전자메일을 영향을 받아 정식화되었고, 마찬가지로 헤다와 본문이 있고, 사이에 빈줄이 들어가는 구성이다. HTTP는 뉴스그룹 프로토콜로 부터 METHOD, STATUS CODE를 이어받는다. METHOD 지정된 주소에 있는 리소스에 대한 조작을 지시한다. 뉴스그룹 메서드 LIST: 그룹 목록 취득 HEAD: 헤더 취득 BODY: 기사 취득 POST: 투고 HTTP의 경우는 파일 시스템과 같은 설계철학으로 만들어졌다. 기본으로 GET, HEAD, POST는 가장 많이 쓰이는 메서드이며, PUT, DELETE는 사용빈도에 따라 필수가 아니라 구현에 따라 Optional한 기능이 되었다. GET: 헤더와 컨텐츠 요청 HEAD: 헤더만 요청 POST: 새로운 문서 투고 PUT: 이미 존재하는 URL의 문서를 갱신 DELETE: 지정된 URL의 문서를 삭제, 성공시 해당 URL은 무효 curl --http1.0 --request POST http://127.0.0.1:18888/ STATUS_CODE 뉴스그룹 프로토콜에서 가져온 기능으로, 5가지 카테고리로 나눌 수 있다. 1XX번대: 처리가 계속됨을 나타낸다. 2XX번대: 성공했을 때의 응답. 200 OK 정상종료 3XX번대: 서버에서 클라이언트로의 명령. 오류가 아니라 정상처리의 범주이며, 리디렉트, 캐시 이용등을 지시한다. 4XX번대: 클라이언트가 보낸 요청에 오류가 있다. 5XX번대: 서버 내부에서 오류가 발생했다. 리디렉트 Redirect를 통해 다른 페이지를 GET하라는 권고로, 해당 요청에서는 시나리오가 없으니, 다시 요청을 날리라는 표현. 코드번호 메서드 변경 영구적/일시적 캐시 설명 301 Moved Permanently O 영구적 한다 도메인 전송, 웹사이트 이전, HTTPS 302 Found O 일시적 지시에 따름(Cache-Control, Expires헤더를 통해) 일시적 관리, 모바일 기반 전송 303 See Other 허가 영구적 하지않음 로그인 후 페이지 전환 307 Temporary Redirect 일시적 지시에 따름 RFC 7231에서 추가 308 Moved Permanetly 영구적 한다 RFC 7538에서 추가 영구적: 영구적인 접근 차단.(무효한 자원, HTTP -&gt; HTTPS) 일시적: 이동하려던 페이지에 언젠가는 이동 가능(존재하는 자원) 303의 경우: 로그인 같은 경우는 보통 POST메서드로 이루어지는데, 자원에 대한 요청이 아니기 때문에, 반환할 컨텐츠가 없으며, 동시에 별도로 처리할 페이지가 있는 경우이다. 따라서 해당 리디렉션은 일시적으로 접근 못하는 것이 아니라 정상적인 시나리오 내에서, 영구적으로 가야하는 곳을 다시 지정하는 시나리오. 301의 경우: 요청된 페이지가 다른 장소로 이동했을떄, 기존도메인에서 301을 처리해주면, 구글검색엔진은 해당 페이지에 대한 평가를 리디렉션으로 상속시킨다. Redirection의 경우 대부분 헤더에 Location키값이 존재히는데, curl --location --max-redirs 5 GET http://127.0.0.0.1:18888/get_to_redirect/ Response-헤더에 location이 존재할 경우, 해당 값으로 GET 최대 5회까지 리디렉션을 보내고, 기존 Request-헤더를 유지한다. 구글은 리디렉트의 사이드 이펙트를 염려하며, 권장 3회 이하, 최대 5회 이하라는 가이드 라인을 제시하였다. RFC 2616에서는 302의 코드설명상, 메서드변경을 허가가 필요하도록 하였고, 현재 유저에이전트는 대부분 GET으로 변경한다. RFC 7231애서는, 302를 301처럼 메서드 변경을 허용하고, 변경허가가 필요한 307, 308을 추가한다. URL RFC 1738에서 등장하였으며, 상대적 URL은 RFC 1808에서 등장. 이들은 모두 HTTP/1.0보다 빠른 시기의 문서로, HTTP/0.9중에 HTTP/1.0를 계획하에 먼저 규격화 되었다. URL의_구조 일반적인 경우 스키마://호스트명/경로의 형태로 구성이 되지만, URL사양에 포함되는 모든 요소가 들어간 경우는 스키마://사용자:패스워드@호스트명:포트/경로#프래그먼트?쿼리로 구성된다. 스키마: 스키마해석은, 브라우저의 책임이며, https. mailto, ftp등이 올 수 있다. 로컬 파일을 브라우저로 열면 file:로 표시된다. 사용자:패스워드: Basic인증 방식이며, 로그에 URL이 남게되어 유출되기 때문에, 웹 시스템에서는 사용하지 않는다. 호스트명: DNS서버에, 엔드포인트로는 실제 IP라우팅 주소값을 특정하여 목적지를 찾아간다. 포트: 스키마에 따라서 기본 well-known-port로 처리되며, 한 서버에 여러포트를 사용하는 경우 복수 서비스를 운영할 수 있다, 프래그먼트: HTML문서 내의 링크 앵커를 지정 쿼리: 해당 웹페이지에 대해서 특정 파라미터를 부여하고 싶은 경우 사용 URL과_국제화 기존엔 URL의 도메인 이름으로 영숫자와 하이픈만 사용 가능. 2003년 RFC 3492에서 IDN(International Domain Name)을 표현하는 인코딩 규칙 PUNY CODE가 정해져, 퓨니코드가 구현된 브라우저에서는 다국어를 도메인 네임으로 사용할 수 있다. 정해진 규칙에 따라, 영숫자이외의 문자를 반각영숫자로 치환해 요청을 보낸다. 한글도메인.kr -&gt; xn--bj0bj3i97fq8o5lq.kr이 된다. 퓨니코드는 반드시 xn--로 시작되는 문자열을 생성한다. BODY HTTP1.0이후부터 요청, 응답 모두 헤더를 포함하게 되면서, 헤더와 바디를 분리할 필요가 있어졌다. 전자메일과 같이 헤더의 끝에 빈줄\\n을 넣으면 그 이후는 모두 바디가 된다. 그러나 바디의 전송할 때, 데이터를 저장하는 포멧이 2종류가 존재하여서, 용도에 맞게 구분법이 다르다. Content-Encoding의 압축알고리즘을 통한 압축전송과 HTMLFORM, XMLHttpRequest를 사용한 Request측 바디 전송 청크 형식의 바디 송신 12345헤더1: 값헤더2: 값Content-Length: 바디의 바이트 수--이 줄부터 지정된 바이트 수 만큼 바디-- Head요청시, 헤더만을 요구하는 요청이지만, Content-Length와 E-Tag등을 바르게 전송해야한다. Curl커맨드의 바디 획득 옵션 옵션 용도 -d, --data, --data-ascii 변환 완료된 텍스트데이터 –data-urlencode 텍스트 데이터 ascii변환 –data-binary 바이너리 데이터 -T Filename, -d @Filename 보내고 싶은 데이터를 파일에서 읽어온다. 송신시 바디를 서버에 보내려면 -d옵션을 사용한다. 그럴 경우 기본적으로 Content-Type:application/x-www-form-urlencoded가 된다. JSON을 전송하고 싶은 경우 1234curl -d \"&#123;\\\"hello\\\":\\\"world\\\"&#125;\" -H \"Content-Type: application/json\" http://127.0.0.1:18888/# JSON파일에서 읽어서 전송curl -d @test.json -H \"Content-Type: application/json\" http://127.0.0.1:18888/ application/x-www-from-urlencoded같은 형식은 http0.9에서 파생되어, http1.0보다 먼저 표준화된 HTML 2.0의 사양이다. 이는 RFC 1866에서 표준화 되었으며, RFC로 정의된 html은 이게 처음이자 마지막. RFC 2854의 결정으로 HTML사양 결정은 W3C로 넘어간다.","categories":[],"tags":[{"name":"http","slug":"http","permalink":"https://codenamenadja.github.io/tags/http/"}]},{"title":"비동기 코루틴 번역-5-End","slug":"python/post/async_coroutine_5","date":"2019-09-05T09:17:02.000Z","updated":"2021-01-25T08:36:06.083Z","comments":true,"path":"2019/09/05/python/post/async_coroutine_5/","link":"","permalink":"https://codenamenadja.github.io/2019/09/05/python/post/async_coroutine_5/","excerpt":"","text":"코루틴_정비하기 우리는 어떻게 크롤러가 동작하는지 설명하는 것으로 부터 시작했다. 이제 asyncio 코루틴을 사용해서 구현할 차례이다. 우리 크롤러는 첫 페이지를 fetch한다. 그것의 링크들을 파싱하고, 그것들을 queue에 더할 것이다. 웹사이트를 돌고나면 pages를 동시적으로 fetching할 것이다. 하지만 클라이언트와 서버의 제한된 load때문에, 우리를 최대 워커 수를 필요로 한다. 언제든 워커가 페이지를 fetching하는 것이 종료되면, 그 워커는 급히 다음 링크를 queue에서 꺼내야한다. 그리고 더 돌아갈 작업이 없다면, 일부 워커는 멈춰야한다. 하지만 워커가 수많은 링크를 지닌 페이지를 히트한다면, 큐는 급격히 커지고, 멈췄던 워커들은 다시 동작해야한다. 따라서 우리의 프로그램은 모든 일이 끝났을 때만 종료된다. 워커가 쓰레드라고 가정해보자, 우리가 크롤러의 알고리즘을 어떻게 표현해야 할까? 우리는 파이썬 표준 라이브러리에서 Synchronized queue를 사용할 수 있다. (공유자원 개념?) 아이템이 queue에 들어갈 때마다, queue는 그의 tasks의 count를 증가시킨다. 워커 쓰레드들은 task_done을 개별 아이템이 끝날 때마다 호출한다. 그러면 메인쓰레드가 Queue에 들어간 각아이템이 task_done콜에 의해 매칭될 때까지 Queue.join에서 블록하고, 종료할 것이다. 코루틴은 asyncio queue를 이용한 동일한 패턴을 사용한다. 1234try: from asyncio import JoinableQueue as Queueexcept ImportError: # 파이썬 3.5 버전 이상 from asyncio import Queue 우리는 워커들의 공유상태를 crwaler class에 모을 것이고, 메인 로직을 그것의 crwal메서드에 작성할 것이다. 12345678910111213141516from asyncio import Queue, get_event_loopfrom aiohttp import ClientSessionloop = get_event_loop()class Crwaler: def __init__(self, root_url, max_redirect): self.max_tasks = 10 self.max_redirect = max_redirect self.q = Queue() self.seen_urls = set() # aiohttp의 클라이언트 세션이 connection을 풀링하고, # HTTP Keep-alives for us. self.session = ClientSession(loop=loop) self.q.put((root_url, self.max_redirect)) 현재 q에서 끝나지 않은 작업은 하나이다. 우리의 메인 스크립트로 돌아가서, event루프와 crwal메서드를 실행한다. 12345678910@coroutinedef crawl(self): workers = [Task(self.work()) for _ in range(self.max_tasks)] yield from self.q.join() from w in workers: w.cancel()if __name__ == \"__main__\": crawler = Crwaler(\"http://xkcd.com\", max_redirect=10) loop.run_until_complete(crawler.crawl()) 만약 워커가 쓰레드 였다면, 우리는 그들이 동시에 실행되는 것을 원하지 않을 것이다. 그들이 특별히 필요로 해지기 전까지 그들이 생성되는 비용을 피하기 위해서, 쓰레드 풀은 대게 필요한 때에만 커질 것이다. 하지만 코루틴은 저렴하다. 그래서 우리는 그들을 간단히 최대 허용 수만큼 시작할 수 있다. 우리가 어떻게 크롤러를 종료시킬 것인지를 아는 것은 중요하다. join기능이 해결될 때, worker tasks들은, 살아있지만 종료된 상태 일 것이다.: 그들은 더 많은 URL들을 기다리고 있지만 오지 않은 것이다. 따라서 메인 코루틴은 그들을 종료시키기 전에, 취소 시킬것이다. 그렇지 않으면, 파이썬 인터프리터가 종료되고, 모든 PyObject의 destructors를 호출할 때, 살아있는 tasks들은 외칠것이다. ERROR:asyncio:Task was destroyed but it is pending! 그러면 어떻게 cancel은 동작하는가?: 제너레이터들은 당신이 모르는 기능을 가지고 있다. 12345678gen = gen_fn()gen.send(None)# 1gen.throw(Exception(\"error\"))\"\"\"Traceback (most recent call last): ...Exception: error 제너레이터들은 throw를 통해서 재진입 되지만, 예외를 발생하고 있다. 만약 제너레이터의 콜스택의 코드에서 그것을 감지하지 못한다면, exception은 불어나서, 맨 위로 올라올 것이다. 그래서 task의 코루틴을 취소하기 위해서 12345678\"\"\"&gt;&gt;&gt; from asycio import Task&gt;&gt;&gt; worker = Task(self.work())&gt;&gt;&gt; worker.cancel()\"\"\"# method of Task classdef cancel(self): self.coro.throw(CacelledError) 어떤 yield from진술들에서 제너레이터가 정지하면, 이것은 재진입하고 예외를 던진다. 우리는 취소를 task의 step메서드를 통해서 핸들링 할 것이다. 1234567891011# method of Task classdef step(self, future): try: next_future = self.coro.send(future.result) except CancelledError: self.cancelled = True return except StopIteration: return next_future.add_done_callback(self.step) 이제 task는 그것이 최소 되었는지 알 수 있다. 그래서 이것이 소멸될 때, 소리지르지 않을 것이다. crawl이 한번 워커를 중지 시키면, 그것은 종료한다. 이벤트루프는 그것을 보면 코루틴이 종료되었다고 코루틴이 프로그램이 종료되기를 기다린다고 본다. loop.run_until_complete(crawler.crawl()) crwal메서드는 모든 메인 코루틴이 해야하는 것을 해낸다. 이것이 queue에서 URL을 가져오고, 그들을 fetch하고, 새로운 링크를 파싱하는 워커 코루틴이다. 각 워커들은 work코루틴을 개별적으로 실행한다. 123456789class Crwaler: @coroutine def work(self): while True: url, max_redirect = yield from self.q.get() # download page and add new links to self.q yield from self.fetch(url, max_redirect) self.q.task_done() 파이썬은 이 코드가 yield from진술을 포함하는 것을 보고, 제너레이터 함수로 컴파일 한다. 따라서 crawl안에, 메인 코루틴이 self.work를 10번 호출할 때, 이것은 실제로 이 함수를 호출 하지 않는다. 그것은 오직 이 코드블록을 바라보는 10개의 제너레이터 객체를 생성할 것이다. 이것은 개별 Task안에 래핑되고, Task는 개별적인 future 제너레이터가 yield하는 것을 받는다. 그리고 future가 resolve될 때, 제너레이터를 send에 개별적인 future의 result를 매개변수로 콜하면서 진행할 것이다. 왜냐하면 제너레이터는 개별적인 스텍프레임을 가지고 있고, 개별적으로 동작하고, 개별적인 지역변수와 인스트럭션 포인터를 가졌기 때문이다. 워커는 queue를 통해서 그의 친구들과 정렬된다. 그것은 아래처럼 새로운 URL을 기다릴 것이다. url, max_redirect = yield from self.q.get() queue의 get메서드는 그 자체로 코루틴이다.: 그것은 누군가 queue에 item을 넣을때 까지 정지하고, 이후 돌아와서 들어온 아이템을 리턴한다. 우연히 아곳은 워커가 crwal의 끝부분에서 메인 코루틴이 그것을 중지할때 멈추는 부분이다. 코루틴의 관점에서 보았을 때, 이 루프의 마지막 실행은 yield from 이 CancelledError을 raise할때 일어난다. 워커가 페이지에서 링크들을 파싱하고, 새로운 링크를 큐에 집어 넣을 때, task_done을 통해 counter를 줄인다. 결국 워커는 워커가 모든 URL들이 fetched된 페이지를 fetch하면, 큐에는 더이상 할 일이 남아있지 않다. 그러므로 이 워커가 task_done을 호출하는 것은 카운터를 0으로 줄인다. 그러면 queue의 join메서드를 기다리고 있는 crawl은 다시 재개되고, 마무리한다. aiohttp 패키지는 기본적으로 redirects를 따르고, 우리에게 최종 response를 가져다 준다. 우리가 구분할 수 있건없건, redirects를 크롤러 내에서 핸들링 하고, 결국 이것은 모든 한 목적지로 가는 redirect path들을 coalesce할 수 있다.: COALESCE: SQL함수로, NULL이 아닌 첫 값을 반환하는 기능 만약 해당 URL을 이미 보았다면, 이것은 self.seen_urls에 있을 것이고, 우리는 이미 이 path에 대해새 다른 엔트리 포인트에서 시도해본적 있는 것이다. first entry point redirects destination /foo /baz /quux /bar /baz /quux /foo에 접근한 워커가 baz를 리디렉트하고, quux로 진행하였다면, baz를 seen_urls로 이동시킨다. /bar에 접근한 워커가, baz로 리디렉트 한다는 것을 알면, Fetcher은 baz를 이미 seen_urls에 있는 것으로 감안하여 enqueue하지 않는다. 만약 응답이 리디렉션이 아니라 페이지문서였다면, fetch는 이것의 링크를 파싱하고, 새로울 것을 enqueue한다. STATUS CODE 301 Moved Permanently 301의 HTTP응답은 location헤더가 포함되는 것이 일반적인데, location헤더에 해당 엔드포인트의 새로운 주소가 포함되어 나온다. 클라이언트는 location헤더의 엔트포인트의 새로운 주소에 해당 요청을 다시 보내게 된다. 12HTTP/1.1 301 Moved PermanentlyLocation: http://www.example.org/index.asp 12345678910111213141516171819202122class Crawler: @coroutine def fetch(self, url, max_redirect): response = yield from self.session.get( url, allow_redirects = False ) try: if is_redirect(response): if max_redirect &gt; 0: # response가 최초 리디렉션 301 MOVED PERMANENTLY 일때, next_url = response.headers[\"location\"] # header에 일반적으로 포함되는 location헤더. if next_url in self.seen_urls: return self.q.put_nowait((next_url, max_redirect - 1)) else: links = yield from self.parse_links(response) for link in links.difference(self.seen_urls): self.q.put_nowait((link, self.max_redirect)) self.seen_urls.update(links) finally: yield from response.release() 만약 이게 멀티스레딩 코드 였다면, race컨디션이 넘쳐났을 것이다. 예를 들어, 워커가 fetch한 링크가 seen_urls에 있는지 확인하려 한다면, 그리고 그 워커가 그것을 queue에 넣고 seen_urls로 더하지 않는다면. 만약 이것이 다른 두 operation에 사이에서 interrupt되었다면, 다른 워커들은, 같은 링크를 다른페이지에서 파싱할 것이고, 또한 그것이 seen_urls에 존재하지 않는다고 볼 것이다. 그리고 그것을 Queue에 더할 것이다. 그러면 동일한 링크가 Queue에 두번 있는 것이고, 중복된 일과 잘못된 통계를 수집할 것이다. 그러나 코루틴은 오직 yield from 인터럽션에만 취약하다. 이것이 코루틴 코드를 races에 멀티스레딩 코드보다 덜 발생하도록 만드는 주요 차이점이다.: 멀티스레딩 코드는 lock을 취득하는 것을 통해서 명시적으로 공유자원에 접근해야한다. 그렇지 않으면 방해 받을 수 있다. 파이썬 코루틴은 기본적으로 방해 받지 않으며, 명시적으로 yield 할때만, 컨트롤을 양보한다. 우리는 우리가 Callback기반 프로그램에서 그랬던 것처럼 fetcher클래스가 필요하지 않다. 그 클래스는 콜백이 결여된 것이었다.: 그들의 지역변수가 call들 사이에서 유지되지 않았기 때문에, 그들은 I\\O를 기다리는 동안 상태를 저장할 장소가 필요하다. 그러나 fetch코루틴은 그것의 상태를 일반적인 서브루틴처럼 저장할 수 있기 때문에, class는 더이상 필요하지 않다. fetch가 work콜러에게 response를 돌려주는 마무리를 할 때, work메서드는 task_done을 queue에서 호출하여, 다음 URL을 queue에서 가져온다. fetch가 새로운 링크를 큐에 넣을 때, 완료되지 않은 tasks의 count를 증가시키고, q.join을 기다리고 있는 메인 코루틴을 정지된 상태로 유지 시킨다. 그러나, 거기에 unseen링크가 없고, 그것이 queue에서 마지막 URL이었을 때, work는 task_done을 호출하고, 완료되지 않은 tasks의 count는 0으로 떨어진다. 이벤트는 join을 재개시키고, 메인 코루틴은 종료된다. 워커들과 메인 코루틴은 정렬하는 Queue코드는 아래와 같다. 123456789101112131415class Queue: def __init__(self): self._join_future = Future() self._unfinished_tasks = 0 def put_nowait(self, item): self._unfinished_tasks = 0 def task_done(self): self._unfinished_tasks -= 1 if !(self._unfinished_tasks): self._join_future.set_result(None) @coroutine def join(self): if self._unfinished_tasks: yield from self._join_future 메인 코루틴, crawl은 join에서 yield from된다. 따라서 마지막 워커가 unfinished_tasks를 0으로 처리하면, 예약한 시그널이 발생해서 crawl로 재진입하고, 종료한다. loop.run_until_complete(self.crawler.crawl()) 어떻게 프로그램이 종료 되는가? crawl이 제너레이터 함수 이기 때문에, 이것을 호출하는 것은 제너레이터를 리턴한다. 제너레이터를 진행시키기 위해서, asyncio는 이것을 task안에 래핑한다. 1234567891011121314class StopError(BaseException): passdef stop_callback(future): raise StopErrorclass EventLoop: def run_until_complete(self, coro): task = Task(coro) task.add_done_callback(stop_callback) try: self.run_forever() except StopError: pass Task가 종료될 떄, 그것을 StopError을 raise하고, 루프는 그것을 시그널로 간주한다. 그러면 이것은 무엇인가? Task는 add_done_callback과 result라 불리는 메서드가 있다? 당신은 task가 future를 닮았다고 생각할 수 있다. 당신의 직감을 옳다. 우리는 당신에게 숨긴 Task class에 대한 세부 디테일을 가져와야 한다. task는 future이다. 12class Task(Future): \"\"\"A coroutine wrapped in a Future.\"\"\" 일반적으로 future은 누군가가 set_result를 할때 해제된다. 그러나 task는 그것의 코루틴이 멈출 때, 자신을 해제시킨다. generator가 값을 리턴할 때, 그것을 특별한 StopIteration예외를 발생시켰던 것을 기억하라. 123456789101112131415161718class Task(Future): def __init__(self, coro): self.coro = coro f = Future() f.set_result(None) self.step(f) def step(self, future): try: next_future = self.coro.send(future.result) except CancelledError: self.cancelled =True return except StopIteration as exc: self.set_result(exc.value) return next_future.add_done_callback(self.step) 따라서 이벤트 루프가 task.add_done_callback(stop_callback)을 호출할 때, 콜백은 StopError를 루프내에서 발생시킨다. 루프는 멈추고 콜 스택은, run_util_complete로 복귀(Unwound to)한다. 우리 프로그램은 종료 되었다. 결론 꽤 자주, 현대적인 프로그램은 CPU-bound하기 보다 I/O-bound한 일이 잦다. 그러한 프로그램에서 파이썬의 쓰레드들은, 두 곳에서 전부 최악의 경우이다.: GIL에 의해서 락을 가진 쓰레드만을 인터프리터가 실행하기 때문에, 병행-컴퓨팅을 방지하고, 선점형 스위칭은 레이스컨디션을 매우 줄여버린다. Async는 가끔 정답인 패턴이다. 하지만 콜백기반 비동기 코드가 커질수록, 이것은 종잡을 수 없이 난잡해지는 경항이 있다. 코루틴들은 쌍당한 대처법이라 할 수 있다. 그들은 자신들을 제대로 된 에러 핸들링과, 스택트레이싱을 통해서, 서브코루틴으로 자연스럽게 구성해낸다. 우리가 yield from문을 슬쩍 애매하게 본다면, 코루틴은 마치 기존의 blocking-io인 쓰레드가 하는 것처럼 보인다. 우리는 멀티쓰레딩-프로그래밍처럼 코루틴들을 정렬해서, 고전적인 패턴처럼 보이게 할 수도 있다. 그러므로, 콜백들에 비교해서 코루틴은 멀티쓰레딩으로 익숙한 코더들에게 익숙한 관용구이다. 그러나 우리가, yield from문을 제대로 인식하려 할 때, 우리는 코루틴이 실행 컨텍스트를 넘겨주는 시기 같이, 유심히 체크해야할 포인트 들이 있다. 쓰레드와 달리 코루틴들은 우리의 코드를 interrupted될 수 있는 것으로 보여준다. Glyph Lefkowitz가 적었듯이, “쓰레드들은 지역적인 추리를 어렵게 한다. 그리고 지역적인 이해는 소프트웨어 개발에서 가장 중요한 것중 하나이다.” 명시적으로 yielding하는 것은 반면에 모든 시스템을 시험하는 것이 아닌 루틴으로써, 루틴을 시험하는 것으로 루틴의 행동을 이해할 수 있도록 한다. (중략…간편하게 사용할 수 있는 인터페이스가 버전업이 되면서 제공되었다 라는 등의 이야기, async def, await) 이런 발전들에 불구하고, 기존의 코어 개념은 그대로 있다. 파이썬의 새로운 내장 코루틴들은, 통상적으로, 제너레이터와 구분되지만, 매우 유사하게 동작한다. 게다가, 그들은 파이썬 인터프리터 내에서, 그 실행부를 공유한다. Task, Future, event loop같은 것들은 asyncio에서 계속 그들의 역할을 맞을 것이다. 이제 당신은 어떻게 asyncio 코루틴이 동작하는지 알았으니, 세부적인 것은 조금 잊어도 좋다. 기계공은 매끈한 인터페이스 뒤에 dlTek. 그러나 당신이 구조를 이해한다면, 너의 코드를 요즘 비동기 환경에서 더욱 효과적이고 올바르게 강화해줄 것이다.","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"python post","slug":"python-post","permalink":"https://codenamenadja.github.io/tags/python-post/"}]},{"title":"pointer in python 번역","slug":"python/post/pointer_in_python","date":"2019-08-30T11:21:27.000Z","updated":"2021-01-25T08:36:06.087Z","comments":true,"path":"2019/08/30/python/post/pointer_in_python/","link":"","permalink":"https://codenamenadja.github.io/2019/08/30/python/post/pointer_in_python/","excerpt":"","text":"Pointer in python: What’s the point 번역, 정리글입니다. index Why Doesn’t Python have Pointers? Objects in Python Immutables vs Mutable objects Understading Variables Variables in C Names in Python A Note on Intern Objects in Python Simulating Pointers in Python [Using Mutable Types as Pointers] [Using Python Objects] Real Pointers With ctypes 왜_파이썬에는_포인터가_없는가 포인터가 파이썬에서 네이티브하게 존재할 수 는 없을까? 잘은 모르지만 아마 가능할 것이다. 그러나 파이썬의 포인터는 Zen of Python에 위배되는 것처럼 보인다. Zen of python / Cpython의 구현의 주요 기여자인 Tim peters의 20개 명언 Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren’t special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you’re Dutch. Now is better than never. Although never is often better than right now. If the implementation is hard to explain, it’s a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea – let’s do more of those! 포인터는 명시적인 변화보다 내재적인 변화를 촉구한다. 때로, 그들은 단순하기보다 복잡하고(특히 초보자들에게,) 더욱 나쁜것은, 그들은 당신이 직접 하기를 원하고, 또는 메모리에 접근하기 때문에 매우 위험할 수 있다. 파이썬은 메모리 주소 같은 디테일의 구현을 추상화하려한다. 파이썬은 때로 속도보다 사용감에 집중한다. 결과적으로 파이썬에서 포인터는 말이 되지 않는다. 그러나 파이썬은 결과적으로 포인터를 사용하는데 있어서의 이점을 준다. 파이썬의 포인터를 이해하는 것은 파이썬 구현부의 디테일을 돌아볼 필요가 있다. Immutable vs Mutable objects Python variables/names 를 이해할 필요가 있다. Object_in_Python 파이썬의 모든 자료구조는 기본 object를 상속받는 인스턴스 클래스들이다. 각 객체는 3개의 데이터를 최소한 포함한다. Reference count Type Value 레퍼런스 카운트는 메모리 관리를 위한 것이고, 파이썬의 메모리 관리를 이해하고 싶다면, Memory Management in Python를 읽어보면 좋다. Python object PyObject라고 불리는 구조체가 있다. 모든 Cpython의 객체가 사용하는 것이다. PyObject는 모든 객체의 부모이며, 아래의 2요소를 포함한다. ob_refcnt: 레퍼런스 카운트 ob_type: 어떤 타입에 관한 포인터 각 객체는 그들 각자의 object-specific memory allocator을 가지고 있다. 그것은 그들 객체를 위한 메모리를 가져온다. 각 객체는 또한 object-specific memory deallocator을 가지고 있다. 그것은 그들의 객체들이 refcount에 따라 메모리를 free한다. 그러나 메모리를 할당하고 해제하는 것에 있어서, 중요한 요소가 있다. 메모리는 컴퓨터의 공유자원이며, 두 개의 프로세스가 동시 접근하려고 하면 문제가 생긴다는 것이다. GIL은 공유자원 Race Condition에 대한 해결책중 하나이다. GIL GIL은 짧게 말해서 mutex이며, 하나의 쓰래드 만이 파이썬 인터프리터를 제어할 수 있도록 한다. 이 말은, 하나의 쓰레드만이 실행부의 상태에 있을 수 있다는 것이다. GIL의 부하는 싱글쓰레드 프로그램을 하는 사람들에게는 나타나지 않는다. 이것은 CPU-bound한 문제, 멀티쓰레딩 코드 에서 bottleneck이 될 수 있는 것이다. GIL이 파이썬의 어떤 문제를 해결해 주는가? 파이썬은 메모리 관리를 위해 refcount를 사용한다. 그것은 파이썬에서 생성된 objects 구조체가 refcount변수를 가지고 있고 추적되고 있다는 것을 의미한다. 이것이 0이 되었을 때, 해당 객체가 점유하는 메모리는 해제된다. 123456&gt; &gt; import sys&gt; &gt; a = []&gt; &gt; b = a&gt; &gt; sys.getrefcount(a)&gt; &gt; #3&gt; &gt; a에 처음 할당된 메모리 주소에 대해서 a,b가 참조하고, sys.getrefcount에 전달된 argument가 순간적으로 참조하고 해제된다. 그래서 3이 나온다. GIL로 돌아가서, 문제는 Refcount변수가 race-condition에서 보호받아야 한다는 것이다. 만약 2개의 쓰레드가 동시에 그 수치를 늘리거나 줄이면, 메모리 누수를 초래해서, 절때 해제되지 않거나, 원치 않게 해제된다는 것이다. 이 refcount변수는 locks를 더함으로 안전하게 보관되어, 쓰레드에 의해 공유되는 모든 자료구조가 일관적이지 않게 수정되는 것을 막는다. 하지만 각 객체에 lock을 더하는 것은 너무 많은 lock이 존재하여 다른 문제를 발생 시킬 수 있다는 것을 의미한다. Deadlocks (데드락은 하나이상의 락이 있을 때 생길 수 있다.) 특정 자원에 2쓰레드 혹은 프로세스가 순차적으로 접근하려고 하면, 두번째 쓰레드는 첫번째 쓰레드가 락을 해제할 때까지 끝없이 기다리게 된다. 다른 문제로는 락을 반복적으로 생성하고 해제하는 것으로 인한 퍼포먼스 저하가 있다. GIL은 인터프리터에 대한 단일 lock을 의미하며, GIL은 바이트코드를 실행하려면 인터프리터 잠금을 획득해야 한다는 규칙을 추가한다. 이것이 deadlocks를 방지하고, 그렇게 많은 퍼포먼스를 요구하지 않도록 한다.(성능적 오버헤드를 방지) 하지만 Cpu-bound한 파이썬 프로그램을 싱글쓰레드로 효과적으로 만들어 버린다. 하지만 Refcount와 GC를 이용한 GIL을 사용하지 않는 경우도 있다 반면에 그것을 그러한 언어들이 때로 GIL이 효과적으로 싱글 쓰레드에 대해서 퍼포먼스 이점을 주는 것을 다른 보상으로 채워야 한다는 것을 의마한다, 예를 들면 JIT컴파일러 같은 것을 통해서. 왜 GIL이 해결책으로 선택 되었는가? Larry Hastings의 말에 따르면, GIL을 선택한 것은 파이썬을 오늘날 만큼 유명하게 만든 하나의 이유라고 한다. 파이썬은 OS가 쓰레드의 개념이 없을때 생겨 났다. 파이썬은 쉽게 사용될 수 있고 빠르게 개발할 수 있도록 만들어 졌다. 이후 파이썬의 일관적이지 않은 변화를 예방하기 위해서, 이 파이썬을 지탱하는 C라이브러리는, Thread-safe 메모리 관리를 필요로 했고, 그래서 GIL이 생겨 났다. GIL은 실행하기 쉽고 파이썬에 쉽게 더해졌다. 이것은 싱글 쓰레드 상에서 하나의 락만이 관리되야한다는 점에 의해 퍼포먼스 향상을 가져왔다. 쓰레드세이프 하지 않은 C라이브러리 들은 더욱 통합하기 쉬워졌다. 그렇게 통합된 C라이브러리들이 파이썬이 여러 커뮤니티에서 손쉽게 적용될 수 있었던 이유이기도 하다. 니가 볼 수 있듯이, GIL은 Cpython개발자들이 초기 파이썬에서 복잡한 문제를 빠르게 해결할 수 있는 방법이었다. 왜 GIL은 아직 사라지 않았는가? GIL은 명백하게 사라질 수 있고, 그런 시도들이 많았으나, 그러한 시도들은 GIL이 주는 해결책에 의존하는 C-extensions를 망가뜨렸다. 물론, GIL을 대체하는 해결책도 있지만, 하지만 그들의 일부는 싱글쓰레드, 멀티쓰레드/IO바운드 프로그램에 성능을 떨어뜨렸고, 일부는 너무 어려웠다. 어찌됐든, 지금 존재하는 파이썬 프로그램이 새로운 버전이 나옴으로 더 느려지는 바라는 사람은 없지 않은가? 귀도 반 로썸이 It isn’t Easy to Remove the GIL의 포스트로 2007년에 해명을 했다. “나는 Py3k에 대한 패치들을 환영할 것이다. 만약! 퍼포먼스가 떨어지지 않는다면.” Garbage Collection 파이썬의 모든 구조체에는 Refcount라는 것이 있고 아래와 같은 경우들에 증가한다. 12345678&gt; numbers = [1,2,3]&gt; more_numbers = numbers&gt; # refcount = 2&gt; total = sum(numbers) # refcount = 3&gt; # refcount = 2&gt; matrix = [numbers, numbers]&gt; # refcount = 4&gt; 파이썬은 현재 오브젝트의 러퍼런스 카운트를 sys모듈을 통해서 확인 할 수 있게 해준다. sys.getrefcount(object) 그렇지만 deallocate기능을 사용하게 되면, 어떻게 메모리가 free되는가? 어떻게 객체가 그 과정을 거치는가? Cpython의 메모리 관리자 위에서 말했듯, 하드웨어에서 Cpython으로 이어지는 추상화 계층이 있다. OS는 실제 물리메모리를 추상화 하고 가상메모리 계층을 만들어 어플리케이션이 접촉할 수 있도록 한다. OS에 따른 메모리 관리자들은 파이썬 프로세스를 위한 메모리 청크를 조각해낸다. 파이썬은 일단 운영체제에 malloc함수로 메모리할당을 요구한 다는 것을 알아야한다. 파이썬 프로세스는 내부적인 사용을 위한 메모리공간과 비객체적인 메모리를 위한 공간을 사용한다. 마지막 공간은 object storage를 위해 사용된다.(int, dict, and elses) 메모리 관리의 구현부인 Cpython source를 참고해도 좋다. 객체 메모리 공간에 파이썬은 object allocator을 두고 있다. 그것이 대부분의 기능이 이루어 지는 곳이다. 흔한 경우로, list나 int같은 파이썬 객체를 생성하거나 삭제하는 것은 그렇게 많은 데이터를 필요로 하지 않는다. 따라서, 메모리 할당자는 작은 양의 데이터 많으로 잘 작동하도록 되어 있다. 그리고 그것은 실제로 그 객체가 필요해지기 전까지 메모리에 할당하지 않으려고 한다. 위에 Cpython source구현부에서는, 객체 할당자를 “작은 블록을 위한 빠르고, 특별한 용도를 위한 메모리 할당자이다. 그리고 일반적인 목적의 malloc위에서 실행된다.” 라고 설명한다. 이제 Cpython의 메모리 할당 전략에 대해 살펴볼 것 이다. Arenas는 페이지 범위내에서 가장 크고 정렬된 단위이다. 페이지 영역은 고정된 길이의 연속적인 메모리 청크라고 할 수 있다.(실제 주소가 달라도, 가상주소상으론 연속적인 주소 그리고 실제주소는 랜덤 엑세스가 가능하기 때문에,) 파이썬은 시스템의 페이지 사이즈를 256kb정도 라고 가정한다.(내 컴퓨터 페이지 사이즈는 4kb이다) Arenas내부에는 pools가 있고, 그것은 하나의 가상 메모리 페이지이다. 그리고 풀은 실제로 동적 할당으로서 작은 메모리 사이즈로 조각난다. 풀 내부에 모든 블록은 동일한 size class이다. size class는 특정 블록 사이즈를 정의하고, 요청한 정보가 들어간다. 아래 차트는 소스코드에서 직접 가져온 커멘트이다. Request in bytes Size of allocated block Size class Index 1-8 8 0 9-16 16 1 17-24 24 2 25-32 32 3 … … . 505-512 512 63 예를 들어 42바이트의 데이터가 요청되었다면, 데이터는 48-byte블록까지 차지하게 될 것이다. Pools Pools는 단일 size class로부터 조합된 블록들로 구성된다. 각 풀은 double-linked list로 같은 size class로 구성된 다른 풀들로 연결되어 있다. 그런 방식에서 볼 때, 더블링크드리스트를 통한 알고리즘은 쉽게 블록사이즈 만큼 요구된 공간을 다른 풀들 사이에서 찾을 수 있는 것이다. 사용된 Pool들의 리스트는 할당가능한 블록들이 남아있는 모든 풀들을 추적한다. 어떤 블록 사이즈가 요구되면, 알고리즘이 usedpools리스트를 확인하여, 블록사이즈가 할당 가능한 pool list를 반환한다. Pool들은 각자 3가지 상태가 존재한다. used: 저장가능한 avaliable blocks를 지님. full: 모든 블록들에 데이터가 할당되었음. empty: 아무런 데이터가 없고, 언제든 어떤 사이즈의 블록들도 할당 할 수 있다. freepools리스트가 비어있는 상태의 풀들의 리스트를 추적한다. 그러면 언제 비어있는 풀은 사용되는 가? 네가 8byte의 청크 메모리를 할당해야 한다고 치자. 만약 거기에 8바이트를 할당 할 수 있는 usedpools가 없다면, 새로운 empty pool애 8바이트가 초기화 될 것이다. 이 새로운 pool은 usedpools리스트로 더해지고, 이것은 이후에 올 요청의 첫 대상이 된다. 마찬가지로 full상태인 pool이 일부 블록을 해제한다면, usedpools리스트로 옮겨진다. Blocks 아까 블록에 실제로 데이터가 할당 되는 것은 해당 블록의 데이터가 실제로 필요해질 떄 그럴 것이라고 했었다. 그러한 이유로 blocks도 3가지 상태를 가진다. untoched: 아직 할당 되지 않은 메모리블록. free: 데이터가 할당된 블록이나, Cpython에 의해서 free되어서 현재 유효한 데이터가 들어있지 않은 블록. allocatd: 실제로 데이터를 가지고 있는 블록. freeblock 포인터는 single linked list인 비어있는 메모리 블록의 list를 가르킨다. 더 많은 길이의 블록이 필요하면, 메모리 할당자는 해당 풀 안에 untouched블록을 더해서 돌려줄 것이다. 할당되어 있던 블록(allocated)가 해제되어 free상태가 되면, single linked list인 Free block list의 head로 오게 된다. Arenas 아레나들은 pools를 포함한다. pools와 blocks는 3가지 상태가 있었던 반면, 아레나들은 명시적인 상태가 없다. 아레나들은 반면에 usable_arenas라 불리는 Double linked list로 구성 및 조직된다. 이 리스트는 freepools를 얼마 가지고 있냐에 따라 정렬된다. freepools가 적을수록 해당 아레나는 리스트의 head쪽으로 오게 된다. 그런데 왜 가장 사용가능한 데이터가 많은 arena가 아니라 그 반대인가? 그곳에 메모리해제의 개념이 있다. block이 free하게 전환되면, 그 메모리는 실제로 OS에게로 돌아가는 게 아니라, 파이썬 프로세스가 그 공간을 계속 점유하고, 다음 데이터를 위해 사용할 것이다. 실제로 메모리를 해제한다면 OS에게 돌려주는 것을 의미한다. 아레나야 말로 유일하게 정말 해제될 수 있는 메모리이다. 따라서, 가장 많이 비어있는 데이터가 가장 쉽게 empty arena가 될 수 있도록 그러한 것이다. 그 방식에서 메모리의 청크는 실제로 해제되고, 파이썬 프로세스의 점유나 감시에서 벗어날 수 있도록 돕는 것이다. type은 Cpython계층에서 타입안전성을 런타임에서 보장할 때 사용된다. 마지막으로 value가 실제로 object와 관련된 페이로드를 말한다. 모든 객체가 동일한 객체는 아니나, 그런 단순한 사실 외에도 당신이 꼭 알아야할 더 중요할 구분이 있다. mutable vs immutable객체 이다. 이 두 타입의 객체의 차이를 이해하는 것은, 파이썬의 포인터의 초반부를 이해하는데 굉장히 도움이 된다. mutable_vs_immutable type immutable? int Yes float Yes bool Yes complex Yes tuple Yes frozenset Yes str Yes list No set No dict No 위에 보이듯 대부분 사용되는 원시타입들은 immutable하다. 아래 기능을 사용하여 그것을 검증할 수 있다. id(): object’s memory address를 반환 is: 두 개체가 동일한 메모리 주소를 가지고 있다면 True를 반환. Understanding_Variables 파이썬의 변수는 구조적으로 C, C++과 다르다. 사실, 파이썬은 변수가 존재 하지 않고, names가 존재한다. 이것은 현학적으로 보일 수 있다. 대부분의 경우에 파이썬의 names를 변수로 이해하는 경우가 많다. 그러나 차이를 아는 것은 중요하다. C에서 변수와 파이썬의 name을 대조하는 것으로 이해하자. C 1234567891011int x = 2337;// x.Location = 0x7f1// x.value = 2337x = 2338;// x.Location = 0x7f1// x.value = 2338int y = x;// y.Location = 0x7f5// y.value = 2338 Python 123x = 2337x = 2338y = x Create PyObject set The typecode to integer to PyObject Set the value to 2337 for the PyObject Create a name called x Point x to the new PyObject Increase the refcount of the PyObject by 1 name PyObject-Type PyObject-Value PyObject_RefCount x integer 2337 1-&gt;0() x integer 2338 0-&gt;1 y integer 2338 1-&gt;2 1234\"\"\"&gt;&gt;&gt; y is xTrue\"\"\" 파이썬에서 너는 변수에 값을 할당하는 것이 아니라, 이름을 Pyobject_레퍼런스에 할당하는 것이다. A_Note_on_Intern_objects_in_python 1234567891011\"\"\"&gt;&gt;&gt; x = 1000&gt;&gt;&gt; y = 1000&gt;&gt;&gt; x is yTrue&gt;&gt;&gt; x = 1000&gt;&gt;&gt; y = 499 + 501&gt;&gt;&gt; x is yFalse\"\"\" Create PyObject(1000) Assign the name x to that object Create PyObject(499) Create PyObject(501) Add these two Objects Create new PyObject(1000) Assign the name y to that object 위에 과정은 REPL에서만 위처럼 동작한다. 만약 파일로서 만들고 실행하면, x is y라는 것은 True가 나올 것이다. 이것은 CPython컴파일러가 영리하게 처리하기 때문에 그렇다. Cpython컴파일러의 Optimizations를 peephole optimizations으로 수행하려 하는데, 실행부를 최대한 줄이려고 하는 것이다. SourceCode on peephole.c를 확인가능하다. 123456\"\"\"&gt;&gt;&gt; x = 20&gt;&gt;&gt; y = 19 + 1&gt;&gt;&gt; x is yTrue\"\"\" 위의 결과가 위의 결과가 다른 이유는 파이썬의 이미 존재하는 객체를 재사용 하는 특성 때문이다.(interned object) 파이썬은 일부의 셋의 객체를 실행할 떄 미리 만들고, global namespace에 저장해놓고 사용한다. CPython 3.7에 따르면 Interned Object는 아래를 따른다. integer numbers between -5 and 256 String taht contains ASCII letters , digits, or underscores only 위의 범위에 포함되는 값들의 타입을 생성하면, 내제되있는 미리 생성된 global namespace에 존재하는 것을 레퍼런싱 하도록 한다. 그것은 물론 Cpython based 3.7이상이라면, 프로세스가 종료될 때까지 유효하다. 이를 통해서 작은 값들은 미리 존재하는 것들을 사용해서(자주 사용될 것으로 예상되는 범위의 것들을) 메모리 할당을 줄인다. 123456789101112131415\"\"\"&gt;&gt;&gt; s1 = \"realpython\"&gt;&gt;&gt; s2 = \"realpython\"&gt;&gt;&gt; s1 is s2True&gt;&gt;&gt; s1 = \"Real Python!\"&gt;&gt;&gt; s2 = \"Real Python!\"&gt;&gt;&gt; s1 is s2False&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.intern(s2)\"Real Python\"\"\"\" 위의경우는 !(exclumation mark)때문에 생기는 현상이다. 이 문자열은 내부 저장된 값이 아닌 것을 사용하기 때문에, 완전히 새로운 Pyobject를 생성하는 것이다. simulating_pointers_in_python 파이썬에 포인터가 구현되어 있지 않다고 해서, 포인터를 사용하는 것에서 이점을 얻을 수 없다는 것은 아니다. 사실 포인터를 흉내낼 수 있는 여러 방법이 있다. Using mutable types as pointers Using custom Python objects using_mutable_types_as_pointers 123456789101112131415161718192021222324# mutable인 List 사용해서 흉내\"\"\"&gt;&gt;&gt; def add_one(x):&gt;&gt;&gt; x[0] += 1&gt;&gt;&gt; #&gt;&gt;&gt; y = [2337]&gt;&gt;&gt; add_one(y)&gt;&gt;&gt; y[0]2338\"\"\"# mutable인 dict 사용해서 흉내\"\"\"&gt;&gt;&gt; counters = &#123;\"func_calls\": 0&#125;&gt;&gt;&gt; def bar():&gt;&gt;&gt; counters[\"func_calls\"] += 1&gt;&gt;&gt; #&gt;&gt;&gt; def foo():&gt;&gt;&gt; counters[\"func_calls\"] += 1&gt;&gt;&gt; bar()&gt;&gt;&gt; #&gt;&gt;&gt; foo()&gt;&gt;&gt; counters[\"func_calls\"]2\"\"\" 이것은 C나 C의 실제 포인터가 아니라, 흉내내는 것이기 때문에, 논법적으로 봤을때, 이 행동으로 조정하는 것은 C나 C의 포인터를 다루는 것보다 훨씬 비용이 비싸다. using_python_objects 1234567891011121314151617181920class Metrics(object): def __init__(self): self._metrics = &#123; \"func_calls\": 0, \"cat_pictures_served\": 0, &#125; @property def func_calls(self): return self._metrics[\"func_calls\"] @property def cat_pictures_served(self): return self._metrics[\"cat_pictures_served\"] def inc_func_calls(self): self._metrics[\"func_calls\"] += 1 def inc_cat_pics(self): self._metrics[\"cat_pictures_served\"] += 1 func_calls나 cat_pictures_served속성자를 통해서 포인터처럼 접근 할 수 있지만, 이것 또한 그저 흉내이고, 이것은 자주 변경되는 것을 처리할 때 유용한 방식이다. real_pointers_with_ctypes ctypes모듈을 사용하면 Cpython에서 포인터를 사용할 수 있다. ctypes에 대해서 사용법이 궁금하다면 이 포스트가 도움이 될 것이다. c함수 파일 123void add_one(int *x) &#123; *x += 1;&#125; 컴파일 12gcc -c -Wall -Werror -fpic add.cgcc -shared -o libadd1.so add.o python에서 사용 1234567891011121314151617181920212223242526import ctypesadd_lib = ctypes.CDLL(\"..libadd1.so\")\"\"\"&gt;&gt;&gt; add_lib.add_one&lt;_FuncPtr object at 0x7f9f3b8852a0&gt;\"\"\"add_one = add_lib.add_oneadd_one.argtypes = [ctypes.POINTER(ctypes.c_int)]\"\"\"&gt;&gt;&gt; add_one(1)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;ctypes.ArgumentError: argument 1: &lt;class 'TypeError'&gt;: \\expected LP_c_int instance instead of int\"\"\"x = ctypes.c_int()\"\"\"&gt;&gt;&gt; xc_int(0)&gt;&gt;&gt; add_one(ctypes.byref(x))998793640&gt;&gt;&gt; xc_int(1)\"\"\"","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"python post","slug":"python-post","permalink":"https://codenamenadja.github.io/tags/python-post/"}]},{"title":"소켓과 스트림을 이해하기 위한 고급 IO함수","slug":"c/io_functions_advanced","date":"2019-08-27T08:23:28.000Z","updated":"2021-01-25T08:36:06.071Z","comments":true,"path":"2019/08/27/c/io_functions_advanced/","link":"","permalink":"https://codenamenadja.github.io/2019/08/27/c/io_functions_advanced/","excerpt":"","text":"index 버퍼링 제어하기(linux_gcc buffer) 스레드 세이프(thread safe) 벡터 입출력(Vector IO) 버퍼링_제어하기 표준 입출력은 세가지 유형으로 사용자 버퍼링을 구현하고, 버퍼의 유형과 크기를 다룰 수 있는 인터페이스를 제공한다. 각각의 버퍼링 타입은 저마다의 목적이 있으니 상황에 맞게 사용해야 한다. type desc ie 버퍼 미사용 사용자 버퍼를 사용하지않고 바로 커널로 데이터를 보낸다. 딱히 이점이 없다. 표준에러stderr 외에 거의 사용하지 않음 행 버퍼 행 단위 버퍼링 수행. 개행문자가 나타나면 버퍼 내용을 커널로 보낸다. 화면출력을 위한 스트림일 경우 유용, 화면출력 메세지는 개행문자로 구분되기 때문이다. 표준 출력stdin 처럼 터미널에 연결된 스트림에서 기본적으로 사용하는 버퍼링 방식 블록 버퍼 고정된 바이트수로 표현되는 블록단위로 버퍼링을 수행. 파일에 적합하여 기본적으로 파일과 관련된 모든 스트림은 블록버퍼를 사용한다. 파일 스트림(FILE). 표준입출력 에서는 블록버퍼링을 full버퍼링이라고 한다. 대부분 기본유형의 버퍼링을 사용하는 편이 올바르고 최선이나, 표준입출력은 버퍼링 방식을 제어할 수 있는 인터페이스를 제공한다. 123#include &lt;stdio.h&gt;int setvbuf(FILE *stream, char *buf, int mode, size_t size); stream의 버퍼링 유형을 mode로 설정한다. mode IONF: 버퍼 미사용 IOLBF: 행 버퍼 IOFBF: 블록 버퍼 buf와 size를 무시하는 IONF를 제외하고, 나머지는 size바이트 크기의 버퍼를 가리키는 buf를 주어진 stream을 위한 버퍼로 사용한다. 만약 buf가 NULL이면, glibc가 자동적으로 지정된 크기만큼 메모리를 할당한다. (파일 저장을 생각하면 아마 1024이상 4kb, 8kb 정도가 아닐까?) setvbuf함수는 스트림을 연다음, 다른 연산(읽기 쓰기 등.)수행하기 전에 호출해야 한다. 성공시 0을 반환, 실패시 그 외의 값을 반환. 제공된 버퍼는 스트림이 닫힐 때까지 반드시 존재해야한다. 흔히 스트림을 받기 전에 끝나는 스코프 내부의 자동변수로 버퍼를 선언하는 실수를 한다. 특히 main()에서 지역변수로 버퍼를 만든 후, 스트림을 명시적을 닫지 않는 경우. 아래 코드에는 버그가 있다. 123456789101112#include &lt;stdio.h&gt;int main(void)&#123; char buf[BUFSIZ]; // stdout을 bufsize크게에 맞춰 블록 버퍼로 설정한다. setvbuf(stdout, buf, _IOFBF, BUFSIZ); printf(\"ARRR!\\n\"); // stdout에 버퍼에 들어가고 개행문자를 확인함과 stdout buf에서 fflush되서 화면에 출력된다. return 0; // buf는 스코프를 벗어나고 해제된다. 하지만 stdout을 닫지 않았다!!&#125; 일반적으로 개발자들은 스트림을 다룰 때 버퍼링에 대해 고민할 필요가 없다. 표준 에러를 제외하고, 터미널은 행 버퍼링으로 동작하는게 맞고 파일은 블록버퍼링을 사용하는게 맞다. 그게 전부다. 블록 버퍼링에서 버퍼의 기본크기는 BUFSIZ이며 &lt;stdio.h&gt;에 정의되어 있다. 이 값은 일반적인 블록크기의 정수배인 최적의 값이다. (64비트 운영체제인 내 컴퓨터에서는 8192bytes) 스레드_세이프:Thread_safe pass 벡터_입출력Vector_io 버퍼세그먼트의 집합을 벡터라 한다. 한번의 시스템콜을 사용해서, 여러개의 버퍼벡터(세그먼트)에 쓰거나, 읽어들일 때 사용. 백터입출력은 선형입출력 메서드에 비해 다음과 같은 장점을 가지고 있다. 자연스러운 코딩 패턴 미리 정의된 구조체가 여러 필드에 걸쳐 데이터가 분리되어 있는 경우, 벡터입출력을 사용하면 직관적인 방법으로 조작할 수 있다. 효율 하나의 벡터 입출력 연산은 여러번의 선형 입출력 연산을 대체할 수 있다. 성능 시스템콜의 횟수를 줄일 뿐 아니라, 내부적으로 선현 입출력 구현에 비해 좀 더 최적화된 구현을 제공. readv_writev read(), write()와 동일하게 동작하지만, 여러 개의 버퍼를 사용한다는 점에서 구분된다. 123456#include &lt;sys/uio.h&gt;struct iovec&#123; void *iov_base; // 버퍼의 시작 포인터 size_t iov_len; // 버퍼의 바이트 수&#125; readv() 파일디스크립터 fd에서 데이터를 읽어 count개수만큼 iov버퍼에 저장. 123#include &lt;sys/uio.h&gt;ssize_t readv(int fd, const struct iovec *iov, int count); writev() count개수만큼의 iov버퍼에 있는 데이터를 파일디스크립터fd에 저장. 123#include &lt;sys/uio.h&gt;ssize_t writev(int fd, const struct iovec *iov, int count); 둘 다 성공시 읽거나 쓴 바이트 개수를 반환한다. 즉, count * iov_len과 같아야 하며, 에러가 발생하면 -1을 반환하고 errno를 설정한다. 표준에러에서 추가로 2가지 에러의 상황이 있다. 반환값의 자료형이 ssize_t이기 때문에 SSZIE_MAX보다 큰 값을 전달했다고 리턴하면 데이터는 전송되지 않고, -1을 반환하며 errno는 EINVAL로 설정한다. (2**32-1을 바이트 취급하면 4gb) POSIX는 count가 반드시 0보다 크고 IOV_MAX와 같거나 작아야한다. 리눅스에서 IOV_MAX값은 1024로 정의. 만약 count가 0이면 0을 반환하고(0회 실행했으니 데이터전송량 0바이트), IOV_MAX보다 크다면, 데이터는 전송되지 않고 -1을 반환하며 errno는 EINVAL로 설정한다. 버프의 기본사이즈 인 블록사이즈(메모리 접근 최소 단위)가 내 운영체제에서는 8kb 이기 때문에, 1024bytes(를 부분 버퍼로)*8(개)이면 데이터가 딱 맞게 한번의 시스템 콜로 1kb 8개 버퍼를 처리하는 것이다. writev_예시 Code 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include &lt;stdio.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;#include &lt;string.h&gt;#include &lt;sys/uio.h&gt;#include &lt;unistd.h&gt;int main()&#123; struct iovec iov[3]; ssize_t nr; int fd; int i; int res; char *buf[] = &#123; \"The term buccaneer comes from the word boucan.\\n\", \"A boucan is a wooden frame used for cooking meat.\\n\", \"Buccaneer is the West Indies name for a pirate.\\n\"&#125;; fd = open(\"buccaneer.txt\", O_WRONLY | O_CREAT | O_TRUNC); if (fd == -1) &#123; perror(\"open\"); return 1; &#125;; for (i = 0; i &lt; 3; i++) &#123; iov[i].iov_base = buf[i]; iov[i].iov_len = strlen(buf[i]) + 1; printf(\"len: %li\\n\", strlen(buf[i])); &#125;; nr = writev(fd, iov, 3); if (nr == -1) &#123; perror(\"writev\"); return 1; &#125;; printf(\"wrote %ld bytes\\n\", nr); res = close(fd); if (res == -1) &#123; perror(\"close\"); return 1; &#125;; return 0;&#125; output 1234len: 47len: 50len: 48wrote 148 bytes strace 1234567891011121314151617181920212223242526272829303132333435execve(\"./writev.out\", [\"./writev.out\"], 0x7ffecf1aa940 /* 27 vars */) = 0brk(NULL) = 0x55bf90861000access(\"/etc/ld.so.nohwcap\", F_OK) = -1 ENOENT (No such file or directory)access(\"/etc/ld.so.preload\", R_OK) = -1 ENOENT (No such file or directory)openat(AT_FDCWD, \"/etc/ld.so.cache\", O_RDONLY|O_CLOEXEC) = 3fstat(3, &#123;st_mode=S_IFREG|0644, st_size=90249, ...&#125;) = 0mmap(NULL, 90249, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7fe81dace000close(3) = 0access(\"/etc/ld.so.nohwcap\", F_OK) = -1 ENOENT (No such file or directory)openat(AT_FDCWD, \"/lib/x86_64-linux-gnu/libc.so.6\", O_RDONLY|O_CLOEXEC) = 3read(3, \"\\177ELF\\2\\1\\1\\3\\0\\0\\0\\0\\0\\0\\0\\0\\3\\0&gt;\\0\\1\\0\\0\\0\\260\\34\\2\\0\\0\\0\\0\\0\"..., 832) = 832fstat(3, &#123;st_mode=S_IFREG|0755, st_size=2030544, ...&#125;) = 0 # fd에 대한 정보 갱신mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fe81dacc000 # 가상 메모리의 데이터 매핑 (메모리를 map하다.)mmap(NULL, 4131552, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7fe81d4cd000mprotect(0x7fe81d6b4000, 2097152, PROT_NONE) = 0mmap(0x7fe81d8b4000, 24576, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x1e7000) = 0x7fe81d8b4000mmap(0x7fe81d8ba000, 15072, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_ANONYMOUS, -1, 0) = 0x7fe81d8ba000close(3) = 0arch_prctl(ARCH_SET_FS, 0x7fe81dacd4c0) = 0 # 아키텍쳐의 특수 프로세스나 쓰레드 상태를 설정함(성립시킴) 프로세스에 할당되는 쓰레드에 실행중인 코드블록의 주소를 전달하는?mprotect(0x7fe81d8b4000, 16384, PROT_READ) = 0mprotect(0x55bf90670000, 4096, PROT_READ) = 0mprotect(0x7fe81dae5000, 4096, PROT_READ) = 0 # 메모리 주소에 대한 권한 변경munmap(0x7fe81dace000, 90249) = 0 # 메모리 해제openat(AT_FDCWD, \"buccaneer.txt\", O_WRONLY|O_CREAT|O_TRUNC, 0100750) = 3fstat(1, &#123;st_mode=S_IFCHR|0620, st_rdev=makedev(136, 1), ...&#125;) = 0brk(NULL) = 0x55bf90861000 # 실행하는 프로세스에 대해 메모리 세그먼트 변경, 추가 등.brk(0x55bf90882000) = 0x55bf90882000write(1, \"len: 47\\n\", 8) = 8write(1, \"len: 50\\n\", 8) = 8write(1, \"len: 48\\n\", 8) = 8writev(3, [&#123;iov_base=\"The term buccaneer comes from th\"..., iov_len=48&#125;, &#123;iov_base=\"A boucan is a wooden frame used \"..., iov_len=51&#125;, &#123;iov_base=\"Buccaneer is the West Indies nam\"..., iov_len=49&#125;], 3) = 148write(1, \"wrote 148 bytes\\n\", 16) = 16close(3) = 0exit_group(0) = ?+++ exited with 0 +++ readv_예시 code 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include &lt;stdio.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;#include &lt;string.h&gt;#include &lt;sys/uio.h&gt;#include &lt;unistd.h&gt;int main()&#123; char foo[48], bar[51], baz[49]; struct iovec iov[3]; ssize_t nr; int fd, i; fd = open(\"buccaneer.txt\", O_RDONLY); if (fd == -1) &#123; perror(\"open\"); return 1; &#125;; iov[0].iov_base = foo; iov[0].iov_len = sizeof(foo); iov[1].iov_base = bar; iov[1].iov_len = sizeof(bar); iov[2].iov_base = baz; iov[2].iov_len = sizeof(baz); nr = readv(fd, iov, 3); // 블럭 단위 버퍼로 저장한다. BUF_SIZ(8kb) // 파일을 읽는 물리 블록 단위는 4096bytes? 그 단위로 읽으면 재정렬이 필요없어. if (nr == -1) &#123; perror(\"readv\"); return 1; &#125;; for (i = 0; i &lt; 3; i++) &#123; printf(\"%d: %s\", i, (char *)iov[i].iov_base); &#125;; if (close(fd) == -1) &#123; perror(\"close\"); return 1; &#125;; return 0;&#125; output 1230: The term buccaneer comes from the word boucan.1: A boucan is a wooden frame used for cooking meat.2: Buccaneer is the West Indies name for a pirate. 사실 리눅스 커널 내부에 모든 입출력은 벡터 입출력이다. read()와 write()구현 역시 하나짜리 세그먼트를 가지는 벡터 입출력으로 되어있다.","categories":[],"tags":[{"name":"c","slug":"c","permalink":"https://codenamenadja.github.io/tags/c/"}]},{"title":"multiple io, select() and poll()","slug":"linux/select_poll","date":"2019-08-22T06:27:08.000Z","updated":"2021-01-25T08:36:06.079Z","comments":true,"path":"2019/08/22/linux/select_poll/","link":"","permalink":"https://codenamenadja.github.io/2019/08/22/linux/select_poll/","excerpt":"","text":"index 다중 입출력 select poll poll vs select epoll 다중_입출력 종종 키보드입력(stdin)과 IPC, 그리고 여러 파일 사이에서 일어나는 입출력을 처리하면서, 하나 이상의 파일디스크립터를 블록할 필요가 있다. 다중 입출력은 어플리케이션이 여러개의 파일 디스크립터를 동시에 블록하고, 그 중 하나라도 블록되지 않고, 읽고 쓸 준비가 되면, 알려주는 기능을 제공한다. 따라서 다음과 같은 설계방식을 따르는 어플리케이션을 위한 중심점이 된다. 다중 입출력: 파일디스크립터중 하나가 입출력이 가능할때, 알려준다. 준비가 됐나? 준비된 파일 디스크립터가 없다면, 하나 이상의 파일 디스크립터가 준비될 때까지 잠든다. 꺠어나기. 어떤 파일 디스크립터가 준비됐나? 블록하지 않고, 모든 파일 디스크립터가 입출력을 준비하도록 관리한다. 1 로 돌아가서 다시 시작한다. 리눅스는 select, poll, epoll인터페이스 라는 세가지를 제공한다. select 12345678#include &lt;sys/select.h&gt;int select(int n, fd_set *readfds, fd_set *writefds, fd_set *execptfds, struct timeval *timeout);FD_CLR(int fd, fd_set *set);FD_ISSET(int fd, fd_set *set);FD_SET(int fd, fd_set *set);FD_ZERO(fd_set *set); select()호출은 파일 디스크립터가 입출력을 수행할 준비가 되거나 옵션으로 정해진 시간이 경과할 때 까지만 블록된다. 감시 대상 파일 디스크립터는 세가지 집합으로 나뉘어 각각 다른 이벤트를 기다린다. readfds: 블록되지 않고 데이터 읽기가 가능한지 (데이터가 없으면 block, read()작업이 가능한지) 감시한다. wrtiefds: 블록되지 않고 write()가 가능한지 감시한다. exceptfds: 예외가 발생했거나, 대역을 넘어서는 데이터(소켓에만 적용)가 존재하는지 감시. 호출이 성공하면, 각 집합은 요청받은 입출력 유형을 대상으로 입출력이 준비된 파일 디스크립터만 포함되도록 변경된다. 예를 들어 7, 9두개의 파일디스크립터가 readfds에 들어있다고 한다면 호출이 반환될 때, 7이 집합에 남아있을 경우, 7번은 블록없이 읽기가 가능하다. 첫번째 인자인 n은 파일 디스크립터 집합에서 가장 큰 파일 디스크립터 숫자에 1을 더한 값이다. 따라서 select()를 호출하려면 파일 디스크립터에서 가장 큰 값이 무엇인지 알아내서 이 값에 1을 더해 첫 인자에 넘겨야 한다. timeout 인자는 timeval구조체를 가르키는 포인터이며 이 구조체는 다음과 같이 정의되어 있다. 123456#include &lt;sys/time.h&gt;struct timeval &#123; long tv_sec; // 초 long tv_usec; // 마이크로 초&#125; 이 인자가 NULL이 아니면 select()호출은 입력이 준비된 파일 디스크립터가 없을 경우에도 tv_sec초와 tv_usec 마이크로 초 이후에 반환된다. 고전적인 유닉스 시스템에서는 select()호출이 반환된 다음에 이 구조체가 정의되지 않은 상태로 남아 있으므로, 매번 호출전에 파일 디스트립터 집합과 더불어 다시 초기화를 해줘야 한다. 실제로 최근 리눅스 버전은 이 인자를 자동으로 수정해서 남아있는 시각으로 값을 설정한다. 따라서 Timeout이 5초고 파일 디스트립터가 준비되기 까지 3초가 경과했다면, tv.tv_sec은 반환될때 2를 담고 있을 것이다. timeout에 설정된 두 값이 모두 0이면, 호출은 즉시 반환되며 호출하는 시점에서 대기중인 이벤트를 알려주지만, 그 다음 이벤트를 기다리지 않는다. select에서 사용하는 fds(집합)은 직접 조작하지 않고 매크로를 사용해서 관리한다. 대다수의 시스템에서는 fds를 비트 배열처럼 간단한 방식으로 구현하고 있다. 매크로 FD_ZERO: FD_ZERO(&amp;writefds); FD_ZERO는 지정한 집합내 모든 파일 디스크립터를 제거한다. 이 매크로는 항상 select()를 호출하기 전에 사용해야 한다. FD_SET: FD_SET(fd, &amp;writefds); 주어진 집합에 파일 디스크립터를 추가한다. FD_CLR: FD_CLR(fd, &amp;writefds); 주어진 집합에서 파일디스크립터 하나를 제거한다. FD_ISSET: FD_ISSET(fd, &amp;readfds); 주어진 집합에 파일디스크립터가 존재하는지 확인한다. 존재하면 0이 아닌 정수를 반환한다. select()호출이 반환된 다음에 파일디스크립터가 입출력이 가능한 상태인지 확인하기 위해 사용된다. 예제 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#include &lt;sys/time.h&gt;#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;#define TIMEOUT 5// 타입 아웃 초#define BUF_LEN 1024// 읽기 버퍼 크기int main(void)&#123; struct timeval tv; fd_set readfds; int ret; // 표준 입력에서 입력을 기다리기 위한 준비 FD_ZERO(&amp;readfds); // select 호출 전 반드시 사용 해줘야. FD_SET(STDIN_FILENO, &amp;readfds); // IN은 데이터가 들어오면 읽을 수 있는 FD // select가 5초 동안 기다리도록 timeval구조체 설정 tv.tv_sec = TIMEOUT; tv.tv_usec = 0; // 입력 대기 시작 ret = select(STDIN_FILENO + 1, &amp;readfds, NULL, NULL, &amp;tv); // Readable한 STDIN_FILENO에 대해서 대기, 감시하는 select 선언 if (ret == -1) &#123; perror(\"select\"); return 1; &#125; else if (!ret) &#123; printf(\"%d seconds elapesd.\\n\", TIMEOUT); return 0; &#125; // 이 아래부터는 select가 0이 아닌 양수를 반환했다는 의미 // 대기중인 집합 중 하나라도 돌아왔다 // 그것을 FD_ISSET으로 선별해서, 그것을 READ혹은 WRITE한다. // 바로 실행할 수 있으니 빠르게 블록되고 해제된다. // 그 전까지는 FD를 감시하며 LOOPING하는 쓰레드가 동작 // 파일 디스크립터에서 즉시 읽기가 가능하다. if (FD_ISSET(STDIN_FILENO, &amp;readfds)) &#123; char buf[BUF_LEN + 1]; int len; // read not block len = read(STDIN_FILENO, buf, BUF_LEN); if (len == -1) &#123; perror(\"READ\"); return 1; &#125; else if (len) &#123; buf[len] = '\\0'; printf(\"read %s\\n\", buf); &#125; return 0; &#125; fprintf(stderr, \"This should not happen!\\n\"); return 1;&#125; POLL 123456789#include &lt;poll.h&gt;int poll(struct pollfd *fds, nfds_t nfds, int timeout);struct pollfd&#123; int fd; // 파일 디스크립터 short events; // 대기 이벤트 short revents; // 발생이벤트&#125; 비트마스크 기반의 세가지 파일디스크립터 집합을 사용하는 Select와 달리, poll은 fds가 가리키는 단일 pollfd구조체 배열을 nfds개수 만큼 사용한다. 각 구조체의 events필드는, 그 파일 디스크립터에서 감시할 이벤트의 비트마스크 이다. 설정 가능한 이벤트는 아래와 같다. name desc POLLIN 읽을 데이터가 존재 POLLRDNORM 일반 데이터를 읽을 수 있다. POLLRDBAND 우선권이 있는 데이터를 읽을 수 있다. POLLPRI 시급히 읽을 데이터가 존재한다. POLLOUT 쓰기가 블록되지 않을 것이다. POLLWRNORM 일반 데이터 쓰기가 블록되지 않을 것이다. POLLWRBAND 우선권이 있는 데이터쓰기가 블록되지 않을 것이다. POLLMSG SIGPOLL메세지가 사용가능하다. 그리고 revents필드에는 아래 이벤트가 추가로 설정될 수 있다. name desc POLLER 주어진 파일 디스크립터에 에러가 있다. POLLHUP 주어진 파일 디스크립터에서 이벤트가 지체되고 있다. POLLNVAL 주어진 파일 디스크립터가 유효하지 않다. 예를 들어 FD에서 읽가와 쓰기를 감시하려면, events를 POLLIN | POLLOUT으로 설정한다. 호출이 반환되면, pollfd구조체 배열에서 원하는 파일 디스크립터가 들어있는 항목을 찾아 revents에 해당 플래그가 켜져있는지 확인한다. 반환과 에러코드 poll()호출이 성공하면, revents필드가 0이 아닌 구조체의 개수를 반환한다. 이벤트가 발생하기 전에 타입아웃이 발생한다면 0을 반환한다. 에러가 발생하면 -1을 반환하며, errno를 아래중 하나로 설정한다. name desc EBADF 주어진 구조체의 파일디스크립터가 유효하지 않다. EFAULT fds를 가리키는 포인터가 프로세스 주소공간을 벗어난다. EINTR 이벤트를 기다리는중 시그널이 발생했다. 다시 호출이 필요하다. EINVAL nfds인자가 RLIMIT_NOFILE값을 초과했다. ENOMEM 요청을 완료하기위한 메모리가 부족하다. 예시 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;poll.h&gt;#define TIMEOUT 5int main(void)&#123; struct pollfd fds[2]; int ret; // 표준 입력에 대한 이벤트 감지를 위한 세팅 fds[0].fd = STDIN_FILENO; fds[0].events = POLLIN; // 표준 출력에 쓰기가 가능한지 감지하기 위한 준비 fds[1].fd = STDOUT_FILENO; fds[1].events = POLLOUT; // 블록 시작 ret = poll(fds, 2, TIMEOUT * 1000); if (ret == -1) &#123; perror(\"poll\"); return 1; &#125; if (!ret) &#123; printf(\"%d seconds elapsed.\\n\", TIMEOUT); return 0; &#125; if (fds[0].revents &amp; POLLIN) &#123; printf(\"stdin is readable\\n\"); &#125; if (fds[1].revents &amp; POLLOUT) &#123; printf(\"stdout is writable\\n\"); &#125; return 0;&#125; POLL과_SELECT비교 poll()은 가장 높은 파일디스크립터 값에 1을 더해서 인자로 전달할 필요가 없다. poll()은 파일 디스크립터 숫자가 큰 경우에 좀 더 효율적을 동작한다. select()로 값이 900인 단일파일 디스크립터를 감시한다고 할 때, 커널은 매번 전달된 파일디스크립터 집합에서 900번째 비트까지 일일이 검사해야한다. select()의 파일 디스크립터 집합은 크기가 정해져 있으므로, trade-off 발생. 집합은 크기가 작으면, select()가 감시할 최대 디스크립터 개수를 제약하며, 집합의 크기가 크면 비효율적이다. 큰 비트마스크에 대한 연산은 비효율적이며, 파일 디스크립터가 연속적이지 않을경우 특히 심각하다. poll()을 사용하면, 딱 맞는 키기의 배열 하나만 사용하면 된다. select()를 사용하면, 디스크립터 집합을 반환하는 시점에 재구성되므로 잇다라 호출하게 되면, 파일디스크립터 집합 재초기화 필요함. poll()시스템 콜은 입력과 출력을 분리하므로, 변경없이 배열을 재사용 가능하다. select()의 timeout반환하게 되면 미정의 상태가 된다. 따라서 코드의 이식성을 높이려면, timeout인자를 재초기화 해야한다. pselect()를 사용할 경우에는 이런 문제가 없다. select()시스템 콜에도 몇가지 장점이 있다. select()는 상대적으로 이식성이 높아 거의 모든 유닉스에서 지원한다. select()는 타입아웃 값으로 마이크로초까지 지정할 수 있다. poll()은 milie-sec단위로 지정할 수 있다. epoll()은 poll select보다 훨씬 뛰어난 리눅스 입출력 멀티플렉싱 인터페이스이다. 추후에 정리한다. epoll poll과 select는 실행할 때 마다, 전체 파일 디스크립터를 요구한다. 그러면 커널은 검사해야할 파일 리스트를 다 살펴본다.p epoll_사용법 123456789#include &lt;sys/epoll.h&gt;int epfd;int epoll_create1(int flags);epfd = epoll_create1(0);if(epfd&lt;0)&#123; perror(\"epoll_create1);&#125; epoll_create1()에서 반환하는 fd는 폴링이 끝난뒤 반드시 close()해줘야 한다. 에러 발생시, EINVAL: 잘못된 flags인자 EMFILE: 사용자가 열 수 있는 최대 파일 초과 ENFILE: 시스템에서 열 수 있는 최대파일을 초과 ENOMEM: 작업을 수행하기 위한 메모리 부족 -1 을 반환하고 errno를 위 중 하나로 설정한다. epoll_create에서 epoll_create1으로 epoll_create()는 구식 메서드며, epoll_create1()으로 대체됐다. epoll_create()는 아무런 인자를 받지 않으며 그대신 size인자를 받는데, 이는 사용되지 않음. size는 감시할 파일 디스크립터 개수에 대한 힌트로 사용되는데, 최신 커널에서는 동적으로 요청된 자료구조로 크기를 정하며, 이 인자는 단지 0보다 크기만 하면 된다. 만약에 size값이 0이거나 0보다 작다면 EINVAL을 반환한다. epoll_제어 epoll_ctl()시스템 콜은 주어진 epoll컨텍스트에 파일 디스크립터를 추가하거나 삭제할 때 사용한다. 123456789101112131415#include &lt;sys/epoll.h&gt;int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);// sys/epoll.h에 정의된 epoll_event 구조체.struct epoll_event&#123; __u32 events; // events union &#123; void *ptr; int fd; __u32 u32; __u64 u64; &#125; data;&#125; epoll_ctl()호출이 성공하면 해당 epoll인스턴스는 epfd파일 디스크립터와 연결된다. op인자는 fd가 가리키는 파일에 대한 작업을 명시한다. op의 값 EPOLL_CTL_ADD: edfd와 연관된 epoll인스턴스에 fd와 연관된 파일을 감시하도록 추가하며, 각 이벤트는 event인자로 정의한다. EPOLL_CTL_DEL: edfd와 연관된 epoll인스턴스에 fd를 감시하지 않도록 삭제한다. EPOLL_CTL_MOD: 기존에 감시하고 있는 fd에 대한 이벤트를 event에 명시된 내용으로 갱신한다 event인자는 그 작업동작에 대한 설명을 담고 있다. epoll_event.events의 값(OR연산으로 여러 이벤트를 묶을 수 있음) EPOLLERR: 해당 파일에서 발생하는 에러 상황. 이 이벤트는 따로 지정하지 않아도 항상 감시한다. EPOLLET: 파일을 감시할 때, edge-trigger를 사용한다. 기본 동작은 레벨트리거 방식. EPOLLHUP: 파일에서 발생하는 행업을 감시한다. 이 이벤트도 따로 지정하지 않아도 항상 감시한다. EPOLLIN: 파일 읽기가 지연되지 않고 바로 가능한지 감시. EPOLLONESHOT: 이벤트 발생 후 파일을 한번 읽고 나면 더 이상 감시하지 않는다. 이를 다시 활성화 하려면, EPOLL_CTL_MOD를 통해서 새로운 이벤트 값을 설정해야 한다. EPOLLOUT: 파일 쓰기가 지연되지 않고 바로 가능한지 감시. EPOLLPRI: 즉 시 읽어야한 OOB(TCP에서 말하는 OUT OF BAND데이터로, 전송순으로 받는 데이터의 순서를 무시하고 보내는 메세지. 잘 사용 안됨)데이터가 있는지 감시. epoll_event구조체의 data필드는 사용자 데이터를 위한 필드다. 이 필드에 담긴 내용은 이벤트 발생 후 사용자에게 반환될 때 함께 반환된다. 주로 event.data.fd를 fd로 채워서 이벤트가 발생 했을 때, 어떤 파일 디스크립터를 들여다 봐야 하는지 확인하는 용도로 사용한다. epoll_ctl()호출이 성공하면 0을 반환, 실패시 -1반환. errno를 다음 중 한 가지로 설정. EBADF: epdf가 유효한 epoll인스턴스가 아니거나, fd가 유효한 파일 디스크립터가 아니다. EEXIST: op가 EPOLL_CTL_ADD인데, fd가 이미 epfd와 연결되어 있다. EINVAL: epfd가 epoll인스턴스가 아니거나, epfd가 fd와 같다. 또는 잘못된 op값이 사용. ENOENT: op가 EPOLL_CTL_MOD, 혹은 EPOLL_CTL_DEL인데, fd가 epfd와 연결되지 않았다. ENOMEM: 해당 요청을 처리하기에는 메모리가 부족하다. EPERM: fd가 epoll을 지원하지 않는다. 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;sys/epoll.h&gt;FILE *stream;int fd, epfd;struct epoll_event event;int ret;stream = fopen(\"sample\", \"r\");if (!stream)&#123; perror(\"fopen\"); return 1;&#125;fd = fileno(stream);if (fd == -1)&#123; perror(\"fileno\"); return 1;&#125;epfd = epoll_create1(0);if (epfd&lt;0)&#123; perror(\"epoll_create1\");&#125;event.data.fd = fd;event.events = EPOLLIN | EPOLLOUT;// 이벤트 epfd에 추가ret = epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &amp;event);// 기존 이벤트 변경ret = epoll_ctl(epfd, EPOLL_CTL_MOD, fd, &amp;event);// 기존 이벤트 삭제ret = epoll_ctl(epfd, EPOLL_CTL_DEL, fd, &amp;event);if (ret)&#123; perror(\"epoll_ctl\"); close(epfd); return 1;&#125;close(epfd);return 0; epoll로_이벤트_대기하기 1234567891011121314151617181920212223242526272829303132#include &lt;sys/epoll.h&gt;#define MAX_EVENTS 64/*int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);*/struct epoll_event *events; // event를 포인터를 담을 수 있는 공간으로int nr_events, i, epfd;#include &lt;stdlib.h&gt;events = malloc(sizeof(struct epoll_event)* MAX_EVENTS);if (!events)&#123; perror(\"malloc\"); return 1;&#125;nr_events = epoll_wait(epfd, events, MAX_EVENTS, -1);if (nr_events &lt; 0)&#123; perror(\"epoll_wait\"); free(events); return 1;&#125;for (i = 0; i &lt; nr_events; i++)&#123; printf(\"event=%ld on fd=%d\\n\", events[i].events, events[i].data.fd); /* 이제 events[i].data.fd에 대한 events[i].events를 블록하지 않고 처리할 수 있다. */&#125;free(events); epoll_wait()를 호출 timeout밀리 초 동안 epoll인스턴스인 epfd에 등록한 파일의 이벤트를 기다린다. 성공시 events에는 해당 이벤트, 파일이 일기나 쓰기가 가능한 상태인지 나타내는 epoll_event구조체에 대한 포인터가 기록. 최대 maxevents만큼의 이벤트가 기록된다. 발생한 이벤트 개수를 반환하며 에러시 -1, errno를 다음중 하나로 기록한다. epoll_wait()가 결과를 반환하면, epoll_event구조체의 evens필드에는 발생 이벤트가 기록된다. data필드에는 사용자가 epoll_ctl()을 호출하기 전에 설정한 값이 담겨있다. EBADF: epfd가 유효한 fd가 아니다. EFAULT: events가 가리키는 메모리에 대한 쓰기 권한이 없다. EINTR: 시스템 콜이 완료되거나 타임아웃 전에 시그널이 발생해서 동작을 멈추었다. EINVAL: epfd가 유효한 epoll인스턴스가 아니거나 maxevents값이 0이하이다. timeout이 0이면 즉히 반환 반환값은 0이다. timeout이 -1이면 이벤트가 발생 할 때까지 해당 호출은 반환되지 않는다.(블로킹 상태) 에지트리거와_레벨트리거 epoll_ctl()로 전달되는 event인자의 events필드를 EPOLLET로 설정하면, fd에 대한이벤트 모니터가 레벨 트리거가 아닌 엣지 트리거로 동작한다. 유닉스 파이프로 통신하는 입출력에 대한 사례는 아래와 같다. 출력하는 측에서 파이프에 1kb만큼 데이터를 쓴다. 입력 받는 쪽에서는 파이프에 대해서 epoll_wait()를 수행하고 파이프에 데이터가 들어와서 읽을 수 있는 상태가 되기를 기다린다. 레벨 트리거로 이벤트를 모니터링하면, 2의 epoll_wait()호출은 즉시 반환하며, 파이프가 읽을 준비가 되었음을 알려준다. 엣지 트리거로 모니터링하면 1단계가 완료될 때까지, 호출은 반환되지 않는다. 기본 동작방식은 레벨트리거방식이다. **poll()**과 **select()**가 동작하는 방식이 레벨트리거 방식인데, 대부분 개발자들이 생각하는 방식 이기도 하다. 엣지 트리거방식은 일반적으로 논블로킹 입출력을 활용하는 접근방식을 요구하며, EAGAIN을 주의깊게 확인해야 한다.","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://codenamenadja.github.io/tags/linux/"}]},{"title":"소켓과 스트림을 이해하기 위한 C의 IO함수들","slug":"c/IO_functions","date":"2019-08-19T09:34:26.000Z","updated":"2021-01-25T08:36:06.067Z","comments":true,"path":"2019/08/19/c/IO_functions/","link":"","permalink":"https://codenamenadja.github.io/2019/08/19/c/IO_functions/","excerpt":"","text":"Index 개요 입출력 함수 표준 입출력과 버퍼 File과 Stream, 그리고 일반파일 입출력 바이너리 데이터 읽기 버퍼 입출력 예제 프로그램 추가 유용한 기능 개요 콘솔 입출력을 위한 입력 스트림과 출력 스트림은 프로그램이 실행되면 자동으로 생성되고, 프로그램이 종료되면 자동으로 소멸되는 스트림이다. 표준 스트림 name desc target stdin 표준 입력 스트림 키보드 대상 stdout 표준 출력 스트림 모니터 대상 stderr 표준 에러 스트림 모니터 대상 stdin이 입출력 리디렉션을 통해서 stdout의 파일로 전송되고 해당 바이트 만큼 stdin을 fflush하여 stdin은 비워진 상태로 인터럽션을 일으키지 않는다. basic concept of Linux / about file 링크 하드웨어 장치를 추상화하는 디바이스 파일로서 커널이 파일로서 관리하는 기본적인 2분류에 대해서 설명했었다. 키보드 디바이스 파일은, 캐릭터 디바이스 파일로서 해당 파일에 캐릭터가 들어오면 바로 인터럽션을 일으키고, 영어의 경우 1바이트를 읽어내고 언어 설정에 따라 해당 키보드 입력 값을 UNICODE캐릭터로 변환 모니터 출력 디바이스파일로 전송한다. 브라우저프로세스를 예를 들면, Keyup이라하는 이벤트는, 키보드 인터럽션에 대해서, output스트림을 stdout이 아닌 브라우저 프로세스 메모리에 대해 연결해 놓은 것이다. 프로세스에서 처리가 끝나면, 부분적인 렌더링을 개시한다. 내가 입력한 것이 모니터로 전달한 것이 아니라, 내가 입력한 값이 속성으로 전달되고, 프로세스가 그것을 모니터 출력으로 연결하는 것이다. 그래서 프로세스에서 value를 조정해서 객체의 속성을 브라우저에 인식시키면, 해당 프로세스의 로직이 바로 모니터 out에 전달하게 하는지 아닌지는 로직에 달려있다. stdin은 전달하고 나면 바로 fflush되어 해당 디바이스 파일은 비워지게 된다. 만약 fflush되어 해당 피일이 비워지지 않는다면, 뭔가 에러가 날 가능 성이 있는데, 가장 확실한 에러는 다음에 값을 입력해서 키보드 디바이스파일에 캐릭터가 들어왔다는 인터럽션이 일어났을 때, 읽어야할 바이트가 1바이트인데, 앞에서 부터 1바이트를 읽는다하면, 이번에 입력된 값과 추후 들어올 값들은 모두 누적되기만 하는 것이다. 입출력_함수 문자 단위 입출력 함수 문자 출력 함수 함수 호출 성공시 쓰여진 문자정보, 실패 시 EOF 반환 putchar int putchar(int c); 인자로 전달된 문자 정보를 stdout으로 전송하는 함수 fputc int fputc(int c, FILE *stream); 위와 동일하지만, FILE *stream의 파일객체를 지정할 수 있다. 출력 스트림이 모니터 디바이스파일이 아닌, 일반 파일로 전송하는 것이 가능하다. 따라서 stdout을 2번째 매개변수로 전달하면 putchar과 동일한 함수가 된다. 문자 입력 함수 파일 끝에 도달하거나, 함수호출 실패 시 EOF 반환 getchar int getchar(void); 모니터 출력 대응용으로 하나의 문자를 받아서 반환한다. fgetc int fgetc(FILE *stream) fgetc와 달리 입력 받을 스트림을 지정할 수 있다. 예제_1 단순 입출력 사용 12345678910111213141516171819# include &lt;stdio.h&gt;int main(void)&#123; int ch1, ch2; ch1 = getchar(); // 문자 키 입력 ch2 = fgetc(stdin); // 엔터 키 입력 putchar(ch1) // 문자 출력 fputc(ch2, stdout) // 엔터 키 출력 return 0;&#125;/*console-&gt;p (p 누르고 엔터)p (p출력, 엔터\\n 으로 ascii 10인 값으로 출력)(엔터로 인한 줄바꿈)*/ 문자 입출력에서의 EOF 1234567891011121314151617181920212223# include &lt;stdio.h&gt;int main(void)&#123; int ch; while(1) &#123; ch = getchar(); if(ch==EOF) break; putchar(ch); &#125; return 0;&#125;/*console -&gt;Hi~ (사용자 입력+엔터)Hi~ ()I like C lang. (사용자 입력+엔터)I like C lang.^Z (EOF를 반환시키는 CTRL+Z , CTRL+D)*/ getchar()함수는 문자 하나를 입력 받기 위한 함수 이지만, 만약 키보드 디바이스파일에 문자 하나이상 입력되었다면, 그 값이 비워질 만큼, getchar()을 누적해서 실행하여 해당 변수에 더한다. getchar()자체는 하나만을 리턴하는게 맞지만 만약 입력값이 남아있다면 파일이 빌 때 까지 계속 실행하는 것이다. 문자 반환형이 int 인 이유는? 반환되는 것은 1바이트 크기의 문자인데, 반환형이 int인 이유는 반환하는 값 중 하나인 EOF 는 -1로 정의된 상수이다. 따라서 char형 이라면, 그리고 char을 unsigned char로 처리하는 컴파일러에 의해 컴파일이 되었다면, EOF는 반환의 과정에서 엉뚱하게도 양의 정수로 형변환이 되어버리고 만다. 그래서 어떤 상황에서도 -1을 인식 할 수 있는 int형으로 반환형을 정의해 놓은 것이다. 문자열 단위 입출력 함수 문자열 출력 함수 성공시 음수가 아닌 값을, 실패 시 EOF반환 문자 열의 끝에는 항상 Null이 1바이트 사이즈로 있다. puts int puts(const char *s); 출력 후 자동 개행 fputs int fputs(conts char *s, FILE *stream); 출력 후 개행 없음 예제_2 123456789101112131415161718192021222324252627282930# include &lt;stdio.h&gt;int main(void)&#123; char text[] = \"Simple String\" // 문자열은 이미 배열 포인터 char *str; str = text // str에는 주소가 들어올 수 있고 text는 이미 배열 포인터이다. printf(\"1. puts tests ---- \\n\"); puts(str); puts(\"So Simple string!\"); printf(\"2. fputs tests ---- \\n\"); fputs(str, stdout); printf(\"\\n\"); fputs(\"So Simple String2!\", stdout); printf(\"\\n\"); printf(\"3. end of main ----\"); return 0;&#125;/*console --&gt;1. puts tests ----Simple StringSo Simple string!2. fputs tests ----Simple StringSo Simple String2!*/ 문자열 입력 함수 gets char *gets(char *s); 1234int main(void)&#123; char str[7]; // 7바이트 메모리 문자배열 할당 gets(str); // 입력 받은 문자열을 배열 str에 저장&#125; 미리 마련해 놓은 배열을 넘어서 문자열이 입력되면, 할당 받지 않은 할당 받지 않은 메모리 공간을 침범하여 오류가 발생. 따라서 가급적이면 아래의 형태로 fgets함수를 호출 하는 것이 좋다. fgets char *fgets(char *s, int n, FILE *stream); 12char str[7];fgets(str, sizeof(str), stdin); // stdin으로부터 문자열 받아 str에 저장. stdin으로부터 문자열을 받아 배열 str함수에 저장화되, sizeof(str)의 길이 만큼만 저장해라. 이럴 경우 stdin에 10바이트가 들어왔으면, 앞에서부터 7바이트만 끊어서 저장하게 된다. &quot;123456789&quot;를 입력. &quot;123456&quot;이 저장. 마지막 1바이트는 NULL문자. 한 줄씩 읽기 12345#include &lt;limit.h&gt;char buf[LINE_MAX];if (!fgets(buf, LINE_MAX, stream))&#123; &lt;!-- 에러 --&gt;&#125; POSIX는 &lt;limit.h&gt;에서 LINE_MAX를 정의하고 있는데, 이는 POSIX의 행처리 인터페이스가 다룰 수 있는 입력행의 최대 길이이다. 원하는 만큼 문자열 읽기 1234567891011121314151617stream = fopen(\"/etc/manifest\", \"r\");char *s;int c = 0;s = str;while (--n &gt; 0 &amp;&amp; (c = fgetc(stream)) != EOF &amp;&amp; (*s++ = c) != d)&#123; // str에 n-1바이트를 읽어 str에 저장하고, \\0을 추가한다. // *s의 다음 주소에 c(n-1바이트를 읽은 것)를 넣고 그것이 d(같은 바이트)와 같지않은 동안 계속 stream에서 읽고 저장한다.&#125;if (c == d)&#123; *--s = \"/0\"; &lt;!-- 키워드 매치가 되면서break했을떄 --&gt;&#125;else&#123; *s = \"/0\"; &lt;!-- n-1 바이트가 0이거나, 파일을 전부 읽었을 때,--&gt;&#125; d를 &quot;\\n&quot;으로 설정하면 fgets와 유사하게 동작한다. 단 버퍼에 개행문자를 저장하지 않는다. 표준_입출력과_버퍼 ANSI C의 표준에서 정의된 함수들을 표준 입출력 함수라 한다. printf, scanf, fputc, fgetc, fputs, fgets 모두 표준 입출력 함수이다. 이 표준 입출력 함수를 통해서 입출력 하는 경우, 해당 데이터들은 운영체제가 제공 하는 **메모리 버퍼**를 중간에 통과하게 된다. 메모리 버퍼는 디바이스 드라이버에 있으며, 커널과 연계된 데이터를 임시로 모아두는 메모리 공간이다. 버퍼링을 하는 이유는 무엇 인가? 물론, 키보드 디바이스는 캐릭터 디바이스로서 단일 문자가 입력되면, 바로 인터럽션이 일어나지만, 버퍼링을 통해서 그것을 저장해놓는 등으로 해당 인터럽션을 중요도가 낮은 인터럽션으로 처리하는 행동을 한다. 그리고 엔터키가 입력되었을떄. 읽어들인다는 행동으로 연결지어, 저장해놓았던 버퍼를 소모하고 메모리를 비운다. 이 버퍼링의 가장 큰 이유는 데이터 전송의 효율성과 관련이 있다. 키보드나 모니터 같은 외부장치의 데이터 입출력은 생각보다 시간이 오래 걸리는 작업이다. 따라서 버퍼링 없이 키보드가 눌릴때마다, 바로 목적지 프로세스로 이동시키는 것보다. 중간에 메모리 버퍼를 둬서 데이터를 한데 묶어 이동시키는 것이 효율적이고 빠르다. 출력 버퍼를 비우는 fflush함수 함수호출 성공 시 0, 실패시 EOF 반환 int fflush(FILE *stream) fflush(stdout)으로 호출하면, 표준 출력버퍼를 비워라 라는 명령이다. 입력버퍼 비우기 출력버퍼를 비우는 것이 데이터가 목적지(프로세스)로 전송됨을 의미한다면, 입력버퍼를 비우는 것은 데이터의 소멸을 의미한다. 예제_1 1234567891011121314151617181920212223#include &lt;stdio.h&gt;int main(void)&#123; char perID[7]; char name[10]; fputs(\"주민번호 앞 6자리 입력: \", stdout); fgets(perID, sizeof(perID), stdin); // block IOWAIT fputs(\"이름 입력: \", stdout); fgets(name, sizeof(name), stdin); // block_IOWAIT printf(\"주민번호: %s\\n\", perID); printf(\"이름: %s\\n\", name); return 0;&#125;/*console --&gt;주민번호 앞 6자리 입력: 231423이름 입력: 주민번호: 231423이름: */ 첫 fgets() 에서 7바이트를 읽어들이라고 했지만, \\n을 만나는 순간 읽지 못하고 6바이트만 읽어들임. 2번째 fputs()에서 \\n이 버퍼에 있기 떄문에, 바로 이스케이프. 예제_1 개선 123456789101112131415161718192021222324#include &lt;stdio.h&gt;void ClearLineFromReadBuffer(void)&#123; while(getchar()!= '\\n');&#125;int main(void)&#123; char perID[7]; char name[10]; fputs(\"번호 앞 6자리 입력: \", stdout); fgets(perID, sizeof(perID), stdin); // block IOWAIT // buffer = '\\n' ClearLineFromReadBuffer(); // 입력버퍼 비우기 // buffer = '' fputs(\"이름 입력: \", stdout); fgets(name, sizeof(name), stdin); // block_IOWAIT ClearLineFromReadBuffer(); // 입력버퍼 비우기 printf(\"주민번호: %s\\n\", perID); printf(\"이름: %s\\n\", name); return 0;&#125; ClearLineFromReadBuffer에서 한문자로 취급되는 \\n null이 읽혀지면 while문을 더 이상 수행하지 않는다. 따라서 그 시점에 버퍼에서 \\n이라는 문자가 읽혀짐으로써 버퍼의 첫 \\n이 사라짐. FILE과_STREAM_그리고_일반파일의_입출력 fopen함수 호출을 통한 파일과 스트림 형성, FILE구조체 성공 시 해당파일의 FILE구조체 변수의 주소 값, 실패시 NULL포인터 반환 FILE fopen(const char *filename, const char *mode); 1234567891011121314151617#include &lt;stdio.h&gt;int main(void)&#123; FILE * fp=fopen(\"simple.txt\", \"wt\"); if(fp==NULL)&#123; puts(\"file open fails!\"); return -1; &#125; fputc('A', fp); fputc('B', fp); fputc('C', fp); fputs(\"sample!\\n\", fp); fclose(fp); return 0;&#125; wt모드로 inode를 가르키고 close하는 순간 실행중인 프로세스 메모리가 해제된다. fputc를 실행한다고, 바로 저장되는 것이 아니라, 운영체제 단에서는 어느정도 버퍼메모리에 저장해놓고, 모든 변경된 버퍼를 수집해서 최적수준으로 정려한 후에 디스크에 쓴다. 이러한 과정을 writeback이라 한다. 이런 방식은 쓰기호출을 빠르게 수행해서 거의 즉시 반환하도록 만든다. 그래서 버퍼메모리에 있지만 아직 파일 HDD로 저장하겠다는 시스템콜로 전달되지 않았다면. 중간에 있던 커널쪽에 프로세스별로 디바이스 드라이버를 통해 관리하던 버퍼가 소멸되면서, 저장되지 못한채로 끝나게 된다. 그런 문제를 방지 하기 위해서, 커널은 최대 버퍼나이를 만들어 나이가 꽉찬 변경된 버퍼를 빠짐없이 기록한다. 물론 HDD에 전달하였으나, HDD에서 또한 버퍼메모리로 잡아놓고 실제 물리적으로 저장하는 것은 좀 더딘 일이 되기도 한다. 바이너리데이터_읽기 어떤 어플리케이션에서는 개별 문자나 행을 읽어서 버퍼에 입력하는 기능만으로는 부족한 경우가 있다. 종종 C구조체같은 복잡한 바이너리데이터를 읽고 써야 하는 경우가 생긴다. 이를 위해 표준입출력 라이브러리는 fread()함수를 제공한다. 123#include &lt;stdio.h&gt;size_t fread(void *buf, size_t size, size_t nr, FILE *stream); stream에서 크기가 size바이트인 엘리먼트를 nr개 읽어서 buf가 가리키는 버퍼에 저장한다. 파일 포인터가 읽은 바이트에 숫자만큼 증가한다. 읽어들인 엘리먼트 개수가 반환된다. nr보다 적은 값을 반환하여 실패나 EOF를 알려준다. ferror(), feof()를 사용해야 두 조건중 어디에 해당하는지 확인 가능하다. 123456char buf[64];size_t nr;nr = fread(buf, sizeof(buf), 1, stream);if (nr == 0)&#123; // error&#125; 바이너리데이터_쓰기 C변수처럼 바이너리 데이터를 직접 저장하려면 표준 입출력에서 제공하는 fwrite()를 사용한다. 123#include &lt;stdio.h&gt;size_t fwrite(void *buf, size_t size, size_t nr, FILE *stream); buf가 가리키는 데이터에서 size크기의 엘리먼트 nr개를 stream에 쓴다. 파일 포인터는 기록한 바이트 개수만큼 전진한다. 성공시 엘리먼트 개수를 반환하고, nr보다 받은 반환값은 실패를 나타낸다. 버퍼입출력_프로그램 struct pirate를 정의 하고 이 타입의 변수 2개 선언. 변수 값중 하나를 초기화, 출력 스트림을 통해 data파일에 이 내용을 기록. data에 대한 입력스트림을 열고 이를 통해 내용을 읽은 다음, 다른 struct pirate인스턴스를 그대로 복구. 마지막으로 그 구조체의 내용을 표준 출력에 전달. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#include &lt;stdio.h&gt;int main(void)&#123; FILE *in, *out; struct pirate &#123; char name[100]; unsigned long booty; unsigned int beard_len; &#125;; struct pirate blackbeard = &#123;\"Edward Teach\", 950, **48**&#125;; // data FILE에 저장할(초기화정보) struct pirate p; // data FILE을 읽어서 저장할. out = fopen(\"data\", \"w\"); if (!out) &#123; perror(\"fopen\"); return 1; &#125; if (!fwrite(&amp;blackbeard, sizeof(struct pirate), 1, out)) &#123; perror(\"fwrite\"); return 1; &#125; if (fclose(out)) &#123; perror(\"fclose\"); return 1; &#125; // in = fopen(\"data\", \"r\"); if (!in) &#123; perror(\"fopen\"); return 1; &#125; if (!fread(&amp;p, sizeof(struct pirate), 1, in)) &#123; perror(\"fread\"); return 1; &#125; if (fclose(in)) &#123; perror(\"fclose\"); return 1; &#125; printf(\"name=\\\"%s\\\" booty=%lu beard_len=%u\\n\", p.name, p.booty, p.beard_len); printf(\"total=%li\\np.name=%li\\np.booty=%li\\np.bread_len=%li\", sizeof(p),sizeof(p.name), sizeof(p.booty), sizeof(p.beard_len)); return 0;&#125; 12345name=\"Edward Teach\" booty=950 beard_len=48total=120p.name=100p.booty=8p.bread_len=4 변수 크기, 정렬등에서 차이가 있기 때문에, 특정 어플리케이션에서 쓴 바이너리 데이터를 다른 어플리케이션에서 읽지 못할 수 있다. 아키텍쳐와 ABI가 동일한 경우에만, 바이너리 데이터를 일관적으로 읽고 쓸 수 있다. 추가_유용한_기능들 스트림 비우기 123#include &lt;stdio.h&gt;int fflush(FILE *stream); 스트림을 비우는 것은 이해해야할 것이 많다. 일단 이것은 사용자 영역에서 일어나는 함수이고, 스트림을 비우면 프로세스 stream에 있고 아직 커널 파일버퍼로 전달되지 않은 데이터를 커널로 비운다. stream이 NULL이면 프로세스에 할당된 모든 파일 디스크립터의 입력스트림이 비워진다. 데이터를 커널메모리로 내리고 프로세스 메모리에서 해제한다. 만약 동기식 입출력처럼 물리적인 기록을 어느정도 보장 받고 싶다면, fsync()를 사용한다. fflush()를 호출한 다음 바로 fsync()를 호출한다. fd스트림_에러체크 123456789#include &lt;stdio.h&gt;if (ferror(f))&#123; printf(\"Error on f!\\n\");&#125;if (feof(f))&#123; printf(\"EOF on f!\\n\");&#125;clearerr(f); // void 타입, 항시 성공. 에러를 비운다. 파일 디스크립터 얻어오기 파일 디스크립터 생성 open creat 파일을 추상화하는 스트림 생성 fopen 디스크립터로부터 추상화된 스트림으로 변환 fdopen 종종 스트림에서부터 디스크립터를 구해야하는 경우가 있다. 예를 들어. 표준 입출력함수가 없을 때, 그 파일 디스크립터를 통해서 시스템콜을 수행할 수 있다면 유용할 것이다. 스트림에서 파일디스크립터를 구하려면 fileno()를 사용한다. 123#include &lt;stdio.h&gt;int fileno(FILE *stream); 성공하면 stream과 관련된 파일 디스크립터를 반환하고, 실패하면 -1을 반환한다. 주어진 스트림이 유효하지 않는 경우에만 실패하며 이 경우 errorno는 EBADF로 설정","categories":[],"tags":[{"name":"c","slug":"c","permalink":"https://codenamenadja.github.io/tags/c/"}]},{"title":"basic concept of linux / about memory","slug":"linux/memory_of_process","date":"2019-08-16T01:09:52.000Z","updated":"2021-01-25T08:36:06.075Z","comments":true,"path":"2019/08/16/linux/memory_of_process/","link":"","permalink":"https://codenamenadja.github.io/2019/08/16/linux/memory_of_process/","excerpt":"","text":"Index BIOS, UEFI등(메인보드 임베디드 소프트웨어)으로 인해 OS가 메모리 레벨(커널메모리)로 이동. (프로세스지만 일반프로세와는 다른취급) OS가 구동, 커널(실행자)과 사용자영역(프로세스)을 관리함. Hdd에서 ELF같은 실행파일을 읽어오면서, 코드영역블록을 TEXT세그먼트-메모리로 할당함과 동시에 커널 메모리에 프로세스 PID 매핑.(선점형 스케쥴링-OS가 강력하게 관제) 커널 내부에 보관되어 있는 페이지 테이블을 사용하여, 프로세스에 할당할 물리주소를 가상주소로 변환하여, 페이지 테이블을 마찬가지로 제공 페이지 테이블(프로세스의)에서 한페이지에 대한 데이터를 페이지 테이블 엔트리라 칭함. 가상주소와 물리주소 대응정보. 프로세스의 가상 메모리 페이지에 최초 실행과 동시에 매핑되는 것은 아래와 같다. 커널 메모리주소 프로세스 코드영역 데이터(실행부, 프로세스의 실행으로서의 정체) 프로세스 데이터영역 데이터(초기 실행에 필요한 자원, C라이브러리 등 라이브러리 오브젝트 파일) 프로세스 주소 공간 메모리 영역 프로세스_주소_공간 프로세스에서 직접 물리메모리 주소에 접근하지 않고, 커널이 개별프로세스에 독자적인 가상 주소 테이블을 제공한다. (이 주소 공간은 0에서 시작 연속적으로 늘어난다.) 페이지와_페이징 워드는 바이트로 구성되고, 페이지는 워드로 구성된다. 페이지는 메모리 관리 유닛(MMU)에서 관리할 수 있는 최소 단위다. 페이지 크기는 아키텍쳐에서 결정 32비트 시스템: 4KB 64비트 시스템: 8KB 프로세스가 mmap()을 통해 메모리 풀(8KB)을 단위로 확보 하는 것을 가상메모리를 확보 했음이라 하며 페이지 단위 메모리 확보,(프로세스당 4gb정도씩 가상주소 선 할당) 최초로 해당 페이지들에 malloc()(바이트 단위 메모리 확보)을 통해서 데이터를 매핑하려 시도하면, 최초에 물리메모리 주소가 매핑되어 있지 않기 때문에, 세그먼테이션 폴트를 일으킨다. 그러면 커널메모리의 MMU에서 해당 가상주소를 물리주소로 변환하고, 데이터를 실제로 메모리에 저장한다. 어떤 변수가 사용된지 오래되었고, 다른 프로세스를 활성하다가 해당 변수를 이 별도 프로세스에서 페이징 아웃 하였을 경우. 다시 해당 변수에 접근하면 해당 페이지의 매핑된 실제 주소가 HDD레벨로 내려가있을 것이고 그것을 페이지 폴트라 한다. 페이지 폴트가 났다는 건 다시 메모리 레벨로 페이지를 끌어올리겠다는 것이기 떄문에, 어딘가 쓸모없다고 판단되는 페이지를 커널이 free()한다. 커널은 페이징에 따른 성능 부하(HDD접근수 증가에 따른)를 줄이기 위해, 가까운 미래에 덜 쓰일 것으로 예상되는 데이터를 페이징 아웃한다. 공유와_copy-on-write 공유 메모리란? 가상 메모리에 존재하는 여러 페이지는 프로세스별로 유일하지만, 여러 가상 페이지의 엔드포인트가 특정 물리 페이지로 매핑될 수 있다. 이런 방식으로 물리 메모리에 있는 데이터를 여러 프로세스에서 공유한다. 예를 들어 여러 프로세스가 표준 C라이브러리를 사용할 때, 각 프로세스는 표준 C라이브러리를 저마다의 가상 주소공간으로 매핑 실제 메모리에는 HDD에서 복사해서 올라온 하나의 페이지만이 존재한다. 공유 데이터를 특정 프로세스에서 수정하였을 때, 공유 데이터는 읽기전용, 쓰기전용, 혹은 모두 가능한 형태로 존재. 쓰기가 가능한 공유 데이터를 어떤 프로세스에서 OVERWRITE하였을 때, 프로세스간 일정 수준의 조정과 동기화가 필요하지만, 반영된 페이지를 공유하거나, COW방식, MMU가 쓰기 요청을 가로체서 예외를 던진다. 그러면 커널은 쓰기 요청한 프로세스를 위해 해당 페이지의 복사본을 생성, 새로 만들어진 페이지에 대해 쓰기요청을 계속 진행 실제로 실 메모리 공간을 절약하기 위해, 여러 프로세스는 공유된 페이지에 대해서 읽기작업을 수행한다. 메모리_영역 커널은 접근 권한과 같은 특정 속성을 공유하는 블록 내부에, 페이지를 배열한다. 이런 블록을 맵핑, 메모리 영역이라고 한다. 모든 프로세스에는 다음과 같은 메모리 영역이 존재한다. segement types adds Text 프로세스의 코드영역, 문자열 상수, 상수변수, 읽기전용 데이터 리눅스에서 이 세그먼트는 읽기전용. 실행파일과 라이브러리 오브젝트파일에서 직접 맵핑 Stack 프로세스의 실행 스텍(지역변수, 함수의 반환데이터) 실행 스텍은 스택 깊이가 깊어지고 얕아짐에 따라, 동적으로 크기 변경된다. 멀티스레드의 경우 스레드당 하나의 스텍이 존재 Heap(DATA) 프로세스의 동적 메모리 이 세그먼트는 쓰기가 가능하고, 크기변경이 가능하다. malloc()은 이 영역을 할당한다. BSS 초기화 되지 않은 전역변수 이 변수는 C표준에 따라(기본값 0) 특수한 값을 담고 있다. 대부분의 주소공간은 매핑된 실행파일 혹은 C나 공유 라이브러리, 데이터파일을 포함. /proc/self/maps파일을 열어 보거나 pmap명령어를 사용하여, 프로세스 맵핑파일을 살펴 볼 수 있다.","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://codenamenadja.github.io/tags/linux/"}]},{"title":"basic concept of linux / process and Thread","slug":"linux/basic_concept_procss_2","date":"2019-08-12T04:07:34.000Z","updated":"2021-01-25T08:36:06.071Z","comments":true,"path":"2019/08/12/linux/basic_concept_procss_2/","link":"","permalink":"https://codenamenadja.github.io/2019/08/12/linux/basic_concept_procss_2/","excerpt":"","text":"Index Process Thread 프로세스의 계층 구조 프로세스의 생성 Process 정의 활성화 상태로 실행중인, 코드 세그먼트로 코드(실행 컨텍스트)가 이동한 프로그램. 데이터, 리소스, 상태, 가상화된 컴퓨터를 포함 커널이 이해하는 실행파일 포멧으로 만들어져 실행가능한 오브젝트 코드로 형태 리눅스에서 가장 일반적인 실행파일 포멧은 ELF(Executable and Linkable Format) 실행 파일은 여러 섹션으로 구성 메타 데이터 코드 데이터 위 섹션은 오브젝트 코드가 담긴 바이트 배열(FILE), 선형 메모리 공간에 적재됨 섹션에 담긴 바이트들은 접근 권한이 같고 사용목적이 비슷하며, 동일하게 취급. 가장 중요한 공통 섹션 섹션 설명 텍스트 섹션 실행가능 코드, 상수, 변수와 같은 읽기전용 데이터. (읽기전용과 실행가능으로 표시 데이터 섹션 정의된 값을 할당한 변수와 같은 초기화된 자료. 일반적으로 읽고 쓰기가 가능한 동적 데이터 BSS(block storage segment) 초기화되지 않은 전역 데이터. 함수 콜스텍처럼 곧 해제될 것으로 기대되는 것이 아닌, 프로그램의 실행과 종료에 생성되면 적지 않게 유지될 전역에 할당되고 사용될 데이터 프로세스는 커널이 중재하고 관리하는 다양한 시스템 리소스와 관련이 있다. 프로세스는 일반적으로 시스템 콜을 이용하여 리소스를 요청하고 조작한다. 리소스들(타이머, 대기중인 시그널, 열린 파일, 네트워크 연결, 하드웨어, IPC매커니즘) 프로세스 리소스는 자신과 관련한 데이터의 통계정보를 포함하고 있으며, 해당 프로세스의 프로세스 디스크립터 형태로 커널 내부에 저장된다. 프로세스는 가상화를 위한 추상 개념이다. 선점형 멀티테스킹과 가상메모리를 지원하는 리눅스 커널은 가상화된 프로세서와 가상화된 메모리를 프로세스에 제공한다. 프로세스와 커널의 입장에서 본 가상화에 대하여 커널은 프로세스에 단일 선형 주소공간(Single linear Address space)를 제공하므로, 프로세스 홀로 시스템에 존재하는 모든 메모리를 제어하는 것처럼 보인다. 커널은 가상 메모리와 페이징 기법(swapping)을 사용해서 프로세스마다 다른 주소공간에서 동작하도록 만들기 때문에, 여러 프로세스가 시스템 상에서 공존. 최신 프로세서는 운영체제가 독립적인 여러 프로세스 상태를 동시에 관리할 수 있도록 하며, 커널은 이런 하드웨어의 도움을 받아 가상화를 관리한다. Thread 정의 각 프로세스는 실행 스레드를 하나 이상 포함한다. 스레드는 프로세스 내부에서 실행하는 활동 단위. 코드를 실행하고 프로세스 동작 상태를 유지하는 추상개념이다. 유닉스의 간결함을 중시하는 철학과 빠른 프로세스 생성시간, 견고한 IPC매커니즘 떄문에, 전통적으로 유닉스 프로그램은 싱글 스레드였고, 스레드 기반으로 옮겨 가려는 요구사항이 적었다. 스레드의 구성요소 독자적 지역 변수를 저장하는 스텍 프로세서 상태 오브젝트 코드의 현재 위치(instruction Pointer) 스케쥴링을 통해 선점한 프로세스의 스레드의 IP를 CPU의 IR로 가져와서 실행됨. 기타 프로세스에 남아있는 대부분의 리소스는 모든 스레드의 공유자원이다. 리눅스 커널의 스레드 구현 스레드는 단순히 주소공간을 비롯 일부 리소스를 공유하는 일반적인 프로세스일 뿐이다. 사용자 영역에서 리눅스는 Pthread라 하는 POSIX 1003.1c에 따라 쓰레드를 구현한다. glibc의 일부인 현재 리눅스 쓰레드의 구현체 명은 NPTL(Nativa POSIX Threading Library). 프로세스의_계층_구조 프로세스는 PID라는 고유 양수값으로 구분된다. - (첫번쨰 프로세스의 pid는 1) 리눅스에서 프로세스는 프로세스 트리라는 계층 구조를 형성한다. 일반적으로 프로세스 트리는 init프로그램으로 알려진 첫 프로세스가 루트가 된다. 새로운 프로세스는 fork() 시스템 콜로 만들어진다. 이 시스템 콜을 호출하는 기준 프로세스를 복사해서 다른 프로세스를 만든다. 첫번째 프로세스를 제외한 나머지 프로세스는 모두 부모-프로세스가 있고, 부모-프로세스가 자식-프로세스 보다 먼저 종료 되면, 자식-프로세스를 부모-프로세스로 승격 시킨다. 프로세스가 종료되면 시스템에서 바로 제거하지 않고, 프로세스 일부를 메모리에 유지한다. 자식-프로세스가 종료될 때 부모-프로세스가 상태를 검사 할 수 있도록 한다. 이를 &quot;종료된 프로세스를 기다린다.&quot;고 표현한다. 만약 자식-프로세스가 종료되었는데 기다리는 부모-프로세스가 없다면, 이때 좀비-프로세스가 탄생한다. 일상적으로 첫 init프로세스는 자신이 fork한 모든 프로세스를 기다려서 새로 생성된 프로세스가 종료될 때 좀비가 되는 것을 막는다. 프로세스의_생성 프로세스의 생성목적 같은 프로그램의 처리를 여러 프로세스가 나눠서 처리, (웹서버 리퀘스트 분산처리) 전혀 다른 프로그램을 생성, (Bash로 부터 새로운 프로그램 init으로서 생성) 1: fork() 2: execve() fork()의 양상 자식 프로세스의 메모리 영역을 미리 할당(init에서 부터 발행하는 시점에 메모리) fork()함수의 리턴값이 각기 다른 것을 이용하여, 부모와 자식을 다른 코드를 실행하도록 분기. (1개의 스크립트 -&gt; 2개의 실행) 부모 프로세스는 fork()로 부터 복귀, 자식 프로세스는 fork()로부터 리턴되는 것 fork.c123456789101112131415161718192021222324252627282930313233#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;err.h&gt;static void child()&#123; printf(\"I'm child process, pid is %d\", getpid()); exit(EXIT_SUCCESS)&#125;static void parent(pid_t pid_c)&#123; printf(\"I'm parent process, pid is %d, child pid is %d\", getpid(), pid_c); exit(EXIT_SUCCESS);&#125;int main(void)&#123; pid_t ret; ret = fork(); if (ret == -1) err(EXIT_FAILURE, \"fork() failed\"); if (ret == 0) &#123; // child process come here child(); &#125; else &#123; // parent process come here returns if created child process (&gt; 1) parent(ret) &#125; // shouldnt come here err(EXIT_FAILURE, \"shouldn't reach here\");&#125; 12345$ cc -o fork fork.c$ ./fork\"i'm parent process, pid is 4193 child pid is 4194\"\"i'm child process, pid is 4194\"$ execve()의 양상 실행 순서 실행파일을 읽은 다음 프로세스의 메모리 맵에 필요한 정보를 읽어 들입니다. 현재 프로세스의 메모리를 새로운 프로세스의 데이터로 덮어 씁니다. 새로운 프로세스의 첫 번째 명령부터 실행합니다. 기존의 프로세스를 별도의 프로세스로 덮어씌우면서 전환하는 방식. 프로세스를 실행하는데 필요한 디테일 실행파일의 inode를 통해 코드를 포함한 HDD영역의 메모리맵 시작주소, 오프셋, 사이즈 코드 외의 변수등에서의 데이터 영역에 대한 같은 정보(오프셋, 사이즈, 메모리맵 시작주소),초기 실행필요한 메모리 최초 실행할 명력의 메모리 주소(entry point) 프로그램의 실행파일 구조 이름 값 코드영역 파일상 오프셋 100 코드 영역 사이즈 100 코드영역 메모리 시작 주소 300 데이터 영역 파일상 오프셋 200 데이터 영역 사이즈 200 데이터 영역 메모리맵 시작 주소 400 엔트리 포인트 400 12// add.cc = a + b 12345# compile to assemblyload m100 r0 # 읽어들여라 / m100 메모리 주소(name a)의 값을 / r0 레지스터에load m200 r1 # 읽어들여라 / m200 메모리 주소(name b)의 값을 / r1 레지스터에add r0 r1 r2 # 더해라 / r0 과 r1을 / r2 레지스터에 store r2 m300 # 저장하라 / r2를 / m300 메모리 주소(name c)에 실행_포멧_ELF_분석 리눅스 대표 실행파일인 ELF포멧의 정보는 readelf명령어로 해석 가능. /bin/sleep을 예제로 정보를 가져오면, 123456789101112131415161718192021readelf -h /bin/sleepELF Header:# Magic: 7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00# Class: ELF64# Data: 2's complement, little endian# Version: 1 (current)# OS/ABI: UNIX - System V# ABI Version: 0# Type: DYN (Shared object file)# Machine: Advanced Micro Devices X86-64# Version: 0x1# Entry point address: 0x1b70# Start of program headers: 64 (bytes into file)# Start of section headers: 33208 (bytes into file)# Flags: 0x0# Size of this header: 64 (bytes)# Size of program headers: 56 (bytes)# Number of program headers: 9# Size of section headers: 64 (bytes)# Number of section headers: 28# Section header string table index: 27 Entry point address: 0x1b70 실행해야 하는 코드 주소 위치는 0x1b70 여기서 코드영역과 데이터 영역의 파일상의 오프셋, 사이즈, 메모리맵 시작주소를 얻으려면 12345678910111213141516171819readelf -S /bin/sleepThere are 28 section headers, starting at offset 0x81b8:Section Headers: [Nr] Name Type Address Offset Size EntSize Flags Link Info Align # 중략 [14] .text PROGBITS 00000000000018d0 000018d0 0000000000003989 0000000000000000 AX 0 0 16 [24] .data PROGBITS 0000000000208000 00008000 0000000000000080 0000000000000000 WA 0 0 32 [25] .bss NOBITS 0000000000208080 00008080 00000000000001c0 0000000000000000 WA 0 0 32 # 중략Key to Flags: W (write), A (alloc), X (execute), M (merge), S (strings), I (info), L (link order), O (extra OS processing required), G (group), T (TLS), C (compressed), x (unknown), o (OS specific), E (exclude), l (large), p (processor specific)","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://codenamenadja.github.io/tags/linux/"}]},{"title":"basic concept of linux / about file","slug":"linux/basic_concept","date":"2019-08-12T01:09:52.000Z","updated":"2021-01-25T08:36:06.071Z","comments":true,"path":"2019/08/12/linux/basic_concept/","link":"","permalink":"https://codenamenadja.github.io/2019/08/12/linux/basic_concept/","excerpt":"","text":"Index 파일과 프로세스, 파이프와 소켓을 다루기 위한 추상적 인터페이스 등이 유닉스의 핵심. file select 시스템 콜 일반 파일 디렉터리와 링크 하드 링크 심볼릭 링크 특수 파일 파일 파일은 리눅스에서 가장 기본적이고 핵심이 되는 추상화 개념 리눅스에서는 모든 것이 파일이다 파일에 접근하려면 먼저 파일을 열어야 한다. 파일은 읽기, 쓰기 모드 등으로 열 수 있고, 이렇게 열린 파일은, 해당 파일에 대한 메타데이터와 연결된 고유한 기술자(descriptor)를 통해 참조될 수 있다. 파일 디스크립터는 사용자 영역에서 공유되며, 응용프로갬이 파일에 접근할 떄 직접 사용. 따라서, 리눅스 프로그래밍의 대부분은 바로 이 파일 디스트립터(fd)를 열고 조작하고 닫는 작업이다. 파일 디스크립터는 정수로 표현된다. C의_select함수 동기적 통신 123456789101112sock = socket.socket()request = \"GET &#123;&#125; HTTP/1.0\\r\\nHOST: xkcd.com\\r\\n\\r\\n\".format(url)encoded = request.encode(\"ascii\")while True: try: sock.send(encoded) # blocking 발생 break # Done except OSError as e: passprint(\"sent\") 이 메서드는 IO wait에 의해 단순히 전력을 낭비할 뿐만 아니라해 효과적으로 다수의 소켓에서 이벤트를 대기하는 것도 할 수 없다. 고전적인 방법으로 BSD Unix의 해결책은 select였다. C함수이고, 그것은 Nonblocking소켓 또는 small array를 그들에게서 부터 기다릴 수 있다. 요즘에는 인터넷 어플리케이션이 무지막지한 연결수를 초래하여서 poll보다는, BSD의 kqueue, Linux의 epoll로 대체하게 되었다. 이들은 select와 비슷한 동작을 가지고 있지만 다수의 연결을 잘 해결해 낸다. 파이썬 3.4의 DefaultSelector이 당신의 OS중에 가능한 최고의 select같은 함수를 사용한다. network I/O에 대한 인터럽션에 대응하는 알림을 등록하기 위해서 우리는 논블로킹 소켓을 생성하고, 이것을 DefaultSelector에 등록한다. 1234567891011121314151617from selectors import DefaultSelector, EVENT_WRITEselector = DefaultSelector()sock = socket.socket()sock.setblocking(False)try: sock.connect((\"xkcd.com\", 80))except BlockingIOError: passdef connected(): seletor.unregister(sock.fileno()) print(\"connected!\")selector.register(sock.fileno(), EVENT_WRITE, connected)# sock.fileno()가 네트워크 소켓에 대한 파일 디스크립터(정수)로 자동 생성되고 할당된다. DefaultSelector() : select함수와 같은 역할 소켓을 생성하면 파일 디스트립터(IO wait, Read, write)가 1:1관계로 자동 생성된다. 소켓과 매핑된 파일디스트립터를 등록하고, 해당 파일에 대해 이벤트를 감지하면 약속된 행동을 개시한다. select함수는 비동기 소켓에 대해 또 다른 파일을 등록해 해당 파일 디스크립터에 대해 특정 이벤트를 감시하는(해당 인터럽션이 중요하다고 여기는 것) 약속을 등록하고 해제해준다.(이 또한 파일) 일반파일 일반 파일은 바이트 스트림이라 불리는 연속적으로 나열된 바이트 배열에 저장된 데이터를 의미한다. 파일 디스크립터 정수로 특정 파일에 대한 METADATA가 담긴 프로세스의 특수 메모리주소를 가리킨다. 파일 읽기 해당 파일 디스트립터가 가르키는 메타데이터의 inode의 오프셋(bytes)만큼 나열된 연속적 데이터(byte-array)를 가져온다. 리눅스에는 파일을 위한 특별한 자료구조가 없다. 따라서 어떠한 값이가도 들어갈 수 있다. 시스템수준에서 살펴보면 리눅스는 바이트 스트림외어 어떤 자료구조도 강제하지 않는다. 파일이 처음 열리면 file-offset은 0이다. 직접 파일 오프셋을 지정할 수 도 있는데, 파일 끝을 넘길 수도 있다. 파일 크기를 넘어 바이트를 기록하면 파일 끝에서 지정한 위치가 시작하는 부분까지 내용은 모두 0이 된다. 파일을 쓰는 작업은 대부분 파일 끝에서 일어난다. 파일 오프셋의 최댓값은 오프셋을 저장하기 위해 사용하는 C타입의 크기로 결정된다. 최신리눅스에서는 64비트 값이다. 하나의 파일은 다른 프로세스나 동일한 프로세스에서 한 번이상, 혹은 동시에 열려질 수 있다. 그들은 열릴때마다 고유한 파일 디스크립터를 반환한다. 프로세스에서는 따라서 파일 디스트립터를 공유할 수 있다. 커널은 파일에 대한 동시 접근을 막지 않는다. 일반적으로 파일 이름을 통해 파일에 접근하지만, 실제로 파일은 inode(information node)라 하는 파일시스템의 고유한 정수값으로 참조 된다. Inode 의 구성 변경된 날짜 소유자 타입 길이 물리 데이터 저장 위치 파일이름은 저장하지 않는다. inode는 유닉스 파일 시스템에서 디스크에 저장된 물리적 객체임과 동시에, 리눅스 커널에서 자료구조로 표현되는 논리적인 개념이기도 하다. 디렉터리와_링크 inode번호로 파일에 접근하면 보안위협이 있어서, 파일 이름을 사용한다. 디렉터리는 파일에 접근하기 위한 이름을 제공하는데, 이 이름과 inode의 Pair를 Link라고 한다. 개념적으로 디렉터리는 일반파일과 유사한 모습이지만 데이터가 존재하지 않는다. type name data directory in directory file’s namespaces file in directory as inode inode no file location, offset, elses 실제 디렉터리도 inode가 있다. 따라서 디렉터리 내부의 링크 역시 다른 inode를 가질 수 있다. 즉, 디렉터리는 계층적인 구조를 형성할 수 있게 된다. 이를 이용하여 /home/sample/data.png처럼 파일 경로를 사용할 수 있는 것이다. /home/sample/data.png같은 경로로 파일을 열겠다고 요청하면, 해당 파일 경로에 속한 각 디렉터리 엔트리 (called as dentry by kernel)를 탐색하여 다음 항목의 inode를 찾음. /로 시작, sample디렉터리의 inode를 찾는다. sample.png의 inode를 얻는다. 리눅스 커널은 dentry 캐시를 활용하여 찾기 결과를 저장하고 나중에 일시적인 지역성을 이용해 탐색속도를 높인다. 지역성(locality) : 한번 접근한 데이터의 근처의 데이터에 또 접근하는 경우가 많다. 비록 디렉터리도 inode가 있고 네임이 있으니, 일반 파일처럼 취급하지만, 커널은 사용자 영역에서 디렉터리를 일반 파일처럼 열고 조작하지 못하도록 제한한다. 따라서 디렉터리는 특수한 시스템 콜을 통해 조작해야하는데, 이를 위한 시스템 콜은 아래와 같다. 링크 추가 링크 삭제 사용자 영역에서 커널의 중제없이 디렉터리를 조작할 수 있게 하면, 조그만 오류가 발생해도 파일시스템 전체를 망가뜨릴 수 있다. 하드_링크 type origin name linkto hard link A h_link_A A’s inode hard link A h_link_B A’s inode symbolic link B B_link_A B’s inode symbolic link B B_link_B B_link_A’s inode 다른 이름으로 동일한 inode를 가르키지 못하게 하는 방법은 없다. 다중 링크는 동일한 inode를 여러가지 이름으로 맵핑한다. 이를 하드링크라 한다. 동일한 inode에 대한 다수의 하드링크에 대해서 커널은 1회의 실행을 통해 원본의 inode를 알아낸다. 따라서 해당 원본에 대한 하드링크가 늘어난다면? 그만큼 Referencing counter이 늘어난다. 원본을 지워도 원본의 inode를 가지고 있는 Referencing이 살아 있기 때문에, 해당 inode의 데이터는 삭제되지 않는다. 링크 카운터가 0이 되는 순간 비로소 해당 inode가 포함하는 자료가 실제로 삭제된다. 심볼릭_링크 심벌릭 링크는 최종 엔드포인트가 origin의 inode를 가르키고 있지만, 독자적인 inode값을 가지게 된다. 그래서 일반 디렉터리처럼 달리 독자성을 가지고 있기 때문에, 파일처럼 생겼다. 하드 링크보다 심볼릭 링크가 결론적으로 오버헤드를 많이 가져오는 이유는, 심벌릭 링크를 효과적으로 탐색하기 위해, 해당 링크와 그 링크로 연결된 파일 둘다 다뤄야 하기 때문이다. 하드링크에서 추가적인 오버헤드가 발생하지 않는 이유는, 파일 시스템에서는 링크가 둘 이상 걸린 파일에 접근하거나, 하나만 걸린파일에 접근하거나 구현상 차이가 없기 때문이다. 심벌릭 링크가 단일로서는 오버헤드가 매우 작지만,(원본 레퍼런스의 couter을 늘리지 않는 점등에서,) 그래도 부정적인 것으로 취급받고 있다. 하드링크는 사용자 입장에서 봤을떄 원본 주소 그 자체이기 때문에, 완전히 투명하다. 그러나 실제로 2번 이상 링크된 파일을 찾아내는 작업이 더 어렵다. 반면 심볼릭 링크를 조작하려면 특수 시스템 콜이 필요하다. 이런 투명성 결여는 심벌릭 링크가 명료하게 생성되며, 파일시스템내부 링크가 아닌, 일종의 바로가기 처럼 사용되기 떄문에, 종종 장점으로 작용하기도 한다. 특수_파일 정의 파일로 표현되는 커널 객체 종류 캐릭터 디바이스 파일 블록 디바이스 파일 네임드 파이프 유닉스 도메인 소켓 특수 파일은 모든 것이 파일이라는 유닉스 철학에 맞게, 파일 시스템 위에 구현된 추상화 개념이며 리눅스는 특수파일을 생성하는 시스템 콜을 제공한다. 하드웨어 장치를 추상화하는 디바이스 파일 하드웨어 장치마다 특수 디바이스파일이 존재한다. 장치의 종류가 같다면 같은 인터페이스의 드라이버를 사용한다. 특수 파일은 개별적이나 종류가 같다면 같은 인터페이스를 사용. 캐릭터 디바이스 파일 디바이스 드라이버는 큐에 바이트를 하나씩 집어 넣고, 사용자 영역에서는 큐에 쌓인 순서대로 바이트를 읽어낸다. 대표적인 예: 키보드 사용자가 'peg’라고 입력 애플리케이션은 키보드 드라이버 인터페이스를 통해 디바이스 파일에서 p, e, g 를 순서대로 읽음. 읽을 문자 없을 시 EOF(End of Line)을 반환. 블록 디바이스 파일 반면 블록 디바이스는 바이트 배열로 접근한다. 보통 저장장치를 말하는데, 위치 지정이 가능한 장치에 여러 바이트를 맵핑해, 사용자 영역에서는 이 배열이 포함하고 있는 유효한 바이트에 임의 접근할 수 있다. (하드 디스크 드라이브, 플로피 드라이브, CD-ROM드라이브, 플래시 메모리) 대표적인 예: HDD 12바이트를 읽음 7바이트 읽음 12바이트 읽음 블록 디바이스의 최소 접근 단위는 섹터이다. 섹터는 디바이스의 물리적인 속성며 2의 승수로 나타내며 512가 가장 일반적이다. 블록 디바이스는 섹터보다 더 작은 데이터 단위를 전송하거나 접근하지 못한다. 따라서 모든 입출력은 섹터 하나를 가져오는 것 혹은 보내는 것을 기준으로 일어난다. 네임드 파이프 FIFO로 부르기도 한다. IPC(Inter Process Communication)메커니즘으로 특수파일을 읽고 쓰는 파일 디스크립터 형태로 통신 채널을 제공한다. 일반 파이프는 특정 프로그램 출력을 자신의 입력으로 잇는 것처럼, 다른 프로그램 입력으로 연결하기 위한 방법이다. 시스템 콜로 만들어지는 일반 파이프는 파일 시스템이 아니라 메모리에 존재한다. 네임드 파이프는 일반 파이프와 동일하게 동작하지만, FIFO라는 특수 파일을 거쳐 접근한다. 서로 무관한 프로세스도 이 파일에 접근하는 방식으로 프로세스 사이에 통신이 가능해진다. 소켓 소켓은 다른 프로세스끼리 통신할 수 있는 고급 IPC의 한 종류이며, 다른머신과의 통신도 가능하다는 점이 다르다. 소켓은 네트워크와 인터넷 프로그래밍의 근간. 로컬 머신 내부에서 통신을 위해 사용하는 소켓형태인 유닉스 도메인 소켓을 포함하여, 많은 변종이 있다. 인터넷에서 통신하는 소켓은 IP를 파악하기위해 host name, port를 사용. 유닉스 도메인 소켓은 파일 시스템에 만들어진 소켓파일(특수 파일)을 사용한다. 파일 시스템 접근단위 블록에서 최소 접근 단위가 섹터라면 파일 시스템에서의 논리적 최소 접근단위는 블록이다. 블록 블록은 파일시스템을 추상화한 개념으로 파일시스템이 존재하는 물리매체와 무관하다. 블록은 보통 섹터크기의 2의 승수이다. 따라서 일반적으로 섹터보다 더 크지만, 페이지 크기 보다는 작다. 페이지 크기: 하드웨어 메모리 관리 유닛에서 지정 가능한 최소 단위 보통 블록 크기는 512, 1024, 4096 바이트다.","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://codenamenadja.github.io/tags/linux/"}]},{"title":"커널의 이해와 시스템 콜 이해","slug":"linux/understanding_kernel","date":"2019-08-11T08:35:49.000Z","updated":"2021-01-25T08:36:06.079Z","comments":true,"path":"2019/08/11/linux/understanding_kernel/","link":"","permalink":"https://codenamenadja.github.io/2019/08/11/linux/understanding_kernel/","excerpt":"","text":"Index 개요 사용자 모드로 구현되는 기능 개요 OS의 구성, 커널,사용자 모드 타입 정의 커널모드 OS의 인터페이스로 내부 장치에 대한 Wrapper manager 사용자모드 OS의 인터페이스를 사용하는 외부 프로세스 커널프로세스의 고유한 역할 title 설명 디바이스 드라이버 관리 외부 프로세스가 디바이스에 대한 동시접근을 제한, 종류가 같은 디바이스는 동일 인터페이스로 조작 프로세스 관리 시스템 프로세스의 상태를 관리하고, 그에 따른 상태를 조정 및 관리 (OnProcessing, , kill) 프로세스 스케줄링 프로세스당 CPU 리소스 사용 시간 부여 메모리 관리 시스템 프로세스당 메모리(물리메모리, 가상메모리) 부여 프로세스의 동작과 OS의 역할이라 함은, 다양한 데이터가 메모리를 중심으로 1. CPU의 레지스터 2. L1,L2 Cache 3. 메모리 4. 저장장치 와 같은 저장장치의 계층 구조를 통해 프로세스를 빠르고 안정적으로 동작시킨다. 저장장치에 보관된 데이터는 디바이스 드라이버에 요청하여 접근 할 수 있지만, 좀 더 간단히 접근하기 위해, 파일 시스템이라하는 프로그램을 통해 접근한다. 1. 프로세스(사용자 모드) 2. 파일시스템(커널모드) 3. 저장장치 드라이버 4. 저장장치 시스템 또한 최우선의 관리자로서 작동하려면 저장장치로 부터 OS를 읽어야 한다. 정확하게는 OS를 읽어들이기 전에 BIOS, UEFI라 하는 하드웨어 임베디드 소프트웨어가 작동하여 하드웨어의 초기와 처리 동작할 OS를 선택하는 부트로더 가 동작합니다. 사용자_모드로_구현되는_기능 다룰 내용 시스템 콜 OS가 제공하는 라이브러리 OS가 제공하는 프로그램 시스템 콜의 종류 1. 프로세스 생성, 삭제 2. 메모리 확보, 해제 3. 프로세스간 통신(IPC) 4. 네트워크 5. 파일시스템 다루기 6. 파일 다루기(디바이스 접근) CPU의 모드 변경 프로세스가 사용자 모드로 사용되는 와중에 커널에 처리를 요청하고자 시스템 콜을 호출하면 CPU인터럽트 발생 CPU가 사용자 모드에서 커널모드로 변경 요청내용 처리 시스템 콜 처리 종료 저장해놨던 CODE segment에 대한 정보를 이어가, IR에 담음 사용자 모드로 전환 프로세스 동작 이어감 시스템 콜 호출의 동작 순서 hello.c1234567#include &lt;stdio.h&gt;int main(void)&#123; puts(\"hello world\"); return 0;&#125; bash12strace -o hello.log ./hellohello world bash12345678910cat hello.logexecve(\"/hello\", [\"./hello\"], [/* 77 vars */]) = 0brk(NULL) = 0x5619fe7aa000access(\"/etc/ld.so.nohwcap\", F_OK) = -1 ENOENT (No such file or directory)access(\"/etc/ld.so.preload\", R_OK) = -1 ENOENT (No such file or directory)...close(3) = 0write(1, \"hello world\\n\", 12) = 12exit_group(0) = ? hello.py1print(\"hello world\") bash1strace -o hello.py.log python3 ./hello.py bash1234567891011121314151617cat hello.py.logexecve(\"/home/junehan/miniconda3/bin/python3\", [\"python3\", \"hello.py\"], 0x7fff882f0768 /* 77 vars */) = 0brk(NULL) = 0x5619fe7aa000access(\"/etc/ld.so.nohwcap\", F_OK) = -1 ENOENT (No such file or directory)readlink(\"/proc/self/exe\", \"/home/junehan/miniconda3/bin/pyt\"..., 4096) = 38access(\"/etc/ld.so.preload\", R_OK) = -1 ENOENT (No such file or directory)...close(3) = 0write(1, \"hello world\\n\", 12) = 12 # write system callrt_sigaction(SIGINT, &#123;sa_handler=SIG_DFL, sa_mask=[], sa_flags=SA_RESTORER, sa_restorer=0x7f8b453db890&#125;, &#123;sa_handler=0x5619fd4904d0, sa_mask=[], sa_flags=SA_RESTORER, sa_restorer=0x7f8b453db890&#125;, 8) = 0sigaltstack(NULL, &#123;ss_sp=0x5619fe7ca720, ss_flags=0, ss_size=8192&#125;) = 0sigaltstack(&#123;ss_sp=NULL, ss_flags=SS_DISABLE, ss_size=0&#125;, NULL) = 0munmap(0x7f8b4564c000, 262144) = 0brk(0x5619fe842000) = 0x5619fe842000exit_group(0) = ?# 705개의 시스템 콜 호출","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://codenamenadja.github.io/tags/linux/"}]},{"title":"에러처리 Decorator과 logging, write to csv","slug":"python/custom_module/error_handler","date":"2019-08-09T08:22:43.000Z","updated":"2021-01-25T08:36:06.079Z","comments":true,"path":"2019/08/09/python/custom_module/error_handler/","link":"","permalink":"https://codenamenadja.github.io/2019/08/09/python/custom_module/error_handler/","excerpt":"","text":"Error hanlder with logging github repo 기초 사용법 Expected Error 2번째 요소를 지정하면서 특정상황에 일으키는 예견된 에러 123456raise ValueError(\"some message\", \"info\")raise PermissionError(\"some message\", \"warning\")raise FileNotFoundError(\"some message\", \"errror\")raise AttributeError(\"some message\", \"debug\")raise ConnectionRefusedError(\"some message\", \"critical\")raise ConnectionAbortedError(\"some message\", \"fatal\") logs into curdir filename debug_expected.log writes as, asctime - name - levelname - {message} {message}: filename - lineno - message UnExpected Error 2번째 요소가 지정 되지 않는 실제 에러 1raise ValueError(\"message\") logs into curdir filename debug_unexpected.log writes same as upper, run logger.log_to_csv() 1234567def log_to_csv(logger_name: Optional[str] = \"unexpected\") -&gt; bool \"\"\" read &#123;logger_name&#125;_debug.log file write into csv as thread reads line. if file exists, write over ends of the line \"\"\" custominzing to do 예상 외 에러를 발생하면 CSV파일이 모두 씌여지고, 해당 CSV파일을 S3저장소에 업로드 마지막줄 기준으로 날짜가 달라졌다면, 오늘 이전 로그를 이메일로 전송(그렇다면 1일 최대 1회 이메일로 파일전송) more to do 순수한 에러핸들러 데커레이터와 로깅이 연동된 데커레이터를 분리","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"custom module","slug":"custom-module","permalink":"https://codenamenadja.github.io/tags/custom-module/"}]},{"title":"python collections 모듈","slug":"python/module/collections","date":"2019-08-03T06:55:54.000Z","updated":"2021-01-25T08:36:06.079Z","comments":true,"path":"2019/08/03/python/module/collections/","link":"","permalink":"https://codenamenadja.github.io/2019/08/03/python/module/collections/","excerpt":"","text":"Index 개요 ChainMap namedtuple Counter UserDict 개요 타입 설명 namedtuple() 팩토리 함수, tuple형태의 명명된 필드를 가진 서브클래스를 생성하는것에 고성능 deque list같은 컨테이너, 양쪽 끝에서 더하고 빼는게 빠르다 ChainMap dict같은 클래스, 복수의 매핑들로 구성된 하나의 뷰를 생성하는데 사용. Counter dict의 서브클래스, hashable객체의 키값을 세기 위한 용도 OrderedDict dict의 서브클래스, 순서가 있는, 엔트리가 추가됐다는 것을 기억한다. defaultdict dict의 서브클래스, 빠진 값을 처리하기위한 팩토리 함수 UserDict dict객체의 래퍼, dict의 서브클래싱을 쉽게 해준다. UserList list객체의 래퍼, list의 서브클래싱을 쉽게 해준다. UserString string객체의 래퍼, string의 서브클래싱을 쉽게 해준다. deque는 의도가 명확하고, namedtuple또한 의도와 사용법이 한정적이기 떄문에, 그 외의 것들을 다룬다. 서브클래싱은 팩토리, 인스턴스를 의미한다. ChainMap ChainMap_introduction ChainMap클래스는 빠르게 다수의 mappings를 연결하기 위해 구성되었다. 따라서 그들은 하나의 유닛으로 다뤄질 수 있다. 때로 새로운 딕셔너리를 생성해서, 여러번 update()를 실행하는 것보다 더 빠를 수 있다. 기본적으로 리스트 컴프리헨션으로 해당 mapped자료구조들을 풀면 keys를 푼다. 매개변수의 args에서 0번째부터의 데이터 유지를 최우선으로 한다. ChainMap에서 Values에 대한 접근이 nametuple에서 제한된다. keys로 접근하면 key가 아닌 value를 리턴한다. 리턴하는 값들의 순서는 역방향이다. 마지막 매개변수의 키값 등을 먼저 출력. 매개변수 후자의 객체(읽히는 순서)에 앞에있는 것을 계속 Update해나가는 식(덮어 씌워짐) 12345678910111213\"\"\"&gt;&gt;&gt; baseline = &#123;'music': 'bach', 'art': 'rembrandt'&#125;&gt;&gt;&gt; adjustments = &#123;'art': 'van gogh', 'opera': 'carmen'&#125;&gt;&gt;&gt; list(ChainMap(adjustments, baseline))['music', 'art', 'opera']# 위 아래는 행동이 동일하다. 코드를 간결하게 유지하고, 속도도 때로 더 빠르다.&gt;&gt;&gt; combined = baseline.copy()&gt;&gt;&gt; combined.update(adjustments)&gt;&gt;&gt; list(combined)['music', 'art', 'opera']\"\"\" namedtuple namedtuple_고찰(키값으로 매핑되는 데이터들 기준) namedtuple인스턴스의 크기는 class인스턴스보다 크다.(아래표는 byte크기 기준 비교) key=[“username”,“id”] values=[“asd”, “dvd”]로 생성 type nametuple class class with __slot__ class 888 1056 888 instance 56 64 56 ChainMap적용 .keys() ['sadf', 'sad', 'sad'] ['id', 'username', 'password'] TypeError:Not Iterable cant not make chainMap ChainMap적용 .values() TypeError:Tuple indices must be integer or slices, not str ['sadf', 'sad', 'sad'] pass ChainMap적용 list() ['sadf',' sad', 'sad'] ['id','username','password'] pass 가장 간결하게 쓰고 싶다면, 그냥 리스트나 튜플로 키값없이 약속된 값 순서대로 타입을 체크해서 처리하는게, 메모리 효율적이다. 그러나 인스턴스에 대해 동일한 기능이 필요하거나, 값의 타입 분리가 쉽지 않다면, 키값은 필요하다. 함수형 프로그래밍을 통해 해당 객체의 타입만 재정의하고, 기능을 별도함수로 처리하면, 동일함수로직을 여러 객체가 사용하기 위해, 함수가 커지는 부작용이 있다. 일단 클래스로 정의하고, __slots__를 통해서 사용할 메서드만 정의하고, 개별적인 validate메서드를 사용하고, 클래스 오버라이딩을 통해서 메모리를 줄이는 방법이 있다. namedtuple의 장점은, 코드 양이 적으니, 파일 크기가 적고, 개발자들 사이에서 더 간결한 표현일 수 있다는 점이다. 12345678910111213141516171819202122232425262728\"\"\"&gt;&gt;&gt; Slot_dict.__dict__mappingproxy(&#123;'__module__': '__main__', '__slots__': ['username', 'id'], '__init__': &lt;function __main__.Sample.__init__(self, s1, s2)&gt;, 'id': &lt;member 'id' of 'Sample' objects&gt;, 'username': &lt;member 'username' of 'Sample' objects&gt;, '__doc__': None&#125;)&gt;&gt;&gt; n_tuple.__dict__mappingproxy(&#123;'__module__': '__main__', '__doc__': 'first(username, id)', '__slots__': (), '_fields': ('username', 'id'), '__new__': &lt;staticmethod at 0x7f6aec52b7b8&gt;, '_make': &lt;classmethod at 0x7f6aec52bcf8&gt;, '_replace': &lt;function namedtuple_first.first._replace(_self, **kwds)&gt;, '__repr__': &lt;function namedtuple_first.first.__repr__(self)&gt;, '_asdict': &lt;function namedtuple_first.first._asdict(self)&gt;, '__getnewargs__': &lt;function namedtuple_first.first.__getnewargs__(self)&gt;, 'username': &lt;property at 0x7f6aec4cc1d8&gt;, 'id': &lt;property at 0x7f6aec4cc2c8&gt;, '_source': \"namedtuple 원본의 __doc__\"&gt;&gt;&gt; slot_dict_instance[\"username\"]'asd'&gt;&gt;&gt; nt.username'asd'\"\"\" csv와 sqlite를 활용 123456789101112EmployeeRecord = namedtuple('EmployeeRecord', 'name, age, title, department, paygrade')import csvfor emp in map(EmployeeRecord._make, csv.reader(open(\"employees.csv\", \"rb\"))): print(emp.name, emp.title)import sqlite3conn = sqlite3.connect('/companydata')cursor = conn.cursor()cursor.execute('SELECT name, age, title, department, paygrade FROM employees')for emp in map(EmployeeRecord._make, cursor.fetchall()): print(emp.name, emp.title) 3.7버전 이후부터 _source속성이 사라지고, namedtuple(NAME:str, FieldNames:iterable, defaults:iterable = None) 초기값을 kwargs['defaults']로 설정 할 수 있다. namedtuple을 상속받고 __slots__를 구현한 클래스 1234567891011121314class Point(namedtuple('Point', ['x', 'y'])): __slots__ = () @property def hypot(self): return (self.x ** 2 + self.y ** 2) ** 0.5 def __str__(self): return 'Point: x=%6.3f y=%6.3f hypot=%6.3f' % (self.x, self.y, self.hypot)\"\"\"&gt;&gt;&gt; for p in Point(3, 4), Point(14, 5/7):... print(p)Point: x= 3.000 y= 4.000 hypot= 5.000Point: x=14.000 y= 0.714 hypot=14.018\"\"\" 위의 서브클래스는 __slots__를 빈 튜플로 정의한다. 이것은 Instacne dictionarie를 생성하는 것을 막도록 하여 메모리 요구사항을 낮게 돕는다. namedtuple’s_hidden_methods somenamedtuple._() 12345\"\"\"&gt;&gt;&gt; t = [11,22]&gt;&gt;&gt; Point._make(t)OrderedDict([('x', 11), ('y', 22)])\"\"\" somenamedtuple._asdict() 12345\"\"\"&gt;&gt;&gt; p = Point(x=11, y=22)&gt;&gt;&gt; p._asdict()OrderedDict([('x', 11), ('y', 22)])\"\"\" somenamedtuple._replace(**kwargs) 12345\"\"\"&gt;&gt;&gt; p = Point(x=11, y=22)&gt;&gt;&gt; p._replace(x=33)Point(x=33, y=22)\"\"\" namedtuples’_hidden_attributes somenamedtuple._fields 12345678\"\"\"&gt;&gt;&gt; p._fields('x', 'y')&gt;&gt;&gt; Color = namedtuple('Color', 'red green blue')&gt;&gt;&gt; Pixel = namedtuple('Pixel', Point._fields + Color._fields)&gt;&gt;&gt; Pixel(11, 22, 128, 255, 0)Pixel(x=11, y=22, red=128, green=255, blue=0)\"\"\" somenamedtuple._fields_defaults defaults가 뒤에서 부터 length에 맞춰서 처리 1234567\"\"\"&gt;&gt;&gt; Account = namedtuple('Account', ['type', 'balance'], defaults=[0])&gt;&gt;&gt; Account._field_defaults&#123;'balance': 0&#125;&gt;&gt;&gt; Account('premium')Account(type='premium', balance=0)\"\"\" Counter Counter_개요 카운터툴은 빠르고 편리한 합계를 위해 제공된다. Iterable한 객체의 값들의 개수를 세기위해서 사용한다. dict까지는 불필요 한데, Set이 중복의 수를 처리할 수 없어서 문제라면, Counter객체를 고려할만하다. 123456789101112131415\"\"\"&gt;&gt;&gt; # Tally occurrences of words in a list&gt;&gt;&gt; cnt = Counter()&gt;&gt;&gt; for word in ['red', 'blue', 'red', 'green', 'blue', 'blue']:... cnt[word] += 1&gt;&gt;&gt; cntCounter(&#123;'blue': 3, 'red': 2, 'green': 1&#125;)&gt;&gt;&gt; # Find the ten most common words in Hamlet&gt;&gt;&gt; import re&gt;&gt;&gt; words = re.findall(r'\\w+', open('hamlet.txt').read().lower())&gt;&gt;&gt; Counter(words).most_common(10)[('the', 1143), ('and', 966), ('to', 762), ('of', 669), ('i', 631), ('you', 554), ('a', 546), ('my', 514), ('hamlet', 471), ('in', 451)] \"\"\" 기존 객체나, ChainMap등에서는 update메서드가 키값에 값을 새롭게 초기화 하지만, Counter에서 update는 중복 시 기존값에 int값을 더한다. 관련 메서드로는 기존값에서 가장 많은 것을 추출하거나, 요소를 그 수만큼 list로 나열하거나, 중복될 경우 수 만큼 빼거나 하는 기존 컨텍스트를 유지하는 기능들이 즐비하다. 1234567891011121314151617\"\"\"# .most_common(i:int)&gt;&gt;&gt; Counter('abracadabra').most_common(3) # doctest: +SKIP[('a', 5), ('r', 2), ('b', 2)]# .subtract(c:Counter) # 없던 키값은 갱신(정수는 값을 줄이고 음수는 값을 늘린다.)&gt;&gt;&gt; c = Counter(a=4, b=2, c=0, d=-2)&gt;&gt;&gt; d = Counter(a=1, b=2, c=-3, d=4 ,e=3)&gt;&gt;&gt; c.subtract(d)&gt;&gt;&gt; cCounter(&#123;'a': 3, 'b': 0, 'c': 3, 'd': -6, 'e': 3&#125;)# .elements()&gt;&gt;&gt; c = Counter(a=4, b=2, c=0, d=-2)&gt;&gt;&gt; sorted(c.elements())['a', 'a', 'a', 'a', 'b', 'b']\"\"\" UserDict UserDict_양상 123456# UserDict([initialdata]:dict)\"\"\"&gt;&gt;&gt; apples:UserDict = UserDict(&#123;'r': 1, 'g':2, 'y':3&#125;)&gt;&gt;&gt; apples.data:dict&#123;'r':1, 'g':2, 'y':3&#125;\"\"\" 이것은 단순히 dict객체를 래핑하고 있는 객체이고, 기본 dict 클래스를 상속받는다. 2.1버전까지는 직접 dict를 서브클래싱 하는 게 불가능했기 때문에, 현재로서는 moot print같은 존재이다.(고려 대상이 아니다.) UserList_User_String 동일하게 data를 통해 원본 객체에 접근이 가능하고, 기본적으로 래핑하고 있어서 서브클래싱을 도와주고 다른 타입으로 만들어주는? 것이라고 만 생각하면 좋다.","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"python module","slug":"python-module","permalink":"https://codenamenadja.github.io/tags/python-module/"}]},{"title":"file path getter","slug":"python/custom_module/file_getter","date":"2019-07-03T08:39:54.000Z","updated":"2021-01-25T08:36:06.079Z","comments":true,"path":"2019/07/03/python/custom_module/file_getter/","link":"","permalink":"https://codenamenadja.github.io/2019/07/03/python/custom_module/file_getter/","excerpt":"","text":"프로젝트 아래 특정하는 폴더 아래의 폴더, 파일 리스트를 검색하고, 완전한 path로 돌려받을 수 있는 모듈 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293import osimport typingROOT_DIR: typing.TextROOT_DIR = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))def _get_all_children_path(folder_name: typing.Text) -&gt; typing.Iterator[str]: abs_path: typing.Text abs_path = ROOT_DIR + \"/\" + folder_name return _map_path(_filter_avails(os.listdir(abs_path)), folder_name)def _filter_avails(files: typing.List[str]) -&gt; typing.Iterator[str]: condition: typing.Callable condition = lambda el: el[0] not in \"._\" return filter(condition, files)def _filter_files( files: typing.Iterator[str], extension: typing.Text) -&gt; typing.Iterator[str]: condition: typing.Callable condition = lambda el: os.path.isfile(el) and extension in el return filter(condition, files)def _filter_folders(files: typing.Iterator[str]) -&gt; typing.Iterator[str]: condition: typing.Callable condition = lambda el: os.path.isdir(el) return filter(condition, files)def _map_path( files: typing.Iterator[str], folder_name: typing.Text) -&gt; typing.Iterator[str]: condition: typing.Callable condition = ( lambda el: f\"&#123;ROOT_DIR&#125;/&#123;el&#125;\" if folder_name is \".\" else f\"&#123;ROOT_DIR&#125;/&#123;folder_name&#125;/&#123;el&#125;\" ) return map(condition, files)def get_things( folder_name: typing.Text = \".\", folder: bool = False, extension: typing.Optional[str] = None,) -&gt; typing.List[str]: \"\"\"Get files and folder under single folder which you specified.Keyword Arguments: folder_name &#123;typing.Text&#125; -- target folder name from ROOT (default: &#123;\".\"&#125;) folder &#123;bool&#125; -- enable finding folders (default: &#123;False&#125;) extension &#123;typing.Optional[str]&#125; -- enable finding files with extension string, if all insert as '' (default: &#123;None&#125;)Raises: FileNotFoundError: foldername not specified ValueError: specific file extension with folder not enableReturns: typing.List[str] -- list out as full path&gt;&gt;&gt; helper_pys = get_things(folder_name=\"helper\", extension=\".py\")&gt;&gt;&gt; any(['python_base/helper/path.py' in el for el in helper_pys])True&gt;&gt;&gt; root_folders = get_things(folder=True, extension=\"\")&gt;&gt;&gt; any(['python_base/helper' in el for el in root_folders])True \"\"\" avail_paths: typing.Iterator filterd: typing.Iterator[str] if not folder_name: raise FileNotFoundError(\"folder_name not specified\") avail_paths = _get_all_children_path(folder_name) if (not folder) and (extension is not None): filterd = _filter_files(avail_paths, extension=extension) elif (extension is None) and folder: filterd = _filter_folders(avail_paths) elif (extension is \"\") and folder: filterd = avail_paths else: raise ValueError(\"extenstion with folder not available\") return [_ for _ in filterd]if __name__ == \"__main__\": import doctest doctest.testmod()","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"custom module","slug":"custom-module","permalink":"https://codenamenadja.github.io/tags/custom-module/"}]},{"title":"python module doctest","slug":"python/module/doctest","date":"2019-06-26T06:36:31.000Z","updated":"2021-01-25T08:36:06.083Z","comments":true,"path":"2019/06/26/python/module/doctest/","link":"","permalink":"https://codenamenadja.github.io/2019/06/26/python/module/doctest/","excerpt":"","text":"Index 개요 doctest.testfile method 개요 doctest모듈은 파이썬 인터프리터 세션과 비슷하게 생긴 텍스트 조각을 찾는다. 그리고 그 세션들을 실행하여, 그들이 보여지는데로 실행되는지 확인한다. doctest를 사용하기 위한 몇가지 일반적인 방식이 있다. 해당 모듈의 docstring들이 잘 업데이트 되어, 모든 상호적인 예시가 적혀진대로 잘 작동하는지 확인하기 위해서. 선형회기 테스팅을 실행하여, 테스트파일 혹은 테스트 객체에 적혀있는 상호적인 예시가 예상한대로 잘 작동하는지 실행하기 위해서. 인풋과 아웃풋을 통해 패키지의 튜토리얼 문서를 작성하기 위해서. 123456789101112131415161718192021222324252627282930313233343536373839404142\"\"\"this is \"example\" module.example module supplies 1 functions, factorial().&gt;&gt;&gt; factorial(5)120\"\"\"def factorial(n:int) -&gt; int: \"\"\"Returns the factorial of n, an exact integer &gt;= 0. &gt;&gt;&gt; [factoral(n) for n in range(6)] [1, 1, 2, 6, 24, 120] &gt;&gt;&gt; factorial(30) 265252859812191058636308480000000 &gt;&gt;&gt; factorial(-1) Traceback (most recent call lst): ... ValueError: n must be &gt;= 0 &gt;&gt;&gt; factorial(30.1) Traceback (most rescent call last): ... ValueError: n must be exat integer \"\"\" import math if not n &gt;= 0: raise ValueError(\"n must be &gt;= 0\") if math.floor(n) != n: raise ValueError(\"n must be exact integer\") if n+1 == n: # catch a value like 1e300 raise OverflowError(\"n too large\") result = 1 factor = 2 while factor &lt;= n: result *= factor factor += 1 return resultif __name__ == \"__main__\": import doctest doctest.testmod() 이것이 생산적으로 doctest모듈을 사용하는데 필요한 모든 정보이다. 이후 섹션은 모든 디테일을 포함한다. 특히 표준 테스트 파일의 Lib/test/test_doctest.py에서는 유용한 예제를 포함한다. TextFile의_docstring_체크 다른 간단한 doctest의 어플리케이션은 텍스트파일의 상호적인 예제를 테스트하는 것이다. 12345678910import doctest doctest&gt;&gt;&gt; doctest.testfile('example.txt')File \"./example.txt\", line 14, in example.txtFailed example: factorial(6)Expected: 120Got: 720 이 짧은 스크립트는 example.txt안에 상호적인 파이썬 예제를 실행하고 확인한다. 123456789101112131415The ``example`` module======================Using ``factorial``-------------------This is an example text file in reStructuredText format. First import``factorial`` from the ``example`` module: &gt;&gt;&gt; from example import factorialNow use it: &gt;&gt;&gt; factorial(6) 120","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"python module","slug":"python-module","permalink":"https://codenamenadja.github.io/tags/python-module/"}]},{"title":"typing","slug":"python/module/typing","date":"2019-06-25T04:07:47.000Z","updated":"2021-01-25T08:36:06.083Z","comments":true,"path":"2019/06/25/python/module/typing/","link":"","permalink":"https://codenamenadja.github.io/2019/06/25/python/module/typing/","excerpt":"","text":"Index 개요 타입 알리아싱 NewType Callable Generic 사용자 정의 generic types 개요 가장 주요한 구조적 지원은 Any, Union, Tuple, Callable, TypeVar, Generic의 타입들로 이루어진다. Type_aliases 타입-별칭은 타입을 alias로 치환함으로서 정의된다. 예를들어 Vector와 List[float]는 서로 교환가능한 동의어로써 사용될 수 있다. 12345678from typing import ListVector = List[float]def scale(scalar: float, vector:Vector) -&gt; Vector: return [scalar * num for num in vector]# typechecks: 실수의 리스트는 Vector로 정의되었다.new_vector = scale(2.0, [1.0, -4.2, 5.4]) Type aliases는 복잡한 타입을 간단히 정의하기에 좋다 123456789101112from typing import Dict, Tuple, SequenceConnectionOptions = Dict[str, str]Address = Tuple[str, int]Server = Tuple[Address, ConnectionOptions]# 정적인 타입체커는 아래 타입을 위의 타입 별칭과 동일하게 취급할 것이다.def boardcase_message( message: str, servers: Sequence[Tuple[Tuple[str, int], Dict[str, str]]],) -&gt; None: pass NewType NewType()헬퍼 함수를 통해서 고유한 타입을 생성할 수 있다. 1234567from typing import NewTypeUserId = NewType('UserId', int)some_id = UserId(524313)&gt;&gt;&gt; type(some_id)int 우리는 여전히 모든 int연산을 UserId타입에서 수행할 수 있다. 하지만 결과는 모두 int타입이 될 것이다. 이러한 테크는 정적인 타입 체커에서만 유효할 것이다. 런타입 레벨에서 sometype = NewType('sometype', Base)는 sometype을 즉시 리턴하는 함수로 취급하여, 전달한 파라메터를 해당 Base-타입으로 돌려보낼 것이다. 그 말인즉슨, Derived(some_value)라는 표현은 새로운 클래스나 어떤 오버해드도 일반적인 함수 호출에서 추가되는 것이 아니라는 것이다. 더 깊히 얘기하자면, some_value is Derived(some_value)는 런타임에서는 항상 옳다는 것이다. 이것은 런타임 소모없이 논리적 에러를 예방하고자 할때 도움이 된다. Callable 프레임워크들은 특별한 정의의 콜백함수가 Callable[[ArgType, Arg2Type], ReturnType] 같은 형식을 이용해서 타입힌트 되는 것을 기대한다. 에를 들어, 12345678910from typing import Callabledef feeder(get_next_item: Callable[[], str]) -&gt; None: passdef async_query( on_success: Callable[[int], None], on_error: Callable[[int, Exception], None] ) -&gt; None: pass Callable[..., ReturnType] 매개변수의 리스트 타입을 생략으로 대신하는 것으로서 callable의 타입을 정의하는 것도 가능하다. Generic 컨테이너 내부에 잡혀있는 객체들에 대한 타입의 정보가 정적으로 유추될 수 없기 때문에, abstract base classes들은 에상되는 타입들을 컨테이너 요소들을 위해서 기록하기 위해 버전 업 되었다. 1234567from typing import Mapping, Sequencedef notify_by_email( employees: Sequence[Employee], overrides: Mapping[str, str]) -&gt; None: ... 제네릭들은 TypeVar이라는 새로운 타이핑 팩토리를 통해서 파라메터화 될 수 있다. 123456from typing import Sequence, TypeVarT = TypeVar('T') # Type변수 선언def first(l: Sequence[T]) -&gt; T: return l[0] User-defined_generic_types 이 부분은 제네릭 타입을 통해서 클래스를 동적으로 변화시키는 것처럼 보여진다. 해당 제네릭 타입에 대한 동적인 상속이 이루어지기 때문에, 그렇게 권고하고 싶은 방식이 아니라고 생각한다. 제네릭은 매개변수로서만 쓰여지는 정도가 적당하다고 생각하기 때문에, 이 부분에 대해서는 필요한 경우 공식문서에서 별도로 참조하자.","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"python module","slug":"python-module","permalink":"https://codenamenadja.github.io/tags/python-module/"}]},{"title":"type-annotation in python","slug":"python/post/annotation_and_type","date":"2019-06-24T10:41:31.000Z","updated":"2021-01-25T08:36:06.083Z","comments":true,"path":"2019/06/24/python/post/annotation_and_type/","link":"","permalink":"https://codenamenadja.github.io/2019/06/24/python/post/annotation_and_type/","excerpt":"","text":"Index Annotation의 정의와 필요성 typing module을 통한 annotation확장 annotation이 docstring을 대체하는가? 포메팅 및 테스트 자동화 Annotation PEP-3107에서 annotation을 소개하고 있다. 기본 아이디어는 힌트를 주는 것이지만, 인터페이스가 구축된 코드를 다룰 수 있도록 함으로, 더욱 정교한 구현이 가능하다. 어노테이션은 타입 힌팅을 활성화 한다. 타입 뿐 아니라 변수를 이해하는데 도움이 되는 어떤형태의 메타데이터라도 지정할 수 있다. 123456class Point: def __init__(self, lat, long): self.lat = lat self.long = longdef locate(latitude:float, longitude:float) -&gt; Point: \"\"\"맵에서 좌표에 해당하는 객체를 탑색\"\"\" 이를 통해 함수 사용자는 예상되는 타입을 알 수 있다. 하지만, 파이썬이 타입을 검사하거나 강제하지는 않는다. 또 위처럼 함수 반환 값에 대한 예상 타입을 지정할 수 도 있다럼 그러나 어노테이션으로 타입만 지정할 수 있는 것은 아니다. 파이썬 인터프리터에서 유효한 어떤 것도 사용할 수 있다. 예를 들어 변수의 의도를 설명하는 문자열, 콜백이나 유효성 검사 함수로 사용할 수 있는 callable등이 있다. 어노테이션을 사용하면 __annotations__라는 특수 속성이 생긴다. 이 속성은 어노테이션의 이름과 값을 매핑한 사전 타입의 값이다. 앞의 예제에서는 다음과 같이 출력된다. 12&gt;&gt;&gt; locate.__annotations__&#123;'latitude':float, 'longitude': float, 'return': __main__.Point&#125; PEP-484 “파이썬은 여전히 동적인 타입의 언어로 남을 것이다, 타입 힌트를 필수로 하자거나 심지어 관습으로 하자는 것은 전혀 아니다.” 코드 전체에 올바른 타입이 사용되었는지 확인하고, 호환되지 않는 타입이 발견되었을떄 사용자에게 힌트를 주는 것이다. 이러한 검사를 도와주는 Mypy에 대해서는 나중에 프로젝트에서 사용하는 방법에 대해 자세히 설명한다. 타입 힌팅은 코드의 타입을 확인하기 위한 도구 이상의 것을 의미한다. 파이썬 3.5부터 새로운 typing모듈이 소개되었고 파이썬 코드에서 타입과 어노테이션을 정의하는 방법이 크게 향상되었다. 기본 개념은 코드의 시멘틱이 보다 읜미있는 개념을 갖게 되면, 코드를 이해하기 쉽고 특적 시점에 어떻게 될지 예측할 수 있다는 것이다. typing python module typing 에서 개괄적으로 다루고 있다. typing 모듈을 사용하면 파이썬에게 이터러블, 시퀸스가 필요하다고 알릴 수 있다. 심지어 타입이나 값을 식별할 수도 있다. 예를 들어 정수의 시퀸스가 필요하다고 정의하는 것이다. PEP-526 파이썬 3.6부터는 함수 파라미터와 리턴타입 뿐만 아니라, 변수에 직접 주석을 달 수있다. 123456class Point: lat: float long: float&gt;&gt;&gt; Point.__annotations__&#123;'lat': &lt;class: 'float'&gt;, 'long': &lt;class 'float'&gt;&#125; docstring_annotation python module doctest에서 doctest와 함께 다룬다. 어노테이션은 docstring을 대체하는 것일까? 어노테이션을 대체하기 위한 동적언어로서 초기 서브형태가 파이썬에 있어서는 docstring이라고 할 수 있다. 기존에는 docstring을 통해 함수의 파라미터, 속성의 타입을 문서화 하였기 때문에, docstring 대신 annotation을 사용해야 하는 것인지 궁금할 수 있다. 심지어 함수의 기본정보, 파라미터 의미와 타입, 반환값, 발생가능한 예외까지 docstring으로 작성하는 포매팅 컨벤션도 유행했었다. 대답은 '예, 사용해야합니다.'이다. 왜냐하면 docstring과 annotation은 서로 보완적인 개념이기 때문이다. docstring에 포함된 정보를 일부 annotation으로 이동 시킬 수 있었던 덕에 docstring은 좀더 명확한 목적을 가질 수 있게 됐다. docstring을 통해 친절한 설명과 &gt;&gt;&gt; doit()문을 통해 doctest와 연동하는 것이다. 친절한 설명이라 함은, 동적 데이터 타입과, 중첩 데이터 타입의 경우 예상 데이터 예제를 제공하여 어떤 형태의 데이터를 다루는지 제공하는 것이 좋다. 1234def data_from_response(responst:dict) -&gt; dict: if response[\"status\"] != 200: raise ValueError return &#123;\"data\": response[\"payload\"]&#125; 이 함수는 사전 형태의 파라미터를 받아 사전 형태의 값을 반환한다. 그러나 상세한 내용은 알 수 없다. response객체의 올바른 인스턴스는 어떤 형태일까? 결과의 인스턴스는 어떤 형태일까? 이 두가지 질문에 모두 대답하려면 환수 반환값의 예상 형태를 docstring으로 문서화 하는 것이 좋다. 1234567891011121314151617def data_from_response(response:dict) -&gt; dict: \"\"\"if response has valid 'status' returns response[\"payload\"] - response sample &#123; \"status\":200, # int \"timestamp: \"....\", # 현재시간 ISO포멧 문자열 \"payload\": &#123;...&#125; # 반환하려는 데이터 &#125; - return sample &#123;\"data\":&#123;...&#125;&#125; - exceptions - if HTTP status != 200 raise ValueError \"\"\" pass 이 문서는 입출력 값을 더 잘 이해하기 위해서 뿐만 아니라, 단위테스트에서도 유용한 정보로 사용된다. 하지만 docstring을 사용했을때 의 이슈는 코드가 좀 더 커지게 되고, 실제 효과적인 문서가 되려면 보다 상세한 정보가 필요하다는 것이다. Makefile_자동검사설정 리눅스 개발환경에서 빌드를 자동화 하는 가장 일반적인 방법은 Makefile을 사용하는 것이다. Makefile은 프로젝트를 컴파일하고 실행하기 위한 설정을 도와주는 파워풀한 도구이다. 빌드 외에도 포메팅검사나 컨벤션 검사를 자동화하기 위해 사용할 수도 있다. 123456789101112131415161718typehint:mypy src/ tests/test:pytest tests/lint:pylint src/ tests/code-format:black - 1** 79 *.pydoc-list:pydocstyle src/ tests/checklist: lint typehint test.PHONY: typehint test lint checklist make checklist 이 중 어떤 단계라도 실패하면 전체 프로세스가 실패한 것으로 간주한다.","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"python post","slug":"python-post","permalink":"https://codenamenadja.github.io/tags/python-post/"}]},{"title":"docker basic","slug":"linux/docker/basic","date":"2019-06-20T03:59:09.000Z","updated":"2021-01-25T08:36:06.071Z","comments":true,"path":"2019/06/20/linux/docker/basic/","link":"","permalink":"https://codenamenadja.github.io/2019/06/20/linux/docker/basic/","excerpt":"","text":"Index DOCKER CE 설치 이후 DOCKER sample another 출력기능 간단한 구현 나아가서 구현 마치며 이미지_생성하기 우분투 이미지 다운로드 1docker run ubuntu: sd sds 파이썬_환경_구성 1ADD ./conf/uwsgi/uwsgi.ini /var/www/django/ini/uwsgi.ini index 폴더_마운팅 1docker container run --rm -it -p 8080:80 -v /Users/junehan/Desktop/docker_test/code:/var/www/django/code django:some_tag 12[uwsgi]chdir= $(BASE_DIR)/code another_출력기능 12def another(): print('another') 백그라운드 나아가서_구현 마치며","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://codenamenadja.github.io/tags/docker/"}]},{"title":"functools의 wraps 사용하기","slug":"python/module/wraps","date":"2019-06-19T10:35:43.000Z","updated":"2021-01-25T08:36:06.083Z","comments":true,"path":"2019/06/19/python/module/wraps/","link":"","permalink":"https://codenamenadja.github.io/2019/06/19/python/module/wraps/","excerpt":"","text":"INDEX 1. 공식문서 정의 1_공식문서_정의 functools.wraps(wrapped, assigned=WRAPPER_ASSIGNMENTS, updated=WRAPPER_UPDATES) 이것은 update_wrapper()를 함수 데코레이터로서 호출하는 편리한 함수이다. wrapper함수를 정의할 때, 이것은 아래와 동일하다. partial(update_wrapper, wrapped=wrapped, assigned=assigned, updated=updated) 예를 들어, 12345678910111213141516171819202122&gt;&gt;&gt; from functools import wraps&gt;&gt;&gt; def my_decorator(f): @wraps(f) def wrapper(*args, **kwargs): print('Calling decorated func') return f(*args, **kwargs) return wrapper&gt;&gt;&gt; @my_decorator def example(): \"\"\"DocString\"\"\" print(\"Called example function\")&gt;&gt;&gt; example()Calling decorated funcCalled example function&gt;&gt;&gt; example.__doc__'DocString'&gt;&gt;&gt; example.__name__'example'","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"python module","slug":"python-module","permalink":"https://codenamenadja.github.io/tags/python-module/"}]},{"title":"functional programming howto 번역","slug":"functional_programming/python/functional_programming_howto","date":"2019-06-19T09:57:26.000Z","updated":"2021-01-25T08:36:06.071Z","comments":true,"path":"2019/06/19/functional_programming/python/functional_programming_howto/","link":"","permalink":"https://codenamenadja.github.io/2019/06/19/functional_programming/python/functional_programming_howto/","excerpt":"","text":"Intoduction 대부분의 연어는 절차적이다 프로그램은 인스트럭션의 리스트이다. 그 지침들은 컴퓨터에게 프로그램의 인풋으로 부터 무얼 해야할 지에 대해서 알려준다. C, pascal, Unix셸 마저 절차적인 언어이다. 선언형 언어에서는, 너는 풀려져야 할 문제에 대한, 특질을 기술한다. 그리고 언어의 실행 및 구현은 컴퓨터적으로 어떻게 효율성을 수행해야 할지 알려준다. 객체지향프로그램은 객체의 콜렉션을 다룬다리 객체는 내부적인 상태를 가지고 있고, 그것을 쿼리하거나, 내부적인 상태를 수정하는 메서드를 지니고 있다. Smalltalk와 java는 객체지향 언어이다. C++과 파이썬은 객체지향을 지원하는 언어이다. 그러나 객체지향을 강요하진 않는다. 함수형 프로그램은 문제를 functions의 조합으로 분해한다. 함수들은 인풋만을 받고, 아웃풋을 내놓는다. 내부적인 상태가 인풋에 대응하는 아웃풋에 영향을 주지 않는다. 잘알려진 함수형 언어는 MLfamily와(ML,OCaml, 그리고 다양한 것들) Haskell이 있다. (중략) 함수형 프로그래밍은 객체지향의 반대 쪽으로 고려될 수 있다. 객체들은 내부적인 상태를 캡슐하고 있고, 그것은 메서드의 컬렉션을 통해서 내부적인 상태를 조정한다. 그리고 프로그램이 상태를 변화시키기 위한 적절한 세트를 지니고 있다. 함수형 프로그래밍은 상태의 변화를 최대한 피하려고 한다, 그리고 functions사이에서 데이터의 흐름을 다루려고 한다. 파이썬에서는 이 두 가지 접근법을 조합해서, 객체를 표현하는 인스턴스를 리턴할 수 있다. 함수형 디자인은, 그 밑에서 일하기 에는, 이상한 강제로 보일 수 있다. 왜 객체는 사이드 이펙트를 피해야만 하는가? 함수현 언어에는 이론적이고 실용적인 이점이 있다. 형식으로 입증가능 모듈성 조합성 디버그와 테스팅의 편리함 (중략) iterators iterators는 파이썬의 함수형 스타일의 중요한 기초이다. iterator은 데이터의 스트림을 대표하는 객체이다. 이 객체는 한번에 하나의 데이터만 리턴한다. 파이썬 이터레이터는 __next__()라는 메서드를 지원해야한다. 그것은 어떤 매개변수를 받지 않고, 스트림의 다음 요소만을 반환한다. 만약 다음요소가 없다면, 그것은 StopIteration예외를 발생시킨다. iterator은 한정되야할 필요는 없다. 이터레이터가 무한한 스트림의 데이터를 생성하는 것은 합리적인 것이다. 빌트인 iter()함수는 그 aribitary한 객체를 받아서, 그 객체의 컨텐츠나 요소들을 반환하는 iterator을 반환하려고 한다. 만약 객체가 iteration을 지원하지 않는다면, TypeError을 Raise한다. 몇몇 파이썬 빌트인 데이터 타입은 iteration을 지원한다. 가장 흔한 것은 lists와 dictionary들이다. 객체는 iterable하다고 불릴 수 있다, 만약 거기서 iterator을 얻을 수 있다면. 123456789101112131415&gt;&gt;&gt; L = [1, 2, 3]&gt;&gt;&gt; it = iter(L)&gt;&gt;&gt; it #doctest: +ELLIPSIS&lt;...iterator object at ...&gt;&gt;&gt;&gt; it.__next__() # same as next(it)1&gt;&gt;&gt; next(it)2&gt;&gt;&gt; next(it)3&gt;&gt;&gt; next(it)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;StopIteration&gt;&gt;&gt; 파이썬은 iterable objects를 다양한 컨텍스트에서 예상한다. 가장 중요한 개체는 for진술이다. for X in Y 진술에서, Y는 이터레이터거나, iter()가 iterator을 생성할 수 있는 것이다. 아래 두 진술은 동일하다. 12345for i in iter(obj): print(i)for i in obj: print(i) 이터레이터는 list, tuple등으로 물질화 될 수 있다. list() tuple() constructor함수를 사용함 으로써. 12345&gt;&gt;&gt; L = [1, 2, 3]&gt;&gt;&gt; iterator = iter(L)&gt;&gt;&gt; t = tuple(iterator)&gt;&gt;&gt; t(1, 2, 3) Sequence unpacking은 이터레이터를 지원한다. 만약 이터레이터가 N개의 엘레먼트를 반환할 것을 네가 알고 있으면, N-tuple로 Unpack할 수 있다. 12345&gt;&gt;&gt; L = [1, 2, 3]&gt;&gt;&gt; iterator = iter(L)&gt;&gt;&gt; a, b, c = iterator&gt;&gt;&gt; a, b, c(1, 2, 3) for X in somelist는 물질화된 것이고 for X in iterator은 그렇지 않은 것이다. 갯수가 정말 작다면, 혹은 단계적으로 찾아가서 특정조건까지 Search하는 경우가 아니라면 materialed한 Iterator을 사용하는 것이 더 바람직 할 수 있으나, max() min() if X in iterator 등의 경우에 무엇이 효과적인지 잘 알고 있을 것이다. 파이썬의 리스트는 지역성이 제한적으로 성립되는 value의 모임이 아닌, reference들의 모임이기 때문에. offset만큼 정확히 이동하지도 않고, 다소 애매할 수 있는 존재이다. 만약 list의 N 번쨰 요소가 이미 메모리에서 잡아 놓았던 값이라면, 그 N번쨰 방에는 생성된 값이 지역성과 무관하게 기존에 잡혀있는 메모리에 대한 레퍼런스를 특징함으로 파이썬은 메모리를 절약한다. 그렇다고 봤을때, 리스트의 시퀸스가 지역성을 보장하지 않는다면, 하나씩 레퍼런스 값을 생성하는, Iterator이 우세하다. max(), min()같은 빌트인 함수들은 하나의 이터레이터 매개변수를 받아서 하나의 결과를 돌려줄 수 있다. “in”, “not in” 연산자 또한 이터레이터를 지원한다.: stream안에서 X를 찾는다면, True아니면 False이다. 너는 명백한 문제로 달려들 것이이다, 만약 이터레이터가 무한하다면, 절때 스트림에서 최대값이나 최소값을 찾을 수 없고, 스트림안에 존자하는지 아닌지에 대한 결과도 무한히 지속되어 끝나지 않을 것이기 때문이다. 이터레이터를 지원하는 데이터 타입 우리는 이미 어떻게 리스트와 튜플이 이터레이터를 지원하는 지 보았다. 사실 어떤 파이썬 시퀸스 타입은 자동으로 이터레이터의 셍성을 도울 것이다. iter()를 딕셔너리에 대해서 호출하는 것은, Keys에 대해서 루프 하게 된다. 12345678910111213141516m = &#123;'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6, 'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12&#125;for key in m: print(key, m[key])Jan 1Feb 2Mar 3Apr 4May 5Jun 6Jul 7Aug 8Sep 9Oct 10Nov 11Dec 12 파이썬 3.7 딕셔너리 이터레이션의 순서는 주입된 순서대로 보장된다. iter()을 사용하면 자동으로 키를 기반으로 이터레이션 루프가 적용된다. 그러나 딕셔너리 그 자체에 이미 다른 이터레이터를 반환하는 메서드가 있다. items() : (key, val)Pair로 원본 객체에 대한 view객체로 만들어준다. values(),keys() : 위와 동일하게 원본에 대한 레퍼런스를 유지한다.(dynamic-view) (Key, Value)- 튜플로 구성된 시퀸스를 dict()로 wrap해주면, 마찬가지로 원본의 형태로 객체로 형태를 돌려서 주지만, 원본과의 레퍼런스는 끊어진다. 파일에서는, readline() 그리고 set는 그 자체로 iterable하다. Generator expressions and list comprehensions Two common operations on iterator’s output-&gt; Performing some operation for every elem seslction a subset of elems that meet some condition List comprehension N generator expressions는 그러한 실행의 구체적인 언급이다. 그것들은 하스켈에서부터 가져온 것이다. 12345line_list = [' line 1 \\n', 'line 2 \\n',]#gen expression -- return iteratorstirpped_iter = (line.strip() for line in line_list)#list comprehsions -- returns liststripped_list = [line.strip() for line in line_list] List comprehension을 통해 너는 파이썬의 리스트를 받는다. 그것은 물질화 된 것이다. nested-Generator expression은 반면에, 물질화된 것을 돌려주는 것이 아니라, 필요한 때에 하나씩 value를 compute해준다. 그것은 list comprehension이 계속적으로 스트림을 이어가야 하는 작업, 무한히 이어지는 시퀸스에 대해서 적합하지 않다는 것이다. Generator-exp는 &quot;()&quot;에 감싸여서, 사용되는데 아래와 같다. 1234567891011121314( expression for expr in sequence1 if condition1 for expr2 in sequence2 if condition2 for expr3 in sequence3 ... if condition3 for exprN in sequenceN if conditionN )seq1 = 'abc'seq2 = (1, 2, 3)[(x, y) for x in seq1 for y in seq2] #doctest: +NORMALIZE_WHITESPACE# [('a', 1), ('a', 2), ('a', 3),# ('b', 1), ('b', 2), ('b', 3),# ('c', 1), ('c', 2), ('c', 3)] if는 해당 시퀸스 레벨에서 적용되는 것이고, 가장 오른쪽을 최초로 왼쪽으로 점차 wrap해 나간다. (중략 - 아는 내용 너무 많음) Passing values into a generators 파이썬 2.4포함 이전 버전에서는, 제너레이터는 오직 아웃풋만을 생산할 수 있었다. 제너레이터 내부로 값을 보내는 것은 불가능 했다. 12345678910111213def gens() f_val = (yield 1) s_val = (yield f_val) return 'done'outergens = gensf_yield = outergens.send(None)# expect f_yield == 1 is Trues_yield = outergens.send(999) # send999 =&gt; execution on outergens works # 1. f_val delegates 999 # 2. yield f_val =&gt; loadFast f_val(:999) 나는 yield표현 주변에 항상 괄호를 사용하는 것을 추천한다. 실제 바이트코드 단위에서 val에 대한 할당 이전에 execution을 main컨텍스트로 전환하기 때문에, 그 이전에 yield 1 이라는 컨텍스트를 돌려주기 전에 전달된 값은 무엇이든 무시된다. 그래서 관례상 conventional Rule로서, 처음에 None만을 send 할 수 있도록 강제되어 있다. 2번쨰 send를 할때 전달된 값이 제너레이터 내부로 컨텍스트 전환과 함께 할당이 이루어 지면서, yield에서 전달값에 대한 로컬 네임스페이스를 리턴하라는 명령을 만나면, 로드하고, 리턴한다. yield문이 추가로 있어도, return문을 만나면 stopIteration을 Raise한다. pep 342는 정확한 룰을 설명한다. yield-expression은 항상 parenthesized해야한다. 반면 이런표현에서 주의해야한다. 123val = yield i # 라고 사용해도 되지만,# 아래와 같은 경우는 꼭 괄호를 사용해라val2 = ((yield i) + 12) when it occurs at top-level expression on the right-hand side of an assignment. 최상위 표현에서, 오른편에 다른 연산이 있을 경우 무조건 괄호를 사용하라. 값들은 제너레이터에 send(value)를 통해서 전달된다. 이 메서드는 제너레이터의 __code__의 컨텍스트로 진입하고, yield표현은 특정 값을 리턴한다. 만약 일반적인 __next__()가 외부에서 불려진다면, 제너레이터 내부에서 yield는 아무것도 반환하지 않는다. 여기에 1씩 증가시키고, 내부적인 카운터를 변화시키는 것을 허락하는 단순한 함수가 있다. 12345678910def counter(max): i = 0 while i &lt; max: val = (yield i) # suspend line # if value provided, change counter # 만약 send를 통해서 val이 외부에서 주입되었다면, 그 값으로 i를 바꾸고 진행한다. if val is not None: i = val else: i += 1 1234567891011121314it = counter(10) # max를 10으로 잡아놓은 제너레이터i1 = next(it) # return it.send(None)과 동일# i1 is 0i2 = it.send(None) # &lt;- is same with i2 = next(it)# i2 is 1i3 = it.send(8) # suspend line에 8을 리턴받고, val은 8이 된다.# 내부적으로 다음 yield문을 찾아 진행한다.# i3 == 8i4 = next(it)# i4 == 9i5 = next(it)# 10으로 바뀌고 POPTOP을 하지 않고,# return None문을 만나 Stopiteration을 Raise send가 기본으로 일어나는 작용이니, next를 통해서 계속 하기 보다는, send(None)이 본래 모습이라는 것을 정확히 캐치하고 사용하길 바란다. send()에 따라서, 제너레이터에는 2가지 메서드가 더 있다: throw(type, value = None, traceback =None) 제너레이터 내부의 컨텍스트에서 Exception을 raise한다; 멈춰진 yield의 시점을 통해서 예외처리가 된다. close() 제너레이터가 이터레이션을 삭제시키기 위해서, GeneratorExit라는 exception을 발생시킨다. 이 exception을 받을때에, 제너레이터의 코드는 GeneratorExit 또는 StopIteration을 내부적으로 처리해야 한다. 예외를 처리하는 어긋난 것(어떠한 에러를 Raise)을 행한다면, RuntimeError.close()를 촉발시킨다. 그것은 파이썬 garbage collector에게서 불려지는 것이며, 제너레이터가 소멸되는 것이다. 만약 GeneratorExit가 일어날때, 코드를 정리하고 싶다면, 12345678910111213141516while True: try: res = mygen1.send(None) except Exception as e: print(type(e)) if isinstance(e, StopIteration): print(\"iterStop\") if isinstance(e, GeneratorExit): print(\"Gen Exit\") finally: print(res) print(\"done\") mygen1.close() # res를 제너레이터부터 받으면 출력하고 바로 close() # close()로 내부적으로 GeneratorExit가 일어나고, # Stopiteration으로 연결된다. 이러한 변화의 누적은, 제너레이터를 단방향 정보 생산자에서, 정보의 소비자인 동시에 생산자로 만들었다. 제너레이터는 또한 coroutine이 된다. subroutine은 컨텍스트의 주도권을 갖은 후 최상위에서 시작해서, return 지점에서 끝나지만, coroutine은, 시작되고, 종료되고, 재진입이 매우 다양한 위치에서 진행된 수 있다.(yield선언문을 통해서) Built-in functions","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"functional programming","slug":"functional-programming","permalink":"https://codenamenadja.github.io/tags/functional-programming/"}]},{"title":"파이썬 bytearray 모듈 클래스 정리","slug":"python/module/bytearray","date":"2019-06-19T09:55:32.000Z","updated":"2021-01-25T08:36:06.079Z","comments":true,"path":"2019/06/19/python/module/bytearray/","link":"","permalink":"https://codenamenadja.github.io/2019/06/19/python/module/bytearray/","excerpt":"","text":"Syntax 1bytearray([source[, encoding[,errors]]]) source optional, 만약 소스가: 문자라면, 인코딩이 필수적 숫자라면, array가 해당 사이즈를 갖고 빈 바이트들로 초기화 버퍼인터페이스를 따르는 객체라면, 읽기전용 버퍼객체가 bytes array를 초기화하는데 사용될 것이다. encoding 선택적이다, 문자일 경우에만 필수적. 일반적인 값으로 “ascii”, “utf-8”, “windows-1250”, &quot;windows-1252&quot;등을 받는다. errors 선택적이다, 가능한 값들은,: ‘strict’ : 인코딩 에러가 날 경우 에러를 raise ‘replace’ : 흉하게 생긴 데이터를 적절한 마크로 변환 시킨다., ‘?’, 'uffd’같은경우 ‘ignore’ : 흉하게 생긴 데이터를 무시하고, 알림없이 진행한다. ‘xmlcharrefreplace’ : 적절한 XML문자 레퍼런스로 대체한다.(인코딩전용) ‘backslashreplace’ : \\를 이스케이프 시퀸스로 사용한다.(인코딩전용) Remarks 바이트배열 타입은 변환할 수 있는 숫자의 시퀸스, 0&lt;=x&lt;256을 가진다. 낮은 레벨의 바이너리 데이터를 다루는 데 사용될 수 있다. 예를 들면, 이미지 내부, 네트워크에서 직접 전달 받을때. Example1 12345678bytearray()# bytearray(b'')bytearray(4)# bytearray(b'\\x00\\x00\\x00\\x00')bytearray([0,1,2])# bytearray(b'\\x00\\x01\\x02')bytearray(buffer('hello'))# bytearray(b'hello') Example2 1234567891011121314151617bytearray('hello', 'ascii')# bytearray(b'hello')bytearray(u'źdźbło', 'ascii', 'strict') #’blade of grass’ in polish# UnicodeEncodeError: 'ascii' codec can't encode character u'\\u017a' in position 0: ordinal not in range(128)bytearray(u'źdźbło', 'ascii', 'ignore')# bytearray(b'dbo')bytearray(u'źdźbło', 'ascii', 'replace')# bytearray(b'?d?b?o')bytearray(u'źdźbło', 'ascii', 'xmlcharrefreplace')# bytearray(b'&amp;#378;d&amp;#378;b&amp;#322;o')bytearray(u'źdźbło', 'ascii', 'backslashreplace')# bytearray(b'\\\\u017ad\\\\u017ab\\\\u0142o')","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"python module","slug":"python-module","permalink":"https://codenamenadja.github.io/tags/python-module/"}]},{"title":"SocketProgramming_HOW 번역","slug":"python/post/SocketProgramming_HOWTO","date":"2019-06-19T09:48:54.000Z","updated":"2021-01-25T08:36:06.083Z","comments":true,"path":"2019/06/19/python/post/SocketProgramming_HOWTO/","link":"","permalink":"https://codenamenadja.github.io/2019/06/19/python/post/SocketProgramming_HOWTO/","excerpt":"","text":"Author: Gordon McMillan 출처 : https://docs.python.org/3.7/howto/sockets.html#socket-howto 1. Sockets INET(i.e IPv4)소켓에 대해서만 이야기하겠다. 하지만 그들이 99%의 실사용 소켓이다. 그리고 STREAM TCP소켓에 대해서만 이야기 할 것이다. 니가 만약 무얼하고 있는지 알고 있다면, 그 무엇보다 스트림 소켓에서 더욱 좋은 퍼포먼스를 가지게 될 것이다. 소켓이 무엇인지에 대한 미스터리를 정리하려 할 것이다. 블로킹 소켓과 NON-blocking소켓에 대해서도 조금 힌트를 준다. 그렇지만, 일단 blocking소켓에 대해서 이야기하는 것으로 시작하겠다. 너는 그들이 어떻게 넌블러킹 이전에 해왔는지 알 필요가 있다. 이들을 이해하는데 생기는 문제는, socket이라는게 미묘하게 다른 다수의 것을 뜻할 수 있다는 것이다. 그러니 첫째로, client socket, server socket사이에 구분을 해보자.(서버 소켓이 더 switch board operator같다.) 클라이언트 어플리케이션 예를 들어 너의 브라우저는, 클라이언트 소켓만을 사용한다. 웹서버는 서버소켓과 클아이언트 소켓을 동시에 사용한다. 2. History IPC(Inter Process Communication)의 다양한 form에서, 소켓은 가장 유명하다. 어떤 플랫폼에서도, 그들은 더욱 빠르고, cross-platform커뮤니케이션이다. 소켓은 유일하게 지금 게임이 되는 존재다. 3. Creating a Socket 네가 링크를 클릭했을떄, 너의 브라우저는 이러한 것들을 한다. 1234# create an INet, STREAMing sockets = socket.socket(socket.AF_INET, socket.SOCK_STREAM)# now connect to web server on port 80s.connect((\"www.python.org\",80)) connect가 끝났을 때, 소켓 s는 페이지에 대한 리퀘스트를 보내기 위해서 사용될 수 있다. 동일한 소켓이 읽고 응답할 것이다. 그리고 소멸된다. 클라이언트소켓은 기본적으로 한번의 교환을 위해서 사용된다. 웹서버에서는 조금더 복잡하다. 첫쨰로 웹서버는 'server socket’을 생성한다. 12345# create INET, STREAMing socketserversocket = socket.socket(socket.AF_INET,socket.SOCK_STREAM)# bind the socket to public host, and a well-known portserversocet.bind((socekt.gethostname(),80))serversocket.listen(5) 몇 가지 알아야 할 사실: 우리는 socket.gethostname()을 사용하여 소켓이 외부 publicIP에 보일 수 있도록 했다. local안에서 설정했다면, 동일 기계안에서만 보이게 될것이다. 두번째 알아야할 사실: well-known port는 public상의 효율성을 위한 약속이다. 네가 만약, 놀고싶으면 4자리 이상의 포트를 사용해라. 마지막: listen의 argument는 소켓라이브러리로 하여금, 우리가 queue를 5개 connect리퀘스트를 올리길 원한다는 것이다.(5 is normal max). 이후 외부 connect는 거절된다. 다른 코드가 적절히 구성되었다면, 그것은 충분할 것이다. 이제 우리가 서버소켓이 80번 포트에 있으니, 우리는 웹서버의 메인루프에 접근할 수 있다. 1234567while True: # accept connection from outside (clientsocket, address) = server.socket.accept() # noe do something with clientsocket # in this case, we'll pretent this is threded server ct = client_thread(clientsocket) ct.run() 사실 3가지 일반적인 방법이, 이것의 루프를 가능하게 하는 방법으로 있다. 쓰레드를 붙여서, 클라이언트 소켓을 제어하는 것. 앱을 재구성해서, nonblocking socket을 사용하게 하는것. select를 활용하여 서버 소켓과 어떤 활성화된 클라이언트 소켓 사이를 다양화 시킨다. 알아야 하는 중요한 것은 이것이다. 이것에 서버소켓이 하는 모든 것이다. 이것은 어떤 데이터를 받지도 않고 데이터를 보내지도 않는다. 이것은 그저 클라이언트 소켓을 생성한다. 각 클라이언트 소켓은 어떤 클라이언트의 소켓이 host와 그의 bound된 port에 connect()함으로써 생성된다. 해당 클라이언트 소켓을 빠르게 우리가 생성한다면, 우리는 더 많은 연결을 기다리도록 돌아갈 수 있다.(listen(5)) 2명의 클라이언트는 자유롭게 소통할 수 있다. 그들은 dynamically allocated port를 사용한다. 어떤 대화가 끝났을때 재사용 되는 포트. 4. IPC(interProcessCommunication) 네가 만약 2개의 프로세스 사이에 빠른 IPC를 원한다면, pipes나 shared memory를 생각해 봐야한다. 네가 만약 AF_INET소켓을 사용하기로 결정했다면, 서버소켓을 localhost에 bind해라. 대부분의 플랫폼에서, 이것은 네트워크의 몇개의 레이어로 하여금 shortcut을 사용하도록 해서, 더 빠르고 덜 요란하게 성립하게 해준다. See also: 멀티프로세싱은 crosee-platform IPC를 higher-level API로 통합시킨다. 5. Using Socket 첫번째로 기억해야 할 것은 웹 브라우저의 클라이언트소켓과 웹 서버의 클라이언트소켓은 개별적인 괴물들이다. 그 말은, 이것은 peer to peer 대화 라는 것이다. 다른식으로 얘기하자면, 데자이너로서 너는 대화를 위한 에티켓으로 무엇을 할지를 정해야 할 것이다. 보통 소켓을 연결시키는 것은, 대화를 시작하는 것이다. 리퀘스트를 보냄으로써, 혹은 조직된 행동에 동의하기로 서명함으로써. 조직된 행동이란 소켓의 룰이 아니다. 이제 소통을 위한 2개의 동사 세트가 있다. 너는 send, recv를 쓸 수 있다. 아니면 너는 , 너의 클라이언트 소켓을 파일같은 괴수로 만들어서 읽고 쓸 수 있다. 두번째 방법은, 자바가 그의 소켓을 대표하는 방법이다. 넌 너에게 소켓에 대해서 flush를 사용할 필요가 있다는 것을 경고하기만 하겠다. 그것은 bufferd 파일이다. 그리고 쉬운 착각이 뭔가 write하기 위해서, 그리고 나서 응답에 대해서 reply한다는 것이다. flush가 없다면, 계속 응답에 대해서 기다려야 한다는 것이다. 왜냐하면 리퀘스트는 아직 너의 출력 버퍼에 적혀진 채로 있을 수 있다. 이제 우리는 가장 주요한 소켓의 가장 큰 걸림돌에 도착했다. 네트워크 버퍼에 대한 와 ```recv``` 명령이다.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152그들은 네가 건네준 모든 바이트를 다루지 않는다. 왜냐하면, 그들의 주된 목적은 네트워크 버퍼를 다루는 것이다. 일반적으로, 그들은 관련된 네티워크 버퍼가 차거나(send), 비었을 때(recv), return한다.그리고 나서, 그들은 너에게 얼마나 많은 버퍼를 다뤘는지 알려준다.너의 메세지가 완전히 다뤄지기전까지 그들을 다시 부르는 것은 너의 소관이다.recv가 0bytes를 반환한다면, 그것은 다른 사이드의 연결이 닫혔거나, 반만 닫았다는 얘기이다. 그러면 너는 해당 연결에 대해서 더 이상의 데이터를 받을 수 없다.너는 상대방이 반만 닫았기 때문에, 데이터를 잘 보낼 수 있을 수도 있다.```HTTP``` 같은 프로토콜은 소켓을 하나의 전송에 대해서만 사용한다. 클라이언트가 요철을 보내면, 응답을 읽는다. 그리고 나면 소켓은 정지된다. 이것은 클라이언트가 응답의 끝을 0바이트를 받는 것을 통해서 알 수 있다는 것이다.그러나 소켓을 추가적인 전송을 위해서 너의 소켓을 재사용 하기로 계획하고 있다면, 너는 소켓에는 EOT(END OF Transfer)가 없다는 것을 알아야 한다.다시 강조한다. 소켓의 send, recv가 0 바이트를 다룬 루에 반환한다면, 연결은 이미 끊어졌다. 만약 연결이 끊어지지 않았다면? 너는 평생 recv 기다려야 할 수도 있다.왜냐하면 소켓이 더 이상 읽어야 할게 없다고 알려주지 않을 것이기 때문이다.네가 그것에 대해서 조금 더 생각한다면, 소켓에 대한 기초구현적인 진실을 깨닫게 될 것이다.- 메세지는 고정된 길이(yuck)거나, 범위가 제한(shrug) 되어야 한다. 또는, 그들이 얼마나 긴지(훨씬 낫다), 혹은 연결 종료를 소통하면서 말이다. 선택은 너의 몫이다. 하지만 어떤 것은 다른 것보다 낫다.네가 연결을 종료하고 싶지 않다고 생각했을때. 가장 쉬운 방법은 고정된 길이의 메세지이다.```pythonclass MySocket:&quot;&quot;&quot;클래스만을 설명한다 - 깔끔함을 위한 코드이지 효율성이 아니다.&quot;&quot;&quot; def __init__(self, sock=None): if sock is None: self.sock = socket.socket(socket.AF_INET,socket.SOCK_STREAM) else: self.sock = sock def connect(self, host, port): self.sock.connect((host, port)) def mysend(self, msg): totalsent = 0 while totalsent &lt; MSGLEN: sent = self.sock.send(msg[totalsent:]) if sent == 0: raise RuntimeError(&quot;socket connection broken&quot;) totalsent = totalsent +sent def myreceive(self): chunks = [] bytes_recd = 0 while bytes_recd &lt; MSGLEN: chunk = self.sock.recv(min(MSGLEN - bytes_recd, 2048)) if chunk == b&apos;&apos;: raise RuntimeError(&quot;socket connection broken&quot;) chunks.append(chunk) bytes_recd = bytes_recd + len(chunk) retutn b&apos;&apos;.join(chunks) 여기서 코드를 전달하는 것은 어떤 메세지 포멧에서도 거의 다 유용하다. 파이썬에서, 네가 문자열을 보내면, 너는 len()을 통해서, 그것의 길이를 정할 수 있다. 첫번째 단어를 가져옴 으로써, 너는 길이를 알아볼 수 있다. 너는 연속된 청크사이즈를 얻게 될 것이다.(4096 ~ 8192가 네트워크 버퍼사이즈로 적당하다.)그리고 네가 구획 문자로 무엇을 받았는지, 확인하면서 말이다. 한가지 네가 주의할 점은: 너의 일반적인 프로토콜이 다수의 메세지가 메세지를 뒤로 위치하도록 보내진다면(어떤 응답 없이), 그리고 네가 recv를 전달해서 청크사이즈의 꾸러미를 수용한다면, 너는 추가로 오는 메세지의 읽는 것을 시작하는 것을 종료해버릴지 모른다. 너는 그것을 다른 곳에 놓고, 그게 필요해질 때까지 저장할 수 있어야 한다. 메세지를 그것의 길이를 앞에 두도록 하는 것은 더욱 복잡해진다. 너느 모든 5개의 문자를 한번의 recv로 받지 못할 수도 있다. 제대로 활용하기 위해서, 너는 그것에서 멀어질 것이다. 하지만, 커다란 네티워크 전송해서, 너의 코드는 매우 빠르게 부서질 것이다. 네가 두 개의 recv loop를 사용하지 않는 다면, 첫번째는 길이를 정하고. 두번째는 데이터를 받아오기 위해서. 불행하게도, 이것은 또한 네가 send할때, 항상 메세지를 한번에 보내서 너의 버퍼에서 삭제하는 것은 아니라는 것에서 밝혀진다. 이걸 읽고나서도, 너는 결국 이것에 물릴 것이다! 공간에서 있어서, 문자열을 만드는 것은, 이러한 증진은 읽는 이들을 위한 훈련으로 남아 있다. 6. Binary Data 소켓을 통해서 바이너리 데이터를 보내는 것은, 딱 맞는 일이다. 문제는, 모든 기계가 같은 포멧의 바이너리 데이터를 받는건 아니라는 것이다. 에를 들어 Motorola 칩은, 16비트를 사용해 1을 표현하고, 두개의 16진수 바이트로 00 01으로 한다. 인텔에서는, 바이트가 거꾸로 된다.1은 01 00과 같다. 소켓 라이브러리는, 16비트 32비트 정수를 전환하는 것으로 ntohl, htonl, ntohs, htoms를 호출한다. n은 네트워크를 뜻하고 h는 호스트를 뜻한다.s는 short, l은 long을 뜻한다. 네트워크의 주문자가 호스트인 경우, 그것은 아무것도 안한다. 하지만 기계가 바이트가 거꾸로 됐다면, 이것은 바이트를 적절하게 바꾼다. 32비트 체제인 요즘엔, 아스키가 바이너리 데이터를 표현하는게, 그냥 바이너리를 표현하는 것보다 통 작다. 왜냐하면, 놀랍게도, 모든 길이가 0이나 1을 가지고 있기 때문이다. &quot;0&quot;이라는 문자는 2바이트를 잡고, 바이너리는 4바이트를 잡는다. 하지만 그것은 고정된 길이의 메세지와는 맞지 않는다. 7. Disconnecting 딱 잘라 말해서, 너는 소켓을 close하기 전에 shutdown해버릴 것이다. shutdown은 소켓에서 경고다. 네가 어떤 argument를 전달했으냐에 따라서, 더 이상 보내진 않겠지만, 듣기는 하겠습니다. 또는 난 듣지 않고 있어요. 라고 말할 수 도 있다. 대부분 소켓 라이브러리는, 이러한 shutdown()을 close와 같이 취급하여 에티켓을 무시해버린다. 그래서 대부분의 경우에, 별도의 shutdown은 필요하지 않다. shutdown을 효과적으로 사용하는 방법은 HTTP같은 교환법이다. 클라이언트가 리퀘스트를 보내면 이후 shutdown(1)을 수행한다. 이것은 서버에게 클라이언트가 전송을 중지한다. 그러나 아직 받을 수 있다. 라고 말해준다. 서버는 EOF를 0바이트를 받아들임으로써 감지할 수 있다. 이것은 리퀘스트가 끝났다고 감지하는 것이다. 서버가 응답을 보낸다. 만약 send가 성공적으로 완수되면, 클아이언트는 아직 듣고 있었던 것이다. 파이썬은 멀치감치 자동 shutdown을 수행한다. 그리고 소켓이 garbage collected됐을때 그것을 자신에게 공식화한다. 이것은 필요할 경우에 자동으로 close처리 할 것이다. 하지만, 이것에 의존하는 것은 매우 나쁜 습관이다. 만약 너의 소켓이close하지 않고 사라졌다면, 소켓은 다른 끝에서 계속 살아 있을 수 있다. 네가 그저 느린 것이라고 판단하고 말이다. 그러니 제발 일이 끝나면 close를 명시해라. 내부적인 장치가 항상 중요하다. 8. When Sockets Die 아마도 blocking소켓을 사용하는 것에서 가장 나쁜것은, 다른 쪽이 close없이 조용해져 버렸을때 일어나는 일이다. 너의 소켓은 아마 걸려 있을것이다. TCP는 연결지향적인 프로토콜이라, 그래서 오래 기다린다. 연결을 소멸시키기 전까지 오래 기다린다. 만약 네가 쓰레드를 사용한다면, 모든 쓰레드는 필수적으로 죽어버릴 것이다. 네가 거기에 대해서 할 수 있는 것이 거의 없다. 쓰레드를 죽이려고 하지마라: 이유 중 하나는, 쓰레드는 프로세스보다 훨씬 효율적인 이유는, 그들이 자동으로 자원을 재활용 함으로써, 오버헤드를 피하고 있기 때문이다. 다르게 말해서, 네가 쓰레드를 죽이려고 하면, 너의 모든 프로세스는 망치게 될 가능성이 높다. 9. Non-blocking Sockets 네가 진행을 이해하고 있다면, 소켓을 사용하는 데 필요한 기계적인 것들을 거의 다 알고 있는 것이다. 너는 아직 동일한 call을 사용할 것이다, 전과 다를바 없이. 이것은 그런 것이다. 네가 제대로 하면, 너의 앱은 거의 할 것을 다 한것이다. 파이썬에서는 사용하여 non-blocking하게 한다. C에서 이것은 더욱 복잡하다.123456789101112&gt; 한가지 말하자면, 너는 BSD의 0_NONBLOCK과 거의 구별할 수 없는 POSIX스타일의 0_NDELAY중에 고르게 될것이다.(TCP_NODELAY)와는 완전히 다르다. 그러나 동일한 개념이다.&gt; 소켓을 만든 후, 그리고 사용하기 전에 너는 그것을 한다.주요 기계적인 차이는 send, recv, connect 그리고 accept는 아무것도 하지않고 return할 수 있다는 것이다. 너는 물론 몇가지 선택권이 있다.너는 return 코드와 error코드를 체크해서, 일반적으로 너를 미치게 할 수 있다.믿지 못해겠다면, 한번 쯤 해보는 것도 좋은 생각이다.너의 앱이 커지면, 버그가 넘치고, CPU를 빨아먹는다.그러니 그냥 생각 없이 이것을 따라라.- USE ```select C에서는, select는 매우 복잡하다. Python에서는 그냥 케익 한조각일 뿐이다. 그러나 C에 매우 가깝기 때문에, python에서 Select를 이해하면, C에서 조금 순탄해 질 수 있다: 1234567read_to_read, ready_to_write, in_error = \\ select.select( potential_readers ,potential_writers ,potential_errs timeout ) 너는 select에 3가지 리스트를 전달한다. 니가 읽으려고 시도하고 싶은 모든 소켓 니가 쓰려고 시도하고 싶은 모든 소켓 마지막(보통 비어있다) 네가 원하는 것에 에러를 체크해줄 것들. 너는 소켓이 하나의 리스트 이상에 전달 될 수 있다는 것을 알아야 한다. select에 대한 호출은 blocking이다. 그러나 timeout을 줄 수 있다. 이것은 일반적으로 민감한 사안이다. 적절히 긴 timeout을 주어라, (예를 들면 1분), 네가 만약 다른 선택을 할 마땅한 이유가 없다면.","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"python post","slug":"python-post","permalink":"https://codenamenadja.github.io/tags/python-post/"}]},{"title":"파이썬 Socket 모듈 정리","slug":"python/module/socket","date":"2019-06-19T09:47:15.000Z","updated":"2021-01-25T08:36:06.083Z","comments":true,"path":"2019/06/19/python/module/socket/","link":"","permalink":"https://codenamenadja.github.io/2019/06/19/python/module/socket/","excerpt":"","text":"socket.socket(family,type,proto,fileno) 주어진 주소를 사용하는 소켓을 생성한다. family는 AF_INET, AF_NET6, AF_UNIX, AF_PACKET, AF_RDS(socket.AF_NET) type은 SOCK_STREAM, SOCK_DGRAM, SOCK_RAW protocol number는 보통 0이다, addrFamily가 AF_CAN이거나 하면 다른 값을 가져야 한다. fileno descriptor가 특정되었다면, 파일 디스크립터에 의해서 , 패밀리 타입, 프로토콜번호가 자동으로 잡힌다. serverSocket.listen([backlog]:Optional int) 서버가 연결을 수락하도록 허용한다. 백로그가 특정되어있다면, 새로운 연결을 거절하기전에 시스템이 허락한 받아들여지지 않은 연결 수를 특정하는 것이다. serverSocket.accept() 연결을 받아들인다. 소켓은 주소에 달라붙어있어야하고, 포트 연결에 대해서 수용하고(listen) 있어야 한다. return value는 pair로 (conn, address)이다. conn은 새로운 소켓객체다, 데이터를 보내고 받는데 사용될 수 있다. address는 해당 소켓이 반대쪽 연결의 끝에 어디에 달라붙어 있는지를 얘기한다. 3.5버전 이후 변경점: 시스템콜이 방해를 받아 signal handler이, exception을 raise하지 않는다면, 이 메서드는 시스템 콜에게 InterruptedError를 raise하지 않고, accept를 재시도 한다. socket.close() 소켓이 닫혔다는 것을 표시한다. FD같은 잠재된 시스템 자원 또한 모든 IO 버퍼에 대한 FD를 markfile()로 확인해서 모든 파일 객체가 닫혔다면, 닫혀버린다. socket.markfile(mode=‘r’, buffering=None, *,encoding=None, errors=None, newline=None) 소켓과 연결지어진 파일 객체를 반환한다. file object : 공식문서 용어해설 파일에 기원을 두는 API를 내부 실제 리소스에게 드러내는 객체. read(), write() 등이 그러한 API이다. 이것이 어떻게 생성됐는지에 따라서, file object는 실제 디스크의 파일이나, 다른 타입의 저장소 혹은 communication device에 접근 할 것인지를 고려할 수 있다. 해당 underlying resourced의 목록 statdard input/ouput in-memory buffers sockets pipes, etc… File Objects는 file-like object 혹은 streams라고 불리기도 한다. 그들은 주로 세가지 카테고리의 파일 객체(정석의 생성법은 open메서드를 통해) binary files, buffered binary files, textfiles socket.recv(bufsize[, flags]) 소켓에서 데이터를 받아온다. 리턴값은 바이트객체. 한번에 받을 수 있는 데이터 양의 총량은 bufsize에 의해 특정된다. 네트워크와 하드웨어를 생각했을때. 최적의 버프사이즈는 2의 제곱수로 잡는 것이 좋다. socket.connect(address) 해당 주소의 remote socket에 접속한다. 주소의 포멧은 address family에 의해 결정된다. 만약 signal에 의해 connection이 interrupted되면, 메서드는 연결이 끝날때 까지 기다리거나, raise socket.timeout on timeout 한다. 만약에 signal handler가 exception을 raise하지 않고, 소켓이 블로킹 소켓이거나 timeout을 가졌다면 논블로킹 소켓에 대해서, 메서드는 InterruptedError exception을 raise한다. 만약 connection 이 signal에 의해 방해 받았을 때. socket.fileno() 소켓의 파일 descriptor을 반환한다. -1을 반환한 다면 실패. select.select()와 유용하게 쓰인다. select.select(rlist,wlist,xlist[,timeout]) 이것은 UNIX select()시스템 콜을 향한 딱 맞는 인터페이스이다. 3가지 매개변수는 waitable object의 연속을 전달한다.: 모두 파일 디스크립터를 통해 fileno() 등을 통해 전달받은 숫자가 된다. rlist: wait until ready for reading wlist: wait ready for writting xlist: wait for an “exceptional condition(by system)” 빈 시퀸스는 허락된다. timeout 매개변수가 생략되었을때, 함수는 하나라도 FD가 준비되었을 때까지 block된다. 리턴하는 것은 준비된 3가지 리스트 객체를 리턴한다. 시퀸스에 받아들일 수 있는 타입들은,파이썬 fileobjects(sys.stdin, open(), os.popen()), 혹은 소켓 객체가 있다. 3.5버전의 변화: 함수는 이제 신호에 의해서 interrupted될 때, timeout과 함께 재시도 된다. InterruptedError일 때만 그렇고 다른 에러를 Interrupted 핸들러가 발생시키면 재시도 되지 않는다.","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"python module","slug":"python-module","permalink":"https://codenamenadja.github.io/tags/python-module/"}]},{"title":"파이썬 인터프리터에 대한 이해","slug":"python/post/python_interpreter","date":"2019-06-19T09:44:43.000Z","updated":"2021-01-25T08:36:06.087Z","comments":true,"path":"2019/06/19/python/post/python_interpreter/","link":"","permalink":"https://codenamenadja.github.io/2019/06/19/python/post/python_interpreter/","excerpt":"","text":"1. intoduction execution of python as split into three main phases Initialization: 다양한 데이터 구조의 셋업, 파이썬 프로세스에 의해 필요하다. 파이썬 인터프리터 셸과 상호적이지 않을 때 의미가 있다고 생각된다. Compiling: 소스코드를 파싱한다. syntex트리로 만들기 위해 abstract syntex tree(AST)를 생성하기 위해 심볼테이블을 생성하고, 코드 객체를 생성한다. Interpreting: 생성된 코드객체의 실행을 의미한다. initialize compile 1.main 4.parse tree generate 2.Py_Main 5.AST generate 6.bytecode generate 7.bytecode optimization 8.code object generate end: codeobect execution ./Programs/python.c : handles initialization main function call Py_main located in ./Modules/main.c main.c handles interpreter initialization process 초기화의 과정으로서, Py_Initialize from pylifecycle.c가 호출된다. 이것이 인터프리터의 자료구조와 쓰레드상태의 자료구조를 초기화 한다.(매우 중요한 자료구조) 인터프리터 상태 자료구조 12345678910111213141516171819typedef struct _is &#123; struct _is *next; struct _ts *tstate_head; PyObject *modules; PyObject *modules_by_index; PyObject *sysdict; PyObject *builtins; PyObject *importlib PyObject *codec_search_path; PyObject *codec_search_cache; PyObject *codec_error_registry; int codecs_initialized; int fscodec_initialized; PyObject *builtins_copy;&#125; PyInterpreterState;","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"python post","slug":"python-post","permalink":"https://codenamenadja.github.io/tags/python-post/"}]},{"title":"파이썬 바이트코드에 대한 이해","slug":"python/post/bytecode_in_python","date":"2019-06-19T09:42:39.000Z","updated":"2021-01-25T08:36:06.087Z","comments":true,"path":"2019/06/19/python/post/bytecode_in_python/","link":"","permalink":"https://codenamenadja.github.io/2019/06/19/python/post/bytecode_in_python/","excerpt":"","text":"어떤 언어는 직접 CPUInstruction으로 컴파일하고, 어떤 언어는 실행 중에 즉시 소스코드를 해석한다. 어떤 언어는 중간단계의 세트의 instruction으로 컴파일한다.(실제 CPU를 향한게 아님) 그리고 가상머신(인터프리터)을 실행해서, 실행 중에 그것들을 CPU instruction으로 변환해서 건네준다. 그게 ByteCode이다. 자바도 바이트코드로 변환한다, C#도 그렇다. 파이썬도 그렇다. 123456789101112131415161718192021#fibonacci.py#fibonacci.pyc -python2#pycache - python3fib.__code__&gt;&gt;&gt; &lt;code object fib at 0x10fb76930, file '&lt;stdin&gt;' line 1&gt;fib.__code__.co_consts&gt;&gt;&gt; (None, 2, 0, 1, (0, 1))fib.__code__.co_varnames&gt;&gt;&gt; ('n', 'current', 'next') :local-varnamefib.__code__.co_names&gt;&gt;&gt; () non-localnamesfib.__code__.co_code&gt;&gt;&gt; b'|\\x00d\\x01k....'import disdis.opname[124]&gt;&gt;&gt; 'LOAD_FAST' Cpython CPython은 스텍기반 가상머신이다. 각 함수는 호출되면, 콜 스텍위로 새로운 엔트리로 콜 프레임(=스텍프레임)을 집어 넣는다. 함수가 리턴되면, 이것의 프레임은 스텍에서 popped off된다. C기반이라, 규칙처럼 적용되서 탈락을 면할 수 없지만, Coroutine은 독립된 스텍프레임으로 힙안에 존재하기 때문에, 함수 실행 이후에(제너레이터 초기화)도 남아있을 수 있다. Cpython은 함수가 실행될 때, 두개의 스텍을 사용한다: evaluation-stack or data-stack, (이것은 지역변수와 인스트럭션) And block-stack(얼마나 많은 블록이 활성화되어있는지 추적한다. loops, try/except, with,…) 1fib(8) ByteCode VAL Evauation Stack 0 LOAD_GLOBAL 0 (fib) 2- 8 2 LOAD_CONST 1 (8) 0- function fib 4 CALL_FUNCTION 1 자세한 것은 공식문서 dis 모듈에 바이트코드에 대한 모든게 들어있다. 꼭 기회를 만들어 보시길. 아래는 실제로 c로 돌아가는 코드이다. 1234567891011121314151617181920switch (opcode)&#123; TARGET(NOP) FAST_DISPATCH(); TARGET(LOAD_FAST) &#123; PyObject *value - GETLOCAL(oparg); if(value == NULL) &#123; format_exe_check_arg( PyExc_UnboundLocalError, UNBOUNDLOCAL_ERROR_MSG, PyTuple_GetItem(co-&gt;co_varnames, oparg) ); goto error; &#125; Py_INCREF(value); PUSH(value); FAST_DISPATCH(); &#125;&#125;/*Many, many more bytecode instructions below...*/ What can we learn from bytecode? Practical한 효과를 그렇게 보기는 힘들 것이다. 하지만 stack-oriented virtual machine을 확인하는 것은 다르다. 넓은 이해를 가질 수 있고, 다양한 스타일의 프로그래밍을 구사하는데 도움이 된다. instruction과 stack에 대해서 우리가 할수 있는 것은, 정말, 정말 극소수지만, 그것은 우리가 찾던 것이다. 하지만 실용적인 목적도 있다. c를 기반으로 하는 가상머신을 거의 다 이해할 수 있다. 파이썬도 그냥 그 중 하나이다. 공부에 대한 통찰력을 길러준다. 실제 가상머신이 바이트코드를 실행하는 것에 대해서 이해하면 퍼포먼스에 접근할 수 있다. Conclusion Python은 항상 C보다 느리다는 것을 주목하라. 그러나! 일반적인 가이드라인을 원하지 않는가? 여기 기술한다. 1. Local names are Faster than Global ones LOAD_CONST &gt; LOAD_FAST &gt; LOAD_NAME or LOAD_GLOBAL 왜냐하면 글로벌 네임을 찾아보는 것은 조금 더 복잡하다. 실제 인스트럭션이 생각보다 길 것이다. (muitple namespaces 사이에서 찾아야 한다.) 2. Loops and blocks Are expensive Lookout for SETUP_LOOP, SETUP_WITH and SETUP_EXCEPTION 3. Attribute acceses, dictionary lookups and list Indexing are expensive Look out for LOAD_ATTR and BINARY_SUBSCR addtional recommends to look for… 1. Obi ike-Nwosu, “inside the Python Virtual Machine”: https://leanpub.com/insidethepythonvirtualmachine/ 2. Alison Kaptur, “A Python Interpreter Written in Python”: http://www.aosabook.org/en/500L/a-python-interpreter-written-in-python.html 3. The CPython bytecode interpreter: https://github.com/python/cpython/blob/master/Python/ceval.c","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"python post","slug":"python-post","permalink":"https://codenamenadja.github.io/tags/python-post/"}]},{"title":"파이썬의 함수형 프로그래밍에 대한 서론 번역","slug":"functional_programming/python/python_functional_programming","date":"2019-06-19T09:39:22.000Z","updated":"2021-01-25T08:36:06.071Z","comments":true,"path":"2019/06/19/functional_programming/python/python_functional_programming/","link":"","permalink":"https://codenamenadja.github.io/2019/06/19/functional_programming/python/python_functional_programming/","excerpt":"","text":"출처 : https://www.dataquest.io/blog/introduction-functional-programming-python 서문 대부분의 우리는 파이썬을 객체지향언어라고 알고있다. 클래스와 객체가 비록 사용하기 쉽지만, 파이썬을 작성하는 데엔 다른 방법이 있다. 자바등의 언어는 객체지향에서 벗어나기 힘들게 만드는 경향이 있지만, 파이썬은 그렇지 않다. 파이썬이 코드를 작성하는데에 다른 접근법을 가능하게 한다는 걸 가지고서, 따라오는 논리적인 질문은, 코드를 적는 다른 방법은 무엇일까? 비록 거기에 많은 답들이 있더라도, 가장 흔하고 대체적인 스타일의 코드 작성법은 함수형 프로그래밍이라고 불린다. 함수형 프로그래밍은 메인 로직이 되어가고 있다. 이 포스트에서 우리는 -객체지향과 비교해 함수형의 기본을 설명한다 -당신이 왜 함수형을 적용하기 힘든지 밝혀낸다 -어떻게 파이썬이 그 둘 사이에서 전환하는지 알려준다 1.객체지향 언어와 비교하여, 1234567891011class LineCounter: def __init__(self, filename): self.file = open(filename, 'r') self.lines = [] def read(self): self.lines = [line for line in self.file] def count(self): return len(self.lines) 클래스를 사용하는 것은 위의 사례로 대표된다. 최고의 객체지향은 아니지만, 이것은 객체지향 디자인에 대한 통찰을 보급한다. 클래스 안에서, 거기엔 친숙한 메서드와 속성이라는 개념이 있다. 속성세트와 객체의 상태를 추론하는 속성, 그리고 메서드가 그 상태를 조작한다. 그 두 컨셉의 작업에서, 객체의 상태는 시기에따라 계속 변해야한다. 상태를 변화시키는 것은 증거가 된다, lines 속성 속에서, read()메서드를 부른 이후. 예로서 이것은 우리가 어떻게 이 클래스를 사용할 건지이다. Scenenario example_file.txt contains 100 lines. 123456lc = LineCounter('example_file.txt')print(lc.lines)&gt;&gt; []print(lc.count())&gt;&gt; 0 The lc object must read the file to set the lines property. 1lc.read() The lc.lines property has been changed. This is called changing the state of the lc 1234print(lc.lines)&gt;&gt; [['Hello world!', ...]]print(lc.count())&gt;&gt; 100 계속 변하는 객체의 상태는 둘다 축복이며 저주이다. 왜 상태의 변화가 부정적인지 알기 위해서, 우리는 대체제를 소개해야 한다. 대체제는 line counter를 독립적인 함수의 연속으로 설계하는 것이다. 123456789def read(filename): with open(filename, 'r') as f: return [line for line in f]def count(lines): return len(lines)example_lines = read('example_log.txt')lines_count = count(example_lines) 2.순수함수로 작업하기 이전의 예제에서 우리는 라인들을 함수만을 이용해서 리스트의 길이를 셀 수 있었다. 우리가 오직 함수만을 사용할 때, 우리는 함수형 접근을 프로그래밍에 하고 있다, 그 접근은, 놀랍지 않게도, 함수형 프로그래밍이라 불린다. 함수형프로그래밍 뒤에 있는 개념은 함수가 상태없이 존재 하기를 요구한다. 그리고 오직 그들에게 주어진 매개변수으로 출력을 하기를 원한다. 그 기준에 맞는 함수는 순수함수라 불린다. 이것은 함수형 과 아닌 것 사이를 나누기 위한 것이다. Create a global variable A. 1234567891011121314151617A = 5def impure_sum(b): # Adds two numbers, but uses the # global `A` variable. return b + Adef pure_sum(a, b): # Adds two numbers, using # ONLY the local function inputs. return a + bprint(impure_sum(6))&gt;&gt; 11print(pure_sum(4, 6))&gt;&gt; 10 이 순수함수의 이점이 외부효과가 있는 함수 명확한 것은, Reduction의 외부효과들이다. 외부효과들은 함수 밖에 있는 것에 대해서 함수의 실행이 변화를 주었을 때 일어난다. 예를 들어 그것들은 우리가 객체의 상태를 바꾸려고 했을 때 일어난다. 어떤 I/O 명령을 실행했을 때, 혹은 print()를 호출했을때, 123456789def read_and_print(filename): with open(filename) as f: # Side effect of opening a # file outside of function. data = [line for line in f] for line in data: # Call out to the operating system # \"println\" method (side effect). print(line) 함수 내부에서 함수 밖에 메모리를 호출하고 조종한다. 프로그래머들은 그들의 코드가 쉽게 테스트되고 디버깅되고 따르기 쉽게 만들기 위해서 외부효과를 줄인다. 그리고 코드의 외부효과가 커질수록, 프로그램을 진행하고 실행의 순서를 이해하기 어려워진다. 비록 모든 외부효과를 없애려고 시도하는 것은 편리하더라도, 그들은 프로그래밍을 쉽게 만든다. 우리가 만약 모든 외부효과를 없애려고 했다면, 그럼 너는 파일도 읽을 수 없고, print를 호출할 수도 없고 모든 변수를 함수 내에서만 초기화 해야한다. 함수형 프로그래밍의 지지자들은 이 트레이드 오프를 이해하고 있다. 그리고 개발의 결과의 퍼포먼스를 해치지 않는 선에서 가능한 많은 외부효과를 제거하려고 노력한다. 3.람다함수 def 형식 이외에 함수 선언에서 우리는 lambda표현을 할 수 있다. 람다 형식은 def선언형식을 가깝게 따라가지만, 이것은 1-1 매핑이 아니다. 이것은 2개의 정수를 더하는 함수의 예이다. 123456# Using `def` (old way).def old_add(a, b): return a + b# Using `lambda` (new way).new_add = lambda a, b: a + old_add(10, 5) == new_add(10, 5)&gt;&gt; True 람다 표현식은 콤마를 사용하여, 매개변수의 연속성을 분리한다. 그리고, 정확히 인터프리터가 그 문장의 콜론에 닿는 때에, 이것은 따로 return에 대한 진술없이 표현식을 리턴한다. 결국, 변수에 람다함수를 할당하는 것은, 파이썬함수처럼 작동하는 것이고, 변수를 함수처럼 사용할 수 있다.new_add() 우리가 비록 람다 차체에 변수이름을 할당하지 않았더라도, 이것은 익명의 함수가 된다. 이 익명함수는 매우 도움이 된다. 특히 다른 함수의 매개변수가 되는 때에. 예를들어, sorted()함수는 옵션의 Key매개변수를 받는다. 그 매개변수는 어떻게 아이템들이 리스트에서 정렬되야 하는지를 나타낸다. 12345unsorted = [('b', 6), ('a', 10), ('d', 0), ('c', 4)]# Sort on the second tuple value (the integer).print(sorted(unsorted, key=lambda x: x[1]))&gt;&gt; [('d', 0), ('c', 4), ('b', 6), ('a', 10)] 4.Map함수 파이썬에서 매개변수로 함수를 전달하는게 다른언어와 구별되는 일은 아니지만, 이것은 최근 프로그래밍 언어들에서 사용되는 개발이다. 이러한 타입의 행동을 따르는 함수들을 first-class fucntions라고 부른다. 어떤 first-class Func를 포한하는 언어는 함수형 스타일로 작성될 수 있다. first-class Func와 함께 세트로 함수형 패러다임에서 주요 사용되는 것이 있다. 이 함수들은 iterable 객체를 받는다. 마치 sorted()처럼, 요소에 개별적으로 함수를 적용시킨다. 다음 일부 섹션에서, 우리는 그들을 개별적으로 시험해볼 것이다. 그러나 그들은 일반적인 포멧인 function_name(function_to_apply, iterable_of_elements). 를 따른다. 우리가 실험해볼 함수는 map()함수이다. map()함수는 iterable객체를 받아서, 새로운 iterable객체를 생성한다. 특별한 map객체이다. 새로운 객체는 모든 요소에 적용되는 일급함수를 가지고 있다. 12345678910111213141516171819202122# Pseudocode for map.def map(func, seq): # Return `Map` object with # the function applied to every # element. return Map( func(x) for x in seq )# 이것은 우리 어떻게 맵을 사용하여 개별 리스트 요소에 10또는 20을 다하는 방식이다.values = [1, 2, 3, 4, 5]# Note: We convert the returned map object to# a list data structure.add_10 = list(map(lambda x: x + 10, values))add_20 = list(map(lambda x: x + 20, values))print(add_10)&gt;&gt; [11, 12, 13, 14, 15]print(add_20)&gt;&gt; [21, 22, 23, 24, 25] map()을 리턴하는 것에서 리스트객체를 리턴하는 것으로 바뀌었다는 점을 주목하라. 돌려받은 map객체룰 사용하는 것은 네가 list처럼 나오는 것을 기대한다면 좀 불편한 것이다. 첫째로, 그것을 출력하는 것은 모든요소를 보여주지 않으며, 둘째로 너는 한번만을 순회하는 것이 가능하다. 왜냐면, 그 기록이 map객체의 인스턴스에 기록되기 때문이다. 4.Filter함수 map과 유사하니 일단 통과 5.Reduce함수 우리가 볼 마지막 함수인 reduce()는 functools패키지 내부에 있다. 그 함수는 이터러블 객체를 받아, 이터러블객체를 하나의 value로 줄인다. Reduce는 filter(), map()과는 다르다. 왜냐하면 이것은 해당 함수가 2개의 인자를 받기 때문이다. 이것은 우리가 어떻게 reduce()를 사용하여 모든 리스트이 요소를 합하는 예이다. 123456789101112131415161718192021values = [1, 2, 3, 4]summed = reduce(lambda a, b: a + b, values)print(summed)&gt;&gt; 10실행도new = 1+2(3)new = new+3(6)new = new+4(10)# 기억해야할 흥미로운점은 람다표현을 사용하면, 두번째 value에 대해선 실행하지 않아도 된다는 점이다.# 예를 들어 어떤 함수가 이터러블 객체의 첫 요소만 항상 반환한다고 보자# By convention, we add `_` as a placeholder for an input# we do not use.first_value = reduce(lambda a, _: a, values)print(first_value)&gt;&gt; 1 6.List comprehensions로 재작성 우리가 결국 리스트로 변환했기 때문에, 우리는 map(),filter()함수를 리스트-C로 재작성해야한다. 우리가 파이썬고유 표현형식을 통해 리스트를 만드는 것으로 이점을 얻을 수 있기 때문에, 이것이 더 그들을 쓰기에 파이써닉한 방법이다. 이것인 이전의 예제를 리스트-C로 변환할 수 있는 방법이다. 123456789101112values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]# Map.add_10 = [x + 10 for x in values]print(add_10)&gt;&gt; [11, 12, 13, 14, 15, 16, 17, 18, 19, 20]# Filter.even = [x for x in values if x % 2 == 0]print(even)&gt;&gt; [2, 4, 6, 8, 10] 예제를 보면, 우리는 람다함수를 더할 필요도 없다! 네가 만약 map()filter()를 너의 코드에 적용하려 한다면, 이것을 가장 추천한다. 먼저 고려해라. 그러나 다음 섹션에서, 우리는 map(),filter()를 사용하여 예시를 보여줄 것이다. 7.function Partials, 작성하기 떄로 우리는 함수의 행동은 그대로 사용하고 싶으면서, 들어오는 매개변수의 수를 줄이고 싶다. 목적은 하나의 인풋을 저장하여, 저장된 매개변수를 default로 하는 새로운 함수를 만든다. 예를 들어 우리가 2를 더해주는 함수를 작성한다고 가정하자. 123456def add_two(b): return 2 + bprint(add_two(4))&gt;&gt; 6 add_two함수는 일반적 함수인 f(a,b)=a+bf(a,b)=a+bf(a,b)=a+b 와 유사하다, 오직 다른건 하나의 매개변수를 기본으로 하고 있다는 것이다. (a=2) 우리는 partial모듈을 사용하여, argument기본이 설정할 수 있다. (람다는 초기화 당시에 라서 람다가 평가되는 시점에 유효한 Reference을 읽어내지만, 이것은 평가시점이 아니라 설정한 당시 value를 유지한다. 마치 클로저변수처럼. partial 모듈은 함수를 받아 매개변수를 freezes(value로서)로 고정시킨다, 첫번째 매개변수로부터 시작해서, 디폴트 매개변수 value로서 완성된 새로운 함수를 리턴한다. 1234567891011121314151617181920212223def add(a, b): return a + badd_two = partial(add, 2)add_ten = partial(add, 10)print(add_two(4))&gt;&gt; 6print(add_ten(4))&gt;&gt; 14#partials는 어떤 함수도 받을 수 있다. 표준 라이브러리들도 포함해서.# A partial that grabs IP addresses using# the `map` function from the standard library.extract_ips = partial( map, lambda x: x.split(' ')[0])lines = read('example_log.txt')ip_addresses = list(extract_ip(lines))","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"functional programming","slug":"functional-programming","permalink":"https://codenamenadja.github.io/tags/functional-programming/"}]},{"title":"TCP분석과 이해","slug":"computing/network/tcp_all_in_one","date":"2019-06-19T09:34:26.000Z","updated":"2021-01-25T08:36:06.071Z","comments":true,"path":"2019/06/19/computing/network/tcp_all_in_one/","link":"","permalink":"https://codenamenadja.github.io/2019/06/19/computing/network/tcp_all_in_one/","excerpt":"","text":"TCP의 중요한 성질 Connected Oriented: 연결지향 Bidirectional Byte stream: 양방향성 바이트스트림(octet-stream) In-order delivery: 순차적인 전송 Reliability through ACK: ACK를 통한 안정성 Flow Control: 수신자의 receive window에 맞는 바이트 수 안쪽으로 데이터를 전송 Congestion control : 일방적으로 일대 다 통신을 강요받는 서버가 네트워크 정체를 방지하기 위해 congest해on window를 사용. 데이터양을 제한. TCP vegas등 알고리즘이 있다. Flow Control과 달리 송신자가 단독으로 구현. 1. 데이터 전송 스텍에는 여러 레이어가 있다. 그렇지만 크게, user영역, device영역으로 나눌 수 있다. 유저 영역과 커널영역의 작업은 CPU가 수행한다. 유저영역+커널영역 = HOST Device영역 = NIC NIC로 인해 IO바운드한 소켓버퍼를 설계할 수 있고 싱글쓰레드의 비동기 처리는 이 장치에 기인한다. Boundary layer description packet User App(process) write(fd,buf,len) data Kernel File 파일descritor을 갱신 Kernel Sockets 버퍼객체를 소켓버퍼로 copy sendsocketbuffer:[BYTE_DATA] (송수신용 버퍼에 맞게 저장된 바이트데이터) Kernel TCP TCP상태와 checksum에 따라 TCP 계층을 구성 [TCP,B_data]:buffer Kernel IP IP헤더추가, IP라우팅추가, 체크섬진행 [IP,TCP,B_data] Kernel Ethernet ethernet 헤더를 붙이고, ARP실행(주소변환) [Eth,IP,TCP,B_data] Kernel Driver NIC에게 소켓을 보내라고 전달하는OS의 마지막 레벨 Device NIC 호스트메모리에서 패킷을 받아 fetch한다. Interrupt the host when send is done [IFG,Pre,Eth,IP,TCP,payload] CheckSum: 중복검사의 한 형태로, 오류정정을 통해, 전자통신이나 기억장치 속에서 송신된 자료의 무결성을 보호하는 단순한 방법이다. Linux나 UNIX를 비롯한 POSIX계열 운영체제는 소켓을 FD로 애플리케이션에 그 고유성을 노출한다. 파일레이어가 단순한 검사후에, 커널소켓에 연결된 소켓구조체(프로그램레벨 소켓객체)를 사용하여, 소켓함수를 제공한다. 커널 소켓은 두개의 버퍼를 가지고 있다. (send Socket Buffer, receice socket buffer) write시스템 콜을 호출하면 유저영역의 데이터가 커널 메모리로 복사된다. 이 다음으로 TCP를 호출한다. 소켓과 연결된 TCP Control Block(TCB)가 있다. TCB를 다룬다. TCB에 있는 데이터, 1234567TCB: [connection state(LISTEN, ESTABLISHED, TIME_WAIT등), receive window, congestion window, sequence넘버, 재전송 타이머 등 ] TCP상태가, 전송을 허용하면, TCPsegment. 즉, 패킷을 생성한다. 이후 IP레이어를 거쳐 다음 HOP등에 대한 라우팅이나 헤더추가가 끝나면, NIC로 결국 내려가게 되는데, NIC는 패킷전송 요청을 OS레벨 드라이버에게서 받고, 메인 메모리에 있는 패킷을 자신의 메모리로 복사하고 네트워크 선으로 전송한다. 놀라운 일이다. NIC는 독립적인 메모리를 가진 디바이스이다. 2. NIC 매우 중요하다, 비동기 싱글쓰레드가 가능한건 순전히 이 부분의 힘이다. NIC가 패킷을 전송할 때 NIC는 호스트CPU에 interrupt를 발생시킨다. 모든 인터럽트에는 인터럽트 번호가 있으며, 운영체제는 이 번호를 이용하여 이 인터럽트를 처리할 수 있는 적합한 드라이버를 찾는다. 드라이버는 인터럽트 핸들러를 드라이버가 가동되었을 때 운영체제에 등록해둔다. 운영체제가 핸들러를 호출하고, 핸들러는 전송된 패킷을 운영체제에 반환한다. 3. 패킷 수신 NIC 드라이버가 미리 할당해 놓은 메모리버퍼가 없으면 NIC가 패킷을 버릴 수 있다! 패킷을 호스트 메모리로 전송한 후, NIC가 호스트 운영체제에 인터럽트를 보낸다. 드라이버 드라이버가 상위 레이어로 패킷을 전달하려면 운영체제가 이해할 수 있도록 운영체제 구조체를 포장해야한다. LINUX:sk_buff BSC:mbuf MS:NET_BUFFER_LIST Ethernet 상위프로토콜인 네트워크 프로토콜을 찾는다. 이때 Ethernet헤더의 ethertype을 검사한다. IPv4의 ethertype은 0x0800이다. 헤더를 제거하고, IP레이어로 패킷을 전달한다. IP레이어 IP헤더 체크섬을 확인한다. : 논리적으로 여기서 IP 라우팅을 해서 패킷을 로컬장비가 처리해야하는지 아니면 다른장비로 전달해야 하는지 판단한다. IP헤더의 proto값을 보고 찾는다. TCP proto값은 6이다. 헤더를 제거하고, TCP레이어로 패킷을 전달한다. TCP TCP체크섬을 확인하는데, 요즘 네트워크 스텍에는 checksum offload기술이 적용되어 있기 떄문에, 커널이 아니라 드라이버 NIC가 이 과정을 처리해준다. TCPcontrol block을 찾는다. 이때 패킷의 &lt;소스 IP, 소스 port, 타킷 IP, 타깃 port&gt;를 식별자로 사용한다. 연결을 찾으면, 페이로드를 receive socket buffer에 추가한다. TCP상태에 따라 새로운 TCP패킷(ACK등)을 전송할 수 있다. TCP가 Receivesocketbuffer로 잔달할때, 이 크기가 결국 receive window이다. 최신 네트워크 스텍은 윈도우 크기를 자동 조절하는 기능을 가지고 있다. 마지막으로 애플리케이션이 read시스템 콜을 사용하면 버퍼에 있는 메모리를 유저공간의 메모리로 복사하고, 복사한 메모리는 제거한다. 제거된 공간덕에 버퍼에 공간이 늘었기 때문에 윈도우 크기를 증가시킨다. 소켓레이어에 sendbuffer[snd.UNA:아직 ACK를 받지 못한 데이터시퀸스 시작 넘버, snd.NXT:다음에 보낼 시퀸스 시작 넘버] 상대방의 ACK와 합의된 window크기에 따라 맞춰서 보내준다. 소켓레이어에 receivebuffer:[nxt:다음번에 받을 상대방의 시퀸스 넘버]","categories":[],"tags":[{"name":"network","slug":"network","permalink":"https://codenamenadja.github.io/tags/network/"}]},{"title":"TCP의 3단계 절차","slug":"computing/network/3_step_in_tcp","date":"2019-06-19T09:32:33.000Z","updated":"2021-01-25T08:36:06.071Z","comments":true,"path":"2019/06/19/computing/network/3_step_in_tcp/","link":"","permalink":"https://codenamenadja.github.io/2019/06/19/computing/network/3_step_in_tcp/","excerpt":"","text":"TCP의 3단계 1. 연결설정 최초 요청을 주는 쪽에서, connect함수 호출과 동시에, three-way handshake가 실행된다! requester responser SYN(SEQ:1000,ACK:-) …listening waiting… SYN+ACK(SEQ:2000,ACK:1001) ACK(SEQ:1001,ACK:2001) …waiting ACK only:lasthandshake receive ACK, ready to send SYN:synchronization ACK: acknowledge now we are connected in TCP. three way hand Shake 본격적으로 원하는 페이로드를 요청하고 받기 이전에 이 과정이 완료되야 connected했다고 한다. 두번째 단계에서는 데이터 송수신이 시작된다. 2. 데이터 송수신 데이터 전송 단계에서 flow control은 첫 번째 단계보다 훨씬 복잡하다. 예시는 responser가 requester에게 많은 양의 데이터를 전송하는 상황이다. RES REQ packet(SEQ:1301, 100bytes) listening(registered requester) waitForACK ACK:1401 SEQ:1401, 100bytes …waiting waitForACK… (didnt receive!) …waiting timeout(wait enough!) …waiting SEQ:1401, 100bytes …waiting waitForACK… ACK:1501 100바이트를 전송하면서 SEQ:1301 을 전송한다. 1301으로부터 offset 100. ACK패킷이 잘 도착했다면, RES는 1301부터 100떨어진 SEQ1401에서 100byte를 전송한다. timeout이 발생, 이전 패킷을 다시 보낸다. 마지막까지 잘 도착했다면, ACK:1501로 마지막 바이트가 끝난 지점을 보낸다. ACK를 수신한 바이트 수만큼 증가시켜서 돌려보내줘야, 유실없이 잘 받았다는 싸인이다. IP(internet Protocol)은 기본적으로 패킷의 유실이 발생할 수 있는 불안정적인 연결을 깔고 가고, TCP는 그만큼 정확한 체크를 통해 서로 안전한 연결성을 보장한다. 3. 연결 종료 four-way handshaking 연결을 종료하는 과정은 소켓프로그래머에게 가장 중요한 단계에 해당된다. 이번에는 연결을 계속 유지되는 것이 기본 TCP소켓 스트림이기 때문에, Client가 server에게 연결종료 하겠다고 알리는 과정이라고 가정한다. Client Server FIN(SEQ:5000,ACK:-) …waitForClientEvent waitForACK… ACK(SEQ:7500,ACK:5001) waitForFIN… FIN(SEQ:7501,ACK:5001) ACK(SEQ:5001,ACK:7502) …waitForACK 이것은 상호간에 종료 통보와 확인을 교환하는 것이다. 서로 메모리를 아끼는 수단. 일반적으로 종료 사인을 보내는 사람이 상대방을 향한 소켓을 종료하겠다는 FIN으로 시작하면, 상대방이 그 FIN에 대해 ACK를 먼저 보내고, 이후 상대방도 자신도 상대방에게 FIN패킷을 보낸다. 마지막으로 연결종료를 시도한 사람이 상대방의 FIN에 대한 ACK패킷을 보내면 통신은 종료되고 서로의 고유한 디스크립터가 붙은 I/O는 캐쉬에서 사라진다.","categories":[],"tags":[{"name":"network","slug":"network","permalink":"https://codenamenadja.github.io/tags/network/"}]},{"title":"소켓프로그래밍 python C nodejs","slug":"python/post/socket_programing","date":"2019-06-19T09:27:36.000Z","updated":"2021-01-25T08:36:06.087Z","comments":true,"path":"2019/06/19/python/post/socket_programing/","link":"","permalink":"https://codenamenadja.github.io/2019/06/19/python/post/socket_programing/","excerpt":"","text":"소켓프로그래밍을 이해할때, 객체 지향적인 언어의 API를 이용해 왔다. nodeJS의 경우와 파이썬의 경우를 먼저 짧게 보여주기로 한다. 1. nodeJS socket Programing 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# typescript로 작성했던 서버사이드 TCP소켓 어플리케이션import * as net from \"net\"class TCPServer &#123; public app: net.Server; public clientSockets: net.Socket[] = []; constructor() &#123; this.app = net.createServer(); this.serverConfigs(); &#125; public static bootstrap() &#123; return new TCPServer(); &#125; private socketConfigs(clientSocket: net.Socket): void &#123; this.clientSockets.push(clientSocket); console.log(`server :client connected to server, all users:$&#123;this.clientSockets.length&#125;`); clientSocket.on(\"data\",(data)=&gt;&#123; console.log(`recevied msg:$&#123;data.toString()&#125;`); if(data.toString().trim().toLowerCase() == \"quit\")&#123; clientSocket.write(\"&gt;&gt;&gt; disconnect order requested!\"); return clientSocket.end(); &#125; this.clientSockets.forEach((otherSocket)=&gt;&#123; if(otherSocket !== clientSocket)&#123; otherSocket.write(data); &#125; &#125;); &#125;); clientSocket.on(\"close\",(isError)=&gt;&#123; const index =this.clientSockets.indexOf(clientSocket); if(index !== -1)&#123; this.clientSockets.splice(index, 1); &#125; &#125;); &#125; private serverConfigs() &#123; this.app.on(\"connection\", (socket) =&gt; &#123; this.socketConfigs(socket); &#125;); this.app.on(\"error\", (err) =&gt; &#123; console.log(`error Message from server : $&#123;err.message&#125;`); &#125;); this.app.on(\"close\", () =&gt; &#123; console.log(\"close server\"); &#125;); &#125;&#125;const singleServer = TCPServer.bootstrap();singleServer.app.listen(8000,()=&gt;&#123; console.log(\"server opende at port 8000\");&#125;) 2. python으로 작성된 크롤링 Tcp소켓 어플리케이션 123456789101112131415161718192021222324252627def fetcher(self): self.sock = socket.socket() self.sock.setblocking(False) try: self.sock.connect(('xkcd.com',80)) except BlockingIOError: pass selector.register( self.sock.fileno(), EVENT_WRITE, self.connected, )def connected(self, key, mask): print('connected') selector.unregister(key.fd) req = 'GET &#123;&#125; HTTP/1.0\\r\\nHOST: xkcd.com\\r\\n\\r\\n'.format(self.url) self.sock.send(req.encode('ascii')) #register next callback selector.register( key.fd, EVENT_READ, self.read_response ) 공통점은 추상화가 매우 잘되어 있다는 점이다. 그 중에서도 노드JS의 추상화는 정말 말도 안되는 수준이라, 우리는 그저 소켓모듈에서 소켓객체를 불러와 callable하거나 한 그것을 생성하고, 전달해주고, 한번 들어온 소켓은 캐쉬에 저장되어 다시 주소를 로드할 필요 없이 연결을 보장하는 스트림인 TCP이기 때문에, 그것을 callback) clientsock.on(\"exit\",callback)``` 로 이벤트를 바인딩 해주어, 클래스 내부에 생성해놓은 static리스트에 넣어놓기만 하면, 알아서 처리해주니, 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970 정확한 과정에 대해서 정말 전혀 몰라도, 아무런 문제없이 돌아가는 것만 같다. - 반면 파이썬은 C를 그대로 가져온 인터프리터여서 그런지, 네이밍등 모양새가 절차적인 특성을 가지고 있다. 그렇다고 해도, 축약되는 부분은 있다. 하지만 이 정도는 박수 받아야 할 정도라는 것은 안다.&gt; 하지만 정확한 절차에 대해서 어떤 과정이 필요한지, 자세히 알아볼 필요가 있다. C의 코드를 기술한다.____## 3. C로 작성된 TCP소켓 어플리케이션```c#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;void error_handling(char *message);int main(int argc, char **argv)&#123; int serv_sock; int clnt_sock; struct sockaddr_in serv_addr; struct sockaddr_in clnt_addr; int clnt_addr_size char message[]=&quot;Hello World!\\n&quot;; if(argc != 2)&#123; printf(&quot;Usage : %s &lt;port&gt; \\n&quot;,argv[0]); exit(1); &#125; serv_sock = socket(PF_INET, SOCK_STRAM, 0); /*서버 소켓 생성*/ if(serv_sock == -1) error_handling(&quot;socket() error&quot;); memset(&amp;serv_addr, 0, sizeof(serv_addr)) serv_addr.sin_family=AF_INET; serv_addr.sin_addr.s_addr=htonl(INADDR_ANY); serv_addr.sin_port=htons(atoi(argv[1])); if(bind(serv_sock, (struct sockaddr*) &amp;serv_addr, sizeof(serv_addr)==-1)) /*소켓에 주소 할당*/ error_handling(&quot;bind() error&quot;); if(listen(serv_sock, 5) == -1) /*연결 요청 대기상태로 진입*/ error_handling(&quot;listen() error&quot;); clnt_addr_size = sizeof(clnt_addr); clnt_sock = accept(serv_sock, (struct sockaddr*)&amp;clnt_addr, &amp;clnt_addr_size); /*연결요청 수락 함수accept*/ if(clnt_sock==-1) error_handling(&quot;accept() error&quot;); write(clnt_sock, message, sizeof(message)); /*데이터 전송*/ close(clnt_sock); /*연결 종료*/ return 0;&#125;void error_handling(char *message)&#123; fputs(message,stderr); fputc(&apos;\\n&apos;,stderr); exit(1);&#125; 서버소켓을 생성한다.(PF_INET:인터넷프로토콜, SOCK_STREAM:TCP방식, 0) 서버소켓에 주소 할당 35~38 줄에서 주소정보 구조체 변수를 초기화해주고, 145에서 서버 소켓에 bind 서버가 주소를 가졌으니, 연결오청 대기상태로 들어가야함. 149에서 listen(서버소켓, 연결대기 시퀸스 길이) 대기 큐에서 대기하고 있는 클라이언트의 연결 요청에 대해서, 수락. 154에서 accept를 통해 대기큐에 첫번째로 기다리고 있는 클라이언트의 연결요청을 수락하고 있다. accept 함수 호출 시 리턴된 파일 디스크립터를 가지고 클라이언트와 데이터를 주고받을 차례. 일반적으로 파일을 가지고 입출력하던 방식과 동일하게 데이터 주고받음. write(클라이언트소켓, message, sizeof)는 시스템이 제공해주는 low Level I/O 이다. 이를 통해 데이터를 전송한다. I/O-bound하다는 것은 여기서 시작된다. 이제 클라이언트에게 전송할 소켓을 닫아주면서 연결을 종료한다.","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"python post","slug":"python-post","permalink":"https://codenamenadja.github.io/tags/python-post/"}]},{"title":"파이썬이 문자열을 저장할 때 어떻게 데이터를 저장하는가? 번역","slug":"python/post/how_python_saves_memory_with_string","date":"2019-06-19T09:25:35.000Z","updated":"2021-01-25T08:36:06.087Z","comments":true,"path":"2019/06/19/python/post/how_python_saves_memory_with_string/","link":"","permalink":"https://codenamenadja.github.io/2019/06/19/python/post/how_python_saves_memory_with_string/","excerpt":"","text":"출처 : https://rushter.com/blog/python-strings-and-memory 파이썬3 이후로 문자 타입은 유니코드를 사용하기 시작했다. 유니코드 문자열은 4bytes(16bits)의 크기까지 캐릭터를 인코딩에 따라 수용할 수 있다. 메모리적 측면에서는 때로 고비용이라고 느껴질 수 있다. 메모리소모를 줄이고 퍼포먼스를 개선하기위해 파이썬은 유니코드 문자열의 3가지 내부적인(Internal rep…) 추상화를 이용한다. 1byte per char(Latin-1 endcoding) 2bytes per char(UCS-2 endcoding) 4bytes per char(UCS-4 endcoding) 파이썬을 프로그래밍할때 파이썬의 모든 문자열은 동일하게 동작한다. 그리고 대부분의 경우에 우리는 어떤 변화도 눈치채지 못한다. 그러나 변화는 매우 주목할만하고, 때로는 예상치못하다 (많은 양의 문자열을 다룰때,) 내부적인 추상화에서 차이를 확인하려면, 우리는 sys.getsizeof 를 이용할 수 있다.(그것은 객체의 크기를 bytes로환산해준다.) 1234567891011121314151617181920import sysstring = 'hello'sys.getsizeof(string)&gt;&gt;&gt; 54# 1-byte encodingsys.getsizeof(string+'!')-sys.getsizeof(string)&gt;&gt;&gt; 1# 2-byte encodingstring2 = '你'sys.getsizeof(string2+'好')-sys.getsizeof(string2)&gt;&gt;&gt; 2#4-byte encodingstring3 = '🐍'sys.getsizeof(string3+'💻')-sys.getsizeof(string3)&gt;&gt;&gt; 4sys.getsizeof(string3)&gt;&gt;&gt; 80 당신이 보는 것처럼 문자열의 내용에 따라, 파이썬은 다른 인코딩을 사용한다. 그것을 기억하라, 파이썬의 모든 개별 문자열은 49~80bytes의 메모리를 추가적인 정보를 보관하는 지점에 추가로 소모한다. 예를 들면 hash, length in bytes, encoding type, string flags 등이 있다. 그것이 빈 문자객체가 49byte의 메모리를 소모하는 이유이다. 우리는 인코딩을 추론할 수 있다, 직접 객체에 대해 ctypes: 를 사용함으로. 123456789101112131415161718192021222324252627import ctypesclass PyUnicodeObject(ctypes.Structure): # internal fields of the string object _fields_ = [(\"ob_refcnt\", ctypes.c_long), (\"ob_type\", ctypes.c_void_p), (\"length\", ctypes.c_ssize_t), (\"hash\", ctypes.c_ssize_t), (\"interned\", ctypes.c_uint, 2), (\"kind\", ctypes.c_uint, 3), (\"compact\", ctypes.c_uint, 1), (\"ascii\", ctypes.c_uint, 1), (\"ready\", ctypes.c_uint, 1), # ... # ... ]def get_string_kind(string): return PyUnicodeObject.from_address(id(string)).kindget_string_kind('Hello')&gt;&gt;&gt; 1get_string_kind('你好')&gt;&gt;&gt; 2get_string_kind('🐍')&gt;&gt;&gt; 4 만약 문자열의 모든 글자들이 ASCll 범위안에 맞을 수 있다면, 그들은 1-byte Latin-1 인코딩을 사용한다. 기본적으로, Latin-1은 최초의 256 유니코드-캐릭터를 추상화한다. 이것은 많은 라틴어를 지원한다. 그러나 라틴어 계열 언어가 아니라면 그럴 수 없다. 아시아, 아프리카등 일대의 언어에 대해서 그러하다. 이것이 그들의 코드포인트(숫자상의 인덱스들)가 1-byte(0-255)범위 밖을 정의하게 된 이유이다. 이모티콘 절때 쓰지마라 문자의 4배 이상 메모리가 소요된다. 123456ord('a')&gt;&gt;&gt; 97ord('你')&gt;&gt;&gt; 20320ord('!')&gt;&gt;&gt; 33 왜 파이썬이 내부적으로 UTF-8을 사용하지 않는가? UTF-8인코딩에 문자가 저장되어 있다면, 개별 글자는 자신이 추상화 하는 정보에 따라 1-4bytes의 메모리를 사용하여 encoded된다. 그것은 저장상 효율적인 인코딩이다. 그러나 한가지 큰 단점이 있다! -개별 문자가 1-4bytes의 메모리에 저장되어 있어서, 문자를 열람하지 않고서, 인덱싱으로 무작위단어에 대해 접근이 불가능하다. 그래서 쉬운 명령인 string[5]의 경우도 UTF-8파이썬은 요구된 문자를 찾을 때 까지 스캔을 해야한다. 고정된 길이의 인코딩은 그런 문제가 없다. 글자를 인덱스에 따라 원본을 찾기위해, 파이썬은 그저 한 캐릭터의 인덱스 번호를 늘리면 그만이다. string interning 빈 문자나 ASC2의 문자열중 특정 문자를 다룰 때, 파이썬은 string interning(문자를 잡아 놓음)을 사용한다. 잡혀있는 문자는 싱글턴 객체처럼 행동해서, 그 말은, 두개의 독립적인 문자객체가 잡혀있다면, 그들은 그들중 하나의 복사본만 메모리에 보관하는 것이다. id()메서드를 통한 다른객체 동일 글자에 대한 동일한 주소에 대한 이야기. 이것은 파이썬의 문자객체는 immutable하기 때문이다. 파이썬에서 문자열을 잡아 놓는 것은, 글자나, 빈 문자객체에만 국한되지 않는다. 문자열,(코드를 컴파일하면서 생성된,)은 그들의 길이가 20글자를 초과하지 않는다면, 또한 저장될(잡히다) 수 있다. 이것은 포함한다. -함수 &amp; 클래스 이름들 -변수명들 -매개변수 이름들 -상수들(코드에 선언된 모든 문자들) -Dictionary의 키 이름들 -속성의 이름들 1234567891011121314151617a = 'teststring'b = 'teststring'id(a), id(b), a is b&gt;&gt;&gt; (4569487216, 4569487216, True)a = 'test'*5b = 'test'*5len(a), id(a), id(b), a is b&gt;&gt;&gt; (20, 4569499232, 4569499232, True)a = 'test'*6b = 'test'*6len(a), id(a), id(b), a is b&gt;&gt;&gt; (24, 4569479328, 4569479168, False) open(‘aa.txt’,‘r’).read()등을 통한 할당은 적용되지 않는다. 그것은 일종의 외부 스트림에 의존 하는 것이지 상수가 아니기 때문이다. 문자를 잡아 놓는 기술은 동일한 내용의 문자선언을 줄여준다. 내부적으로, 문자를 잡아 놓는 것은 global dictionary에 의해 유지된다.(global dictionary, 문자들이 키로서 보관되는 곳이다. 그곳에 이미 특정 문자열이 메모리 안에 있는지 보기 위해 파이썬은 dictionary의 key조회하는 명령을 수행한다.","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"python post","slug":"python-post","permalink":"https://codenamenadja.github.io/tags/python-post/"}]},{"title":"파이썬의 GC와 메모리 관리자","slug":"python/post/memory_and_GC","date":"2019-06-19T09:22:23.000Z","updated":"2021-01-25T08:36:06.087Z","comments":true,"path":"2019/06/19/python/post/memory_and_GC/","link":"","permalink":"https://codenamenadja.github.io/2019/06/19/python/post/memory_and_GC/","excerpt":"","text":"reference counting -official Python doc Py_INCEREF(), Py_DECREF()등을 통하여 레버런스 Count를 관리하고, (파이썬 인터프리터와 메모리관리자가) refcount가 최초로 0에서 1로 Init되면서, garbage collection의 Generation[0]부터 등록을 시작한다. refcount에 따라서 자연스럽게 Deallocate memory를 시도하는데, 만약 Cyclic Garbage Collector에 최초에 참가한다는 타입이라고 초기화된 객체라면(PyObject_New()를 Exnted 하는 PyObject_init()으로 인한 선언이었을때, : 추가적으로 타입등의 정보를 추가하여 객체의 Fields를 더해서 선언하는, detector’s Set of observed Objects에 추가된다. 이것이 객체 생성의 최종 마무리 이며 다른 필드에 대한 영향은 없다. 함수 실행과 종료와함께 Deallocate되는 객체에 대해서는 따로 GC에 대한 등록없이 스텍과 함께 사라지도록 구현되어있을것이다. 그러나 순환참조가 일어나는 객체라면 초기화 당시, 타입과 함께 Python_init()으로 선언될 때 Garbage collection의 어느 부분에 dictionary의 형태로 관리된다. GC의 라이프사이클에 대해 gc.get_thredhold()를 통하여 리턴되는 것은 generation[0]~ generation[2] 까지 수용할 수 있는 총 객체의 수다. 최초 객체 선언시 Refcount도 올라가지만, Generation[0]에 등록함으로써 generation에 바인딩된 이벤트가 특정 수치에 도달하였을 때 generation[0].count=0 gen[1].count++, 그리고 gen[0]에서 아직 레퍼런스가 남아있거나, cyclic garbage collector의 set of observed object에 등록되어 있고 아직 유효하다면 0세대에 등록되어 있는 객체들은 1세대로 등록된다. 메모리 해제를 위한 검사가 실행되면 gen[2]부터 gen[0]의 순서로 검사가 실행된다. 순환참조가 일어나지 않는 객체의 메모리 해제 기본적으로 객체는 Refcount라는 개념이 내장되어있다, 그 객체가 Gc의 generation에 등록되어 있지 않다고 한다면, 자체적으로 Ref가 0이 되는 스스로의 allocation을 해제한다. 인스타그램은 GC를 사용하지 않기로 했다고한다. 인스타그램의 서버는 순환참조를 일으키는 객체의 생성을 최대한 막고 함수지향적인 코드를 작성하여 서버를 운영해 왔다고 한다. 그러나 GC가 늘 onloadState이라는 점이나 일부 순환참조가 일어나는 객체들 등에 의해서, cache miss가 적지 않다고 느꼈다. 그래서 파이썬 오피셜이 제안했던 것처럼 gc.disable()을 통해 해제 했으나, 서드파티에서 gc.enable()을 호출하는 등 결국 효과를 보지 못했기 때문에, threshold의 제한수를 없애주어서 Gc가 수행되는 시점을 사라지게 했다.\\ 수행되는 시점에 그 모든 객체에 대해서 순환참조 탐지 알고리즘을 적용하는 것이 기존의 Cache를 가득 메우는 점이 맘에 안들었던 것이다. 그 양이 적지 않아 워낙에 대규모 서비스 이다보니, 다른일로 돌아가야 하는데도 가상메모리상에 pageFault가 일어났다고 한다. 결과 개별 서버에 8gb 정도의 메모리사용량을 비울 수 있다고 했다. 이는 더 많은 메모리 소모를 필요로 하는 서버를 생성할 수 있다는 얘기이다. CPU의 IPC(Cycle 당 instruction 수)가 10% 가까이 증가하였다. cache miss가 2~3%가량 이득을 보았던 점이 IPC성능의 관건있다고 한다. cache miss는 큰 자원소모이다. cache miss -&gt; CPU Pipeline에 Stall발생 youtube(https://www.youtube.com/watch?v=twQKAoq2OPE)에 등장하는 것처럼 최대한 많은 공유 라이브러리를 공유 메모리로서 캐쉬안에 보관 될 수 있어서, 많은 page들이 캐싱상태에 존재하고이는 Stall을(CPU와 캐시사이의 새로운 allocation등을 위한 Fetch를 포함한 결과까지) 최대한 막아주기 때문에 Cpu가 빠르게 동작하도록 한다. OS는 프로세스들이 (코드 혹은 데이터 세그먼트)공유를 통해 최적화를 하기 원하지만, 힙 안에서, 개별적인 세그먼트를 통해 독립적인 네임스페이스를 형성하기 때문에, reference-counted code objects는 이 이점을 가질 수 없다. 1—&gt; 36-- r libc.so (C 기반-라이브러리를 읽어와서) 0—&gt; 35-- r python.exe (파이썬을 플랫폼으로) 프레임 워크가 기인하는 라이브러리(C등으로 구성된,)는 stdio를 임포트하는 라인이 있더라도, 해당라인은 명령으로서 미리 힙의 크기만 빈 크기만 할당하고, 실제로 필요해졌을때, 해당요소를 HDD레벨에서 로드하여 가져온다. 동등한 프로세스의 경우 PID가 달라서 프로그램은 그들의 코드레벨 혹은 Data 레벨이 어떤 동일한 데이터를 공유할 수 있는지 알수 없다. ChildProcess개념에서 childprocess.spawn() as fork까지 개념이 늘어가면, 기존의 프로세스가 spawn한 child process가 자신의 부모프로세스에 대해 코드레벨에서 동일한 프레임을 참조하는 페이지 공유환경이 성립된다. 스텍이 구현된 프레임은 자식프로세스가 새로운 콜스텍 메모리 w해야 하기 때문에 다른 페이지로 매핑된다.","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"python post","slug":"python-post","permalink":"https://codenamenadja.github.io/tags/python-post/"}]},{"title":"파이썬의 메모리 관리 번역","slug":"python/post/memory_management","date":"2019-06-19T09:20:11.000Z","updated":"2021-01-25T08:36:06.087Z","comments":true,"path":"2019/06/19/python/post/memory_management/","link":"","permalink":"https://codenamenadja.github.io/2019/06/19/python/post/memory_management/","excerpt":"","text":"파이썬 내의 메모리 관리는 사적인 힙을 포함한다, 모든 파이썬 객체와 자료구조를 포함하는. 이 프라이빗한 힙의 관리는 내부적으로 파이썬 메모리 관리자에의해 보장받는다. 파이썬 메모리 관리자는 다른 컴포넌트들을 가지고 있다, 다양한 다이나믹 저장소 관리 측면에서 업무를 하는. 예를 들면 공유 분할, 선할당 혹은 캐싱처럼 말이다. 가장 낮은 레벨에서 본래 메모리 할당자는 보장한다, 프라이빗 힙엔 충분한 방이 있다고, 모든 파이썬 관련 데이터를 보관하기 위해, 운영체제의 메모리 관리자와 소통함으로써. 순수 메모리 할당자 위에, 몆몆 객체-특정 할당자가 동일한 힙에서 작동하고, 개별적인 메모리 관리 정책을 수행한다, specific 모든 객체 타입에 대해 적용되는 정책. 예를 들면 정수객체는 힙안에서 문자객체들, 튜플객체들 혹은 딕셔너리 객체들과 다르게 관리된다 힙 안에서, 왜냐하면, 정수는 다른 저장소 요구사항을 적용받고, 속도와 스페이스에 있어서 트레이드 오프가 있기 때문이다. 파이썬 메모리 관리자는 그러므로 특정-객체 할당자들에게 일부 업무들에 있어서 달라붙는다. 그러나 후에 연산이 실행됨이 프라이빗 힙에서 진행된다는 것을 보장한다. 이것은 중요하다 이해하는 것에 있어서, 파이썬 개인힙의 관리가 인터프리터 그 자체로 의해 진행되고, 유저는 그것에 대한 컨트롤이 없다는 것, 비록 만약 유저들이 주기적으로, 자주 객체들이 힙의 메모리 블락을 가르키도록 다루더라도 말이다. 힙-스페이스에 대한 메모리 할당에 있어서 파이썬 객체들은 그리고 다른 내부적인 버퍼들은 파이썬 메모리 관리자에 의해서 요구되는 것으로 인해 실행된다. Python/C API functions가 그들의 문사에 리스트화 된 것을 통해서. 충돌을 막기 위해, 확장된 작성자들은 절때 파이썬 객체를 C라이브러리에서 송출된 함수들로 조정하려 해서는 안된다. C라이브러리: malloc(), calloc(), realloc() and free(). 이것은 결과로 할것이다, 뒤섞인 호출들, C-할당자와 파이썬 메모리 매니저 사이에, 치명적인 결과와 함께, 왜냐하면 그들은 다른 알고리즘을 수행하고, 다른 힙에서 구동되기 때문이다. 그러나 한가지는 안전하게 할당되고 메모리블럭들을 놓아줄수 있다. 특별한 목적을 위한 c-라이브러리 할당자에 의해서. 그 예시는 아래예시이다. 123456789PyObject *res;char *buf = (char *) malloc(BUFSIZ); /*for I/O*/if (buf == NULL) return PyErr_NoMemory();#...Do some I/O operation involving buf...res = PyBytes_FromString(buf);free(buf)' /*malloc'ed*/return res; 이 예제에서, 메모리의 I/O버퍼에 대한 요청은 C-라이브러리 할당자에 의해서 이루어진다. 파이썬 메모리 매니저는 오직 문자객체가 결과로서 리턴되는 것의 할당에관해서만 관련된다. 대부분의 상황에서, 그러나, 이것은 추천된다, 메모리를 파이썬 힙으로부터 구체적으로 할당하는 것을. 왜냐하면 나중의 문제는 파이썬 메모리 매니저에 의해서 관리될 수 있기 때문이다. 예를 들어 이것은 요구된다 인터프리터가 C로 작성된 새로운 객체타입으로 확장되었을때. 다른 이유로 파이썬 힙을 사용하는 것은 파이썬 메모리 매니저에게 알리기 위해서 이다. 확장된 모듈에 대한 메모리 요구를. 만약 요구된 메모리가 예외적으로 내부적으로 사용되고, 높은 수준의 특별한 목적에 의해 사용하더라도, 파이썬 메모리 매니저에게 메모리에 관한 모든 요구를 찾아 올려보내는 것은 인터프리터로 하여금 더 많은 이것의 메모리 전체 기록에 관한 정확한 이미지를 요구하도록 야기할 것이다. 결과적으로, 특정 환경 아래에서, 파이썬 메모리 관리자는 적절한 액션을 취할 수 있고 아닐 수 있다. 예를 들면 Garbage-collection 처럼, 메모리 컴팩트화 혹은 다른 예방차원의 과정을. C-라이브러리를 이전 예제처럼 사용하는 것은, I/O버퍼를 위해 할당된 메모리가 파이썬 매니저로부터 도망가도록 한다는 것을 기억하라.","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"python post","slug":"python-post","permalink":"https://codenamenadja.github.io/tags/python-post/"}]},{"title":"비동기 코루틴 번역-1","slug":"python/post/async_coroutine_1","date":"2019-06-19T09:11:02.000Z","updated":"2021-01-25T08:36:06.083Z","comments":true,"path":"2019/06/19/python/post/async_coroutine_1/","link":"","permalink":"https://codenamenadja.github.io/2019/06/19/python/post/async_coroutine_1/","excerpt":"","text":"A Web crawler with ayncio Coroutines Written by, A.Jesse Jiryu and guido van Rossum intoduction 고전적인 컴퓨터 과학은 강조한다 효율적인 알고리즘을, 컴퓨테이션을 가능한 빠르게 수행하는 알고리즘. 그러나 많은 네트워크 프로그램은 그들의 시간이 아니라, 많은 연결을 유지하는 곳에, 그것은 느리거나 드문 이벤트를 지닌다. 이 프로그램들은 많은 다른 도전을 과시한다.: 많은 거대한 네트워크 이벤트들을 효과적으로 기다리기를. 이 문제에 대한 동시대의 접근은 비동기적 I/O 혹은 Async이다. 이 챕터는 간단한 웹크롤러를 보여준다. 크롤러는 전형적인 비동기 어플리케이션이다. 왜냐하면 이것은 많은 응답들을 기다리고 있기 떄문이다. 그러나 작은 컴퓨테이션을 수행한다. 많은 페이지를 한번에 송수신 한다면, 더 빠르게 이것은 종료된다. 이것이 만약 쓰레드를 진행중인 각각의 리퀘스트에 힘을 쏟는다면, 그러면 동시성 리퀘스트 수가 느는 것 만큼 이것은, 메모리가 부족해질것 이거나, 다른 쓰레드관련 리소스를 소모하게 될 것이다, 이것이 소켓을 다 소모하기 전에. 이것은 쓰레드에 대한 필요성을 피하게 해준다. 비동기 IO를 사용함으로써. 3가지 스테이지로 나눠서 예를 보여준다. 첫째 스테이지: 우리는 비동기 이벤트루프를 보여주고, 콜백을 사용하는 이벤트루프를 이용하는 크롤러를 그린다. 이것은 매우 효과적일 수 있다. 그러나 이것을 더 복잡한 문제로 끌고 갈수록 그것은 관리할 수 없는 스파게티코드로 인도할 것이다. 둘째 스테이지: 그러므로 우리는 파이썬 코루팅들이 효과적이면서 동시에 확장가능하다는 것을 보여줄 것이다. 우리는 간단한 코루틴을 실행할것이다. 파이썬에서 제네레이터를 사용함으로써. 셋째 스테이지: 우리는 최대로 능력을 끌어올린 파이썬 코루틴을 사용할 것이다. 파이썬의 표준 &quot;asyncIO&quot;라이브러리, 그리고 그들을 async queue를 이용해 조직화 한다. The Task 웹 크롤러는 모든 페이지를 찾고 다운로드한다, 비록 그들을 기록하거나, 번호를 메기려 할 목적이라도 말이다. 루트 URL로 시작해서, 모든 페이지를 fetch하고, 그것을 보여지지 않은 페이지들의 링크로 파싱한다. 그리고 그들을 queue에 올린다. 이것은 모든페이지에 대해서 Fetch가 완료하고, queue가 비었을 때 멈출 것이다. 우리는 이 프로세스를 많은 페이지를 동시적으로 다운로드 함으로써, 가속할 수 있다. 크롤러가 새로운 링크를 찾으면, 이것은 동시Fetch 명령을 실행한다.(개별소켓위의 새로운 페이지들을 얻어 오기 위한) 이것은 응답이 도착하는 데로 파싱하고, 새로운 링크를 큐에 올린다. 거기엔 그런 리턴을 절감하는 포인트들이 함께 올 수 있다.(어딘가 너무 많은 동시성이 퍼포먼스를 저하시키는 곳에) 그래서 우리는 동시성 리퀘스트의 수의 한도를 정한다. 그리고 큐에 남은 링크들을 그냥 둔다. 일부 진행중인 리퀘스트가 끝날 때까지. Traditional approach 고전적으로 우리는 쓰레드 풀을 생성해서 크롤러를 동시적으로 만들었다. 각 쓰레드는 소켓을 통해서 한번에 하나의 페이지를 다운받는 역할을 맡고 있었다. 예를 들어 다운로드 xkcd.com을 한다면: 1234567891011121314def fetch(url): sock = socket.socket() sock.connect((\"xkcd.com\",80)) req = 'GET &#123;&#125; HTTP/1.0\\r\\nHost: xkcd.com/r/n/r/n'.format(url) sock.send(request.encode('ascii')) response = b'' chunk = sock.recv(4096) while chunk: response += chunk chunk = scok.recv(4096) #Page is now downloaded. links = parse_links(response) q.add(links) 기본적으로 소켓의 수행은 블로킹이다. 쓰레드가 connect, recv등을 수행할때 그러하다. 이것은 수행이 끝날때까지 멈춰있다. 동시적으로 많은 페이지를 한번에 받기위해서, 우리는 많은 쓰레드가 필요하다. 정교한 어플리케이션은 쓰레드를 생성하는 비용을 쓰레드풀에 대기중인 쓰레드를 보관함으로써 분할처리한다. 그리고 그들을 동시적인 일에 재사용 할 수 있게 체크해나간다. 이것은 소켓의 커넥션풀과 동일한 것을 하는 것이다. 그러나 쓰레드는 비싸다, OS는 그들을 프로세스당 쓰레드의 수가 많아지는 것을 강력하게 제어한다. 프로세스나 유저, 혹은 머신이 가질 수 있는 쓰레드말이다. 제시의 시스템에서 파이썬 쓰레드는 50k정도 비용의 메모리를 소모한다. 천개중 열개의 쓰레드를 실행하는 것은 실패를 부른다. 그것을 동시적인 명령으로 동시성 소켓으로 확장한다면, 소켓이 다 떨어지기전에, 쓰레드가 다 떨어진다. 쓰레드당 오버헤드 혹은 시스템의 제한은 굉장히 목을 조른다. 영향력있는 기사인 The C10K problem - Dan kegel 에서는 IO동시성을위한 멀티쓰레딩의 제한을 그린다. 그는 이렇게 시작한다. 이제 웹서버가 만명의 클라이언트를 동시에 처리해야 한다. 그렇지 않은가? 어찌됐든, 웹은 이제 커다란 장소가 되었다. 1999년에 작성된 이것은 만개의 연결이라고하면 다소 앙증맞게 들린다. 그러나 그떄와 달라진건 규모일 뿐이지 같은 이야기를 우리는 하고 있다. 그때는 연결당 쓰레드를 생성하는 것은 실용적이지 않았다. 이제 OS의 쓰레드 생성제한수는 높아졌다. 게다가 우리의 장난감 웹 크롤러는 쓰레드만으로 잘 작동할 것이다. 그러나, 거대한 어플리케이션에서, 10만개의 연결과 함께하는 그런 어플리케이션에서는, 제한 수는 이것을 말한다.: 거기에는 제한수를 뛰어넘게 소켓을 생성할 수 있는 시스템이 많은데, 쓰레드는 부족하다. 이걸 우린 어떻게 극복할 것인가?","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"python post","slug":"python-post","permalink":"https://codenamenadja.github.io/tags/python-post/"}]},{"title":"비동기 코루틴 번역-2","slug":"python/post/async_coroutine_2","date":"2019-06-19T09:11:02.000Z","updated":"2021-01-25T08:36:06.083Z","comments":true,"path":"2019/06/19/python/post/async_coroutine_2/","link":"","permalink":"https://codenamenadja.github.io/2019/06/19/python/post/async_coroutine_2/","excerpt":"","text":"Coroutines 우리는 당신을 Promise로 유도하고 싶다. 비동기처리에 대해서 콜백의 효율성과 조합하여 과거 멀티쓰레딩 프로그래밍의 클래식한 것을 사용해서 작성하는 것이 가능하다. 이 조합은 &quot;Coroutine&quot;이라는 패턴과 함께 얻을 수 있는 것이다. 1234@async.coroutinedef fetch(self,url): respnse = yield from self.session.get(url) # 콜백의 첫 단계 body = yield from response.read() # 연속적 이벤트에 대한 콜백 이것은 또한 스케일러블하다. 멀티쓰레딩으로 작업하던 당시, 50Kb의 자원이 하나의 쓰레드를 생성하는데 들어갔었고, OS는 그것에 대해 엄격한 CAPs를 적용하고 있었다. 그러나 파이썬의 코루틴은 거의 3K만을 차지하고 있고, 따라서 매우 쉽게 수십만개의 코루틴한 객체들을 생성하고 실행할 수 있다. 코루틴에 대한 컨셉은 아주 쉽다!!? (고전적인 컴퓨터 과학과 비추어 봤을때…) 그것은 서브루틴(함수 개념)이며 멈출 수 있고 다시 진입 할 수 있다. 반면에 쓰레드는 우선적으로(OS가 강제력을 가지는 스케쥴링 방식) OS에 의해 멀티테스킹 되어진다. asyncio라이브러리에 구현되어 있는 코루틴들은 제너레이터를 기반으로 구성 되어 있다. Future class와 yield from선언자가 그것이다. 그것은 파이썬의 프라이빗 힙 중 스텍프레임에 직속으로 할당되는 프레임들이기에, 기존의 메인 쓰레드를 중심으로 생성된 쓰레드가 스텍프레임으로서 레벨에 존재한다. 개별적으로 이전의 공유자원에 대해 접근할 수 있지만, Race condition이 발생할 수 있고, 프로그래머 레벨에서 MUTEX(Mutual exclusion) 처리로 핸들링을 해주거나, 세마포어 기법을 통해 멀티프로그래밍 환경에서 실행자들이 임계구역(공유자원)에 접근하기 전에 각각 전처리, 후처리 등을 추가하는 방법이 있다. MUTEX는 기본적으로 작동원리의 레벨이나, 그 기저에 위치한 것이 아니라, 업무의 할당이라는 개념으로 로직-코드레벨에서 직접 제어하는 것이다. 그러나 코루틴은 자신이 서브루틴으로서 직접 언제 멈출지, 이후 어떤 코루틴을 실행할 지 선택할 수 있다. 코루틴에 대한 많은 실행법이 있지만, (“파이썬만 해도 여러가지를 가지고 있다!”) 스탠다드 라이브러리 &quot;asyncio&quot;에 있는 코루틴들은 제너레이터를 기반으로 만들어져 있다 Future class yield from CALLBACK_on_Socket_connected() yield 하는 싱글 스레드의 주도권은 일시적을 해당 제너레이터로 넘어간다. yield from 을 통해 외부 Global frame에서 전달받은 것은 자신의 프레임 내부로 가져온다. routine과 위와 같은 방식으로 소통한다. python.exe입장에서는 자신의 프로세스가 늘어나는것도, 공식적인 쓰레드가 늘어나는 것도 아니어서, OS에 대해서 영향을 주는 것이 아니다. 그저 자신의 기존 스텍에 대해서 혼란을 주지 않고, 힙에 있는 가스텍을 메인스텍으로 대체한다는 개념에 더욱 가깝다. 파이썬 3.5로 부터, 코루틴은 파이썬 언어의 네이티브한 특성이 되었다. 3.4버전 부터 구현된 코루틴을 기존의 언어의 자원을 이용하여 이해하는 것은 3.5버전의 네이티브 코루틴을 걸고넘어지는 기초가 된다. 3.4버전의 제너레이터 기반의 코루틴을 설명하기 위해서, 우리는 일단 제너레이터를 이해하는 것부터 시작해서, 어떻게 그들이 asyncio에서 코루틴으로 사용되었는지 접근 해야한다. 제너레이터 기반 코루틴을 설명하고 나면, 그것을 기꺼이 비동기적 웹 크롤러에 사용할 것이다.","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"python post","slug":"python-post","permalink":"https://codenamenadja.github.io/tags/python-post/"}]},{"title":"비동기 코루틴 번역-4","slug":"python/post/async_coroutine_4","date":"2019-06-19T09:11:02.000Z","updated":"2021-01-25T08:36:06.083Z","comments":true,"path":"2019/06/19/python/post/async_coroutine_4/","link":"","permalink":"https://codenamenadja.github.io/2019/06/19/python/post/async_coroutine_4/","excerpt":"","text":"제너레이터로_코루틴_구성하기 우리의 코루틴들은 Asyncio라이브러리 등으로 단순화 될 것이다. 우리는 generator, future, yield from을 사용할 것이다. 123456789101112class Future: def __init__(self): self.result = None self._callbacks = [] def add_done_callback(self,fn): self._callbacks.append(fn) def set_result(self,result): self.result = result for fn in self._callbacks: fn(self) 위의 future은 원천적으로 pending상태이다. 그리고 set_result를 통해서 resolved된다. fetcher에 futures와 코루틴을 적용해 보자. 123456789101112131415class Fetcher: def fetch(self): self.sock = socket.socket() self.sock.setblocking(False) try: self.sock.connect(&#123;\"xkcd.com\", 80&#125;) except BlockingIOError: pass selector.register(self.sock.fileno(), EVENT_WRITE, self.connected ) def connected(self, key, mask): print(\"connected!\") fetch 메서드는 소켓연결을 시도하고, 콜백을 등록한다. 그 콜백은 소켓이 준비되었을때, 실행될 것이다. 우리는 이 두 단계를 하나의 코루틴으로 조합할 것이다. 123456789101112131415161718192021222324def fetch(self): sock = socket.socket() sock.setblocking(False) try: sock.connect(&#123;\"xkcd.com\", 80&#125;) except: pass f = Future() def on_connected(): f.set_result(None) selector.register( sock.fileno(), EVENT_WRITE, on_connected ) yield f selector.unregister( sock.fileno() ) print(\"connected\") 이제 fetch는 제너레이터 함수이다. 우리는 pending중인 future을 생성한다. 그리고 yield를 통해서 이것을 정지시키고, 소켓이 연결되었을 때까지, stop상태로 heap에서 대기한다. on_connected함수가 등록되어 커널이 소켓에서 예약해놓은 이벤트 시그널을 감지하면, 다시 실행될 것이다. 그 콜백인 내부 on_connected가 future을 진행 시킬 것이다. 그러면 future이 진행될 때, 무엇이 제너레이터로 재진입하게 만드는가? 우리는 코루틴 driver가 필요하다. 이것을 task라 명명하겠다. 1234567891011121314151617181920class Task: def __init__(self, coro): self.coro = coro f = Future() f.set_result(None) self.step(f) def step(self, future): try: next_future = self.coro.send(future.result) except StopIteration: return next_future.add_done_callback(self.step)# begin fetchingfetcher = Fetcher(\"/353/\")Task(fetcher.fetch())loop() task는 fetch제너레이터를 None을 send()함으로써 시작시킨다. fetch는 그것이 future을 yield하기 전까지 동작한다. 그 future은 task가 next_future로써 처리한다. 소켓이 연결되고 나면, 이벤트 루프가 on_connected를 실행시킨다. 그 함수는 future을 진행시키고, step을 호출한다. 그 step함수가 코루틴 함수로 재진입을 실행한다. 코루틴을_yield from으로_전환하기 소켓이 연결되고 나면, 우리는 GET에 대한 응답을 받을 것이다. 이러한 과정은 더이상 callback들 사이에 분산될 필요가 없다. 그것을 하나의 제너레이터 함수에 모을 것이다. 1234567891011121314151617181920212223def fetch(self): sock.send(request.encode(\"ascii\")) while True: f = Future() def on_readable(): f.set_result(sock.recv(4096)) selector.register( sock.fileno(), EVENT_READ, on_readable ) chunk = yield f selector.unregister(sock.fileno()) if chunk: self.response += chunk else: # Done Reading break 햔재 코드는 소켓에서 모든 메세지를 읽고 있다. 우리가 이 fetch에서 어떻게 서브루틴으로 전환 할 수 있을까? yield from을 사용해야 할 때이다, 예시를 보자 123456789101112131415161718192021222324252627def gen_fn(): result = yield 1 print(f\"result of yield_1 &#123;result&#125;\") result_2 = yield 2 print(f\"result of yield_2 &#123;result_2&#125;\") return \"done\"def caller_fn(): gen = gen_fn() rv = yield from gen # line 10 print(f\"return val of yield-from: &#123;rv&#125;\") return \"all done\"if __name__ == \"__main__\": caller = caller_fn() caller.send(None) print(\"instruction: \", caller.gi_frame.f_lasti) # intruction: 10 caller.send(\"hello\") # result of yield_1 hello print(\"instruction: \", caller.gi_frame.f_lasti) # instruction: 10 caller.send(\"goodbye\") # result of yield_2 goodbye # return val of yield-from: done # Traceback... # StopIteration: all done caller가 gen으로 부터 yield from 하는동안, caller은 instruction상태를 유지한다. 루틴으로부터 send받은 것을 yield from 으로 컨텍스트를 양도한다. caller밖의 시점에서 바라볼때, 우리는 그것이 caller에서 오는 것인지, 또 다른 제너레이터에서 오는 것인지 알 수 없다. 그리고 gen내부에서, 우리는 또한, caller에서 온것인지, 루틴에서 온것인지 알 수 없다. yield from진술은 마찰없는 선언문이다. 값들이 gen이 종료되기 전까지, 안팎으로 이동한다. 코루틴은 이렇듯 yield from을 통해서, 컨텍스트를 서브-코루틴으로 양도하고, 그 결과를 받을 수 있다. 마지막에 gen에서 돌려받은 값을 &quot;return val of yield-from: done&quot;으로 받은 것을 생각하자. rv = yield from gen 우리가 초기에 callback기반 비동기 프로그래밍을 평가 할때, 우리의 주요 불평은, stack ripping에 대한 것이었다. stack-ripping: 콜백이 예외를 발생하면, stack trace가 무의미해진다. 그러면 우리가 만든 코루틴 기법은 어떤 결과를 보일까? 1234567def gen_fn(): # raise Exception(\"my error\") result = yield 1 print(f\"result of yield_1 &#123;result&#125;\") result_2 = yield 2 print(f\"result of yield_2 &#123;result_2&#125;\") return \"done\" 가장 link-depth가 깊은 스텍에 에러를 억지로 발생해 보면, 정확히 에러를 분석할 수 있는 것을 알 수 있다. caller_fn이 gen_fn을 바라보고 있다는 것을 스텍 트레이싱을 통해서 알 수 있다. 우리는 서브-코루틴을 호출할때, 래핑을 하여 에러 핸들링을 할 수 있다. 12345678def caller_fn(): gen = gen_fn() try: rv = yield from gen except Exception as exc: print(f\"caught &#123;exc&#125;\") print(f\"return val of yield-from: &#123;rv&#125;\") return \"all done\" 따라서 우리는 서브-코루틴을 마치 일반적인 함수(서브루틴)처럼 로직은 정비하였다. 본론으로 돌아가 우리의 fetcher에서 부터 서브코루틴을 정비하자. 우리는 read코루틴을 작성하고, 하나의 chunk를 받을 것이다. 12345678910111213141516171819202122232425262728293031323334def read(sock): f = Future() def on_readable(): f.set_result(sock.recv(4096)) # self.result로 받으면 등록된 콜백을 전부 순회하고 종료한다. selector.register(socket.fileno(), EVENT_READ, on_readable) chunk = yield f # EVENT_READ가 발생하면 onreadable을 이미 등록하고, 동작하면이 아니라 바로 Future 인스턴스를 돌려준다. # 해당 소켓은 논 블로킹 소켓으로 4kb이상 들어오면 그만큼 읽어내고 flush # 모든 약속들은 Future인스턴스에 의해 처리된다고 보면 된다. selector.unregister(sock.fileno()) return chunk # chunk는 읽은 데이터가 아니라, 루틴에서 전달 받은 것.def read_all(sock): response = [] chunk = yield from read(sock) # yield 1회 하고 종료 바로 돌아온다. while chunk: # 루틴에서 전달 받은것이 유효한 동안. response.append(chunk) # chunk = yield from read(sock) return b\"\".join(response)# reads = read_all(sock)# reads.send(None) f를 돌려주고, 종료 chunk에는 값이 할당되지 않는다.# 이후 넌블로킹 소켓에서 데이터를 읽으면 예약한 대로, 4kb 청크 데이터에 대한 Future에 예약된 콜백들을 실행하지만 별도 send를 추가로 진행해야 read가 마저 진행된다.# reads.send(True) read로 전달, read의 chunk에 처음으로 값이 맺힌다.# 연달아 read는 1회 종료되고, unregi된다. read_all로 리턴하면서 종료한다.# read_all의 chunk에 chunk를 리턴한다.# Future_set_result를 통한 콜백 순회가 이루어 지고, 그러면 그 콜백의 마지막 요소가# reads.send(True)로 다시 실행하면,# 만약 그러다가 읽을 데이터가 이제 없다면, 소켓 selector타입아웃으로 처리해서, chunk를 False로 리턴하는 함수도 예약하면,# 마지막에 read_all에 send(False)한 것은 b\"\".join(response)를 통해서 완전한 응답을 받을것이다.# 요청이 들어온 것에 대한 하나의 처리가 종료되는 것. yield from read는 2번의 send를 통해 종료되기 전까지 read all을 진행시키지 않는다. 그리고 1회 실행에서 read_all이 정지되어 있는 동안, asyncio의 EVENTLOOP이 등록되어 있는게 계속 돌아간다. 1234class Fetcher: def fetch(self): sock.send(request.encode(\"ascii\")) self.response = yield from read_all(sock) 기적적이게도, Task class는 수정이 필요하지 않다. Task(fetcher.fetch()) loop() read가 future을 yield할 때, task는 이것을 yield from 진술을 통한 연결에서 전달 받는다. 정확하게 future이 fetch에서 직접적으로 yield되는 것처럼 되는 것처럼. loop가 future을 진행시키면, task는 그것을 결과를 fetch로 보낸다. 그리고 값은 read에서 돌려받는다. 마치 정확히 task가 read를 조종하는 것처럼 보인다. class name method return Task init(self, coro) self.coro = coro, f = Future(), f.set_result(None), self.step(f) Task step(self, future) next_future = self.coro.send(future.result), next_future.add_done_callback(self.step) Fetcher init(self, path) self.location = path Fetcher fetch(self) socket setup, socket.connect(path), f = Future, selector.register(socket, EVENT_WRITE, f.set_result(None)), yield f, selector.unregi(socket) Future init(self) self.result = None, self._callbacks = [] Future add_done_callback(self, fn) self._callbacks.append(fn) Future set_result(self, result) self.result = result, for fn in _callbacks: fn(self) from name do Fetcher_instance Fetcher.init(self, path) fetcher = Fetcher(&quot;/353/&quot;) Task_instance Task(fetcher.fetch()) task.coro = fetcher.fetch(), f = Future()… Future_instance Future.init(self) self.result = None, self._callbacks = [] task Task(fetcher.fetch()) …self.step(f) task step(f) next_future = self.coro.send(future.result#None)… fetcher send(None) yield new Future() task step(f) …new_future.add_done_callback(self.step) selector_EVENT_WRITE_Callback future.set_result(None) task.step(None) task step(f) self.coro.send(None)… fetcher send(None) unregister(socket), print(“connected”), raise StopIteration task step(f) … except StopIteration: return 이것으로 1회 Path에 대한 소켓 연결이 활성화 되어 WRITE가 가능해진 상태가 되었음을 공지 받는다. 우리의 코루틴 구현을 좀 더 치밀하게 만들기 위해서, 하나를 수정한다. 우리는 future이 socket이벤트를 기다릴때, Fetcher.fetch에서 현재 yield를 사용해서, future인스턴스를 리턴한다. 하지만 yield from을 사용해서, 서브코루틴을 가르키도록 할 것이다. 이것은 더욱 깔끔한 것이 될 것이다. 그러면 yield from을 사용하는 코루틴은 더이상 어떤 타입에 대해서 기다리는지 걱정할 필요가 없게 된다. 1234class Future: def __iter__(self): yield self return self.result 이제 next(f)는 코루틴 함수가 되었고, yield f 를 yield from f로 교체하도록 한다. 기능상 동일하다. 진행중인 Task는 future을 send를 통해서 받고, 그리고 future이 EVENT_WRITE되고 나면, yield from의 2번째가 작동할 것이다. 그렇다면, yield from을 사용하는 이점은 무엇인가? yield로 바로 돌려 받는 것과 yield from을 통해 서브코루틴을 가르키도록 하는 것은 무엇이 더 나은가? 메서드는 caller의 영향 없이 자유롭게 구현을 변환 할 수 있다. 두 경우 모두 다, caller은 오직 yield from메서드만을 통해서 결과를 기다릴 수 있게 된다. 우리는 어떻게 asyncio가 두 세상을 동시에 가지는지 그려냈다.: 동시성 I/O가 멀티쓰레드보다 더 효과적이고, callback보다 더 정교하다. 물론 실제 asyncio는 우리의 밑그림 보다 더욱 정교하다. 실제 프레임웤은 zero-copy I\\O, fair scheduling, exception handling을 요구한다. asyncio유저에게 코루틴을 활용해 코딩하는 것은 당신이 여기서 본 것 보다 훨씬 간단하다. 우리는 첫번째 원칙부터 코루틴을 구현해 낸것이다. 그렇게 때문에 callbacks, tasks, futures등을 본 것이다. 그리고 Non-blocking소켓까지 보았고, select를 호출했다. 그러나 asyncio를 사용하면 위의 것들은 전혀 등장하지 않을 것이다. 우리가 위에서 그렸듯이 아주 부드럽게 URL을 fetch할 수 있다. 12345678@asyncio.coroutinedef fetch(self, url): response = yield from self.session.get(url) #session이 yield를 거쳐서 최종적으로 return 하는 것. send(something)은 session.get의 yield로 전달 된다. body = yield from response.read() # Response.read가 yield하는 것이 계속 되고 종료되면 return 하는 것을 chunk로 누적시켜 return할 것이다. return body # 최종적으로 EVENT_READ등을 통한 콜백들이 순차적으로 예약되고 진행되면서 알아서 작동하고, 마지막에 return 될 것이다. 이제 우리의 원래 목적인 async web crwaler를 asyncio를 통해서 구현하는 것으로 돌아간다.","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"python post","slug":"python-post","permalink":"https://codenamenadja.github.io/tags/python-post/"}]},{"title":"비동기 코루틴 번역-3","slug":"python/post/async_coroutine_3","date":"2019-06-19T09:11:02.000Z","updated":"2021-01-25T08:36:06.083Z","comments":true,"path":"2019/06/19/python/post/async_coroutine_3/","link":"","permalink":"https://codenamenadja.github.io/2019/06/19/python/post/async_coroutine_3/","excerpt":"","text":"어떻게_제너레이터가_동작하는가 제너레이터 이전에 어떻게 파이썬 루틴들이 동작하는지 알아봐야할 것이다. 기본적으로 파이썬 함수가 서브루틴을 부를때, 서브루틴이 종료될 때까지, 컨트롤을 얻기 때문에, 블로킹이 일어난다. 12345def foo(): bar()def bar(): pass 파이썬의 인터프리터는 Cpython을 기반으로 작동한다. 파이썬 함수를 실행하는 C function은 달콤하게도, **PyEval_EvalFrameEx**라고 불려진다. **PyEval_EvalFrameEx**는 파이썬 스텍프레임 오브젝트를 수용해서, 프레임의 Context안에서, 파이썬의 바이트코드를 평가한다. 여기에 foo에 대한 바이트코드가 있다. 12import disdis.dis(foo) 2 0 LOAD_GLOBAL 0 (bar) 3 CALL_FUNCTION 0 (0 positional, 0 keywordpair) 6 POP_TOP 7 LOAD_CONST 0 (None) 10 RETURN_VALUE dis - Disassembler for Python bytecode 해당 모듈은 CPython bytecode를 disassembling 함으로써 분석한다. CPython 실행 디테일: 바이트코드는 CPython의 실행부에 대한 명세이다. 2 0 load_global 0(len) 맨 앞에 있는 것은 라인넘버이다. foo Function은 bar을 그의 스텍에 로드하고, 부르고 있다. 그리고 foo의 return value(기본적으로 None)을 자신의 스텍에서 pop한다. None을 자신의 스텍에 로드하고, 마찬가지로 None을 리턴한다. Py_EvalFrameEx가 CALL_FUNCTION 바이트코드를 만났을때, 그것으로 새로운 파이썬 스텍프레임을 생성하고 recurse 한다. 파이썬의 스텍프레임이 기본적으로 힙에 존재한다는 사실을 아는 것은 매우 중요하다! 기본 스텍이 조차도 OS의 메모리의 스텍세그먼트가 아니라 OS_heap &gt; private heap에 명시적으로 stack-segment를 이루고 있다! 그렇다면 명시적인 스텍세그먼트에 프레임으로 저장한다고 하면, 임의로 컨트롤 하기가 매우 쉬워지는 것이다. 파이썬 인터프리터는 평범한 C프로그램이다. 그래서 스텍프레임 또한 일반적인 스텍프레임이다. 그러나 파이썬 실행되는 파이썬의 스텍프레임은 힙에 있다. 다른 놀라움 사이에서, 그것은 파이썬 스텍프레임이 그것을 존재하게한 function call stackframe보다 오래 살 수 있다는 것을 의미한다. 이것을 상호작용적 측면에서 보기 위해서, 현재의 프레임을 bar의 내부에서 저장해라 12345678910111213141516171819import inspectframe = Nonedef foo(): bar()def bar(): global frame frame = inspect.currentframe()foo() # this frame was executing the code for 'bar'.frame.f_code.co_name# 'bar'caller_frame = frame.f_back # its back pointer refers to the frame for 'foo'.caller_frame.f_code.co_name#'foo' foo()를 실행하면 순차적으로 벌어지는 일 PyEval_EvalFrameEx in foo func Executes evaluate self.f_code made CodeByteCode of foo recursion occurs PyEval_EvalFrameEx in bar func Executes… eval self.f_code PyObject 설명 PyEval_FrameObject: C구조의 객체, frameObject를 설명하기 위한 객체이다. 이 타입의 필드들은 언제든 바뀔 수 있도록 설계되어 있다. PyEval_EvalFrame(PyFrameObject *f): new Reference, 실행부에 있는 프레임을 평가한다. 후위에 대한 호환성을 유지하기 위해 PyEval_EvalFrameEx의 축소화된 인터페이스를 갖고 있다. PyEval_EvalFrameEx(PyFrameObject *f,int throwflag): 이것은 어떠한 가공도 덧붙여 지지 않은 파이썬 해석부(interpretation)의 메인 함수이다. 이것은 말 그대로 2000lines만큼 길고, 대상 바이트코드와 실행-Calls를 필요한 만큼 해석하며 코드오브젝트와 연관된 execution-프레임인 f 는 실행된다. 이제 놀라운 효과를 위해서 같은 building blocks(codeObj, stackObj)를 사용하는 무대는 파이썬 제너레이터를 위해 준비가 되었다. 이것이 제너레이터 함수이다. 12345678910def gen_fn(): result = yield 1 print('result of yield: &#123;&#125;'.format(result)) result2 = yield 2 print('result of 2nd yield: &#123;&#125;'.format(result2)) return 'done'generator_bit = 1 &lt;&lt; 5 # 00100000bool(gen_fn.__code__.co_flags &amp; generator_bit)True 파이썬이 gen_fn을 바이트코드로 변환할 때, 그것은 yield문을 발견하고 gen_fn이 일반적인 함수가 아닌 제너레이터 함수라는 것을 안다. 해당 frameobject에 flag를 세워서 이 사실을 기억하도록 한다. 초기화 이후 next()같은 것으로 첫번 째 콜을 할 때, 플래그를 인식해서, 힙의 어딘가에 기존 서브루틴이 맺히는 곳이 아닌 다른 힙의 어느 공간에 생성. 12345gen = gen_fn()\"\"\"type(gen)&gt;&gt;&gt; &lt;class 'generator'&gt;\"\"\" 네가 제너레이터 함수를 호출할 때, 파이썬은 그때 그것의 제너레이터 플레그를 알아볼 수 있다. 그리고 실제로 함수를 실행하는게 아니라 대신에, 제너레이터를 생성한다. 1234\"\"\"gen.gi_code.co_name&gt;&gt;&gt; 'gen_fn'\"\"\" gen_fn를 호출함으로써 생기는 모든 제너레이터 네임은 이 모두 이 코드를 가르킨다. 하지만 이들 모두는, 각각 자신의 스텍프레임을 갖고 있다. 이 스텍프레임은 파이썬의 스텍프레임에 쌓이는게 아니라, 독특하게도, 파이썬의 힙에서 언젠간 사용될 것을 기다리며 저장된다. object Attr value Object PyGenObject.gi_frame :PyFrameObject PyGenObject.gi_code :PyCodeObject PyGenObj madeOf, object attr PyFrameObject f_lasti: 마지막 인스트럭션 포인터(서브루틴과 구별) f_locals: 스텍-지역변수 PyCodeObject gen_fn’s bytecode 프레임에는 Last instruction이라는 포인터가 있고, 마지막에 실행된 지침을 보관하고 있다. 최초 초기화 시에 last instruction poiter은 -1이다. 이는 제너레이터가 시작되지 않았음을 의미한다. 12gen.gi_frame.f_lasti&gt;&gt;&gt; -1 우리가 send를 호출 할떄, 제너레이터는 그의 첫번째 yield를 만나고 멈춘다. send에 대한 리턴 값은 1, 왜냐하면 그것이 gen이 yield호출에게 전달한 것이기 때문이다. 12gen.send(None)&gt;&gt;&gt; 1 이제 제너레이터의 인스트럭션 포인터는 시작에서부터 3 bytecodes 만큼 떨어졌다. 56바이트로 컴파일된 바이트코드 실행부 중에서 그만큼 진행한 것이다. 1234gen.gi_frame.f_lasti&gt;&gt;&gt; 3len(gen.gi_code.co_code)&gt;&gt;&gt; 56 제너레이터는 언제든 계속 될 수 있다. 왜냐하면 기존의 서브루틴들이 맺히는 부분에 존재하지 않기 때문이다. 이것이 콜트리(Call hierarchy)에 있는 위치는 고정된게 아니다. 그리고 이것은 FIFO 의 실행 순서를 따르지 않아도 된다. 이건 자유롭고, 구름같이 떠다니는 정도의 자유이다. 우리는 제너레이터에게 'hello’라고 value를 send할 수 있다. 그리고 그것은 yield표현에 대한 결과가 될 것이다. 그리고 제너레이터는 2를 yield할 때까지 수행된다. 123456res = gen.send(\"hello\")res# 2gen.gi_frame.f_locals# &#123;\"result\": \"hello\"&#125; 지금은 gen이라는 네임에 묶인 하나의 제너레이터 스텍이지만, 다른 것으로 또 할당을 하면, 그것의 고유의 스텍프레임과, 지역변수를 가지고 있을 것이다. 우리가 다시 send를 호출하면, 제너레이터 코루틴은 그것의 두번째 yield를 루틴 컨텍스트로 돌려줄 것이고 그리고 그것을 StopIteration 예외를 발생시켜서, 그것이 종료되었음을 알린다. 1234567gen.send(\"goodbye\")# generator마지막에 리턴하는 것이 'done' 문자열.\"\"\"Traceback (most recent call last) :...StopIteration: done\"\"\"","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"python post","slug":"python-post","permalink":"https://codenamenadja.github.io/tags/python-post/"}]},{"title":"파이썬의 가상 메모리","slug":"python/post/virtual_memory_with_malloc","date":"2019-06-19T09:05:23.000Z","updated":"2021-01-25T08:36:06.087Z","comments":true,"path":"2019/06/19/python/post/virtual_memory_with_malloc/","link":"","permalink":"https://codenamenadja.github.io/2019/06/19/python/post/virtual_memory_with_malloc/","excerpt":"","text":"Heap을 구현한(및 연결점인) C의 malloc()이 느리기 때문에, HEAP이 느리다 Heap 의 자료구조는 헤더를 통해 다음 bytesArray를 연결하는 double Link list 처음에는 단순하게 스텍처럼 쌓이다가 일부 메모리가 해제되어, 새로운 크기의 메모리를 할당하려고 할때, 데이터가 사라진 남아있는 헤더들이 유지하고 있는 사이즈와 맞는 요소를 찾아 linked List를 순회하며 찾는다. 이로 인해 생기는 현상을, Memory Fragmentation(메모리 단편화) 라고 한다. Heap에 배치된 총 메모리에 비해 남은 메모리가 있어도, 맞는 크기가 없으면, Scaleup 하는 것 외에는 방법이 없다. 파이썬등 인터프리터언어들의 엔진은 라인별로 실행하는 특성상 대부분 동적할당 혹은 부분적으로에 의해 일이 잦은데, 프로세스를 실행하는 때에, Heap의 정크메모리(많은 양의)를 할당 받아, 그 커다란 메모리를 엔진이 직접 관리하면서, CPU-Memory-HeapSegement python requires Junk Heap Memeory space. python call it Private Heap. python.exe’s private heap Data Segment Code Segment Stack Segment Heap Segment 역할을 나눠 직접 모니터링 하면서 관리한다. 그래서 c의 malloc()을 이어받아, 기능적으로 힙을 관리하는 자체 메서드를 생성했다. virtual memory OS: when Process starts? give mapped page-Table to Process sytex of pageElem ({page#num:[] , frame#num:[], valid bit:[]}) 4gb정도의 크기를 가상 메모리로 할당하고, 실제로 프로레스를 시작하는데 필요한 만큼은 바로 Page Fault를 일으킨 후 실제 메모리와 매칭된다. pageFault(validbit is 0) -&gt; map to real ram addr -&gt; valid bit up to 1 preparing: 첫 주소도 알려주고 4기가 라고 알려주지만, 실제 바로 필요한 메모리 만큼은 실제로 연결 해준다. pc가 다음 명령을 검토 -&gt; 페이지 테이블 첫 주소로 이동 -&gt; 매핑된 프레임주소 -&gt; 실제 메모리에 저장 -&gt; vaild bit = 1(실제 할당됨) 매핑되는 프레임주소(MMU 하드웨어로서 페이지를 프레임으로 변환해줌) 관건은 코드세그먼트에서 알고있는 메모리주소로 이동하면 그 메모리 주소는 페이지 테이블에 실제 메모리 주소가 매핑되어 있다는 것. Logical메모리인 페이지 테이블에 Validbit가 -이면 실제 매핑된 메모리주소에 없다는 것임. 최초 프로그램 실행(가짜메모리인 테이블만 있다) -&gt; valid bit 가 0이다.(읽은 적이 없으니까 최초시작이니까) -&gt; 하드디스크 -&gt; 실제주소로 가져오고 저장 -&gt; 페이지테이블로 돌아온다 -&gt; Valid bit =1 .-&gt; 프로그램 카운터가 해당 실제 지침을 가지고 돌아… -&gt; 지침레지스터에 전달 프로그램에서 이전에 실행했던 컴포넌트는 이미 메모리에 있어서 최초 프로그램내의 컴포넌트를 실행할 때보다 빠르게 반응하는 것에 대한 실재.","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"python post","slug":"python-post","permalink":"https://codenamenadja.github.io/tags/python-post/"}]},{"title":"Inspect 모듈 파헤치기","slug":"python/module/inspect","date":"2019-06-19T07:37:06.000Z","updated":"2021-01-25T08:36:06.083Z","comments":true,"path":"2019/06/19/python/module/inspect/","link":"","permalink":"https://codenamenadja.github.io/2019/06/19/python/module/inspect/","excerpt":"","text":"Index 공식문서 정의 [메서드 리스트][i1-1] Type class별 inpect지원 속성, 메서드 Traceback module Frame module Frame object획득하기 Code module dis모듈로 코드 해석 Function module Generator module Coroutine module 유용한 활용 지원 타입의 source가져오기 마치며 공식문서_정의 인스펙트 모듈은 유효한 모듈, 클래스, 메서드, 함수, 트레이스백, 프레임 객체, 코드객체에 대해 정보를 가질 수 있는 유용한 함수들을 제공한다. 4가지 종류의 주요 서비스 타입체킹 소스코드 획득 클래스와 함수 측정 인터프리터 스텍 확인 class_Type에_대한_Inspect_백업 Traceback_module Type Attribute Description traceback tb_frame 해당 레벨의 프레임객체 tb_lasti 바이트코드에서 마지막 시도된 지침의 인덱스 tb_lineno 소스코드상 현재 라인 넘버 tb_next 다음 내부의 트레이스백 객체 Frame_module Type Attribute Description frame f_back 다음 외부 프레임 객체(이 프레임의 호출자) f_builtins 해당 프레임에서 보여진 builtin 네임스페이스 f_code 이 프레임에서 실행된 코드객체 f_globals 이 프레임에서 보여진 Global 네임스페이스 f_lasti 바이트코드에서 마지막으로 시도된 지침의 인덱스 f_lineno 소스코드상 현재 라인 넘버 f_locals 현재 프레임에서 보여진 Local 네임스페이스 f_trace 이 프레임의 함수를 추적, 없으면 None Frame_object획득하기 123456789101112def handle_stackframe_without_leak(): frame_start = inspect.currentframe() do_somethong = lambda : pass do_something() frame_end = inspect.currentframe() try: print(frame_start.f_locals) print(frame_end.f_locals) print(frame_start is frame_end) # True # do something with the frame finally: del frame_start, frame_end Code_module Type Attribute Description code co_argcount *args의 수량 co_code 날것으로 컴파일된 바이트코드 co_cellvars 튜플로 된 셀 객체의 이름들 co_consts 바이트코드에서 사용된 상수로 구성된 튜플 co_filename 해당 코드 객체가 생성된 파일명 co_firstlineno 소스코드에서 첫번쨰 라인의 수 co_flags 생략(봐도 모르겠다.) co_inotab 생략(불필요) co_freevars free Variables의 이름들로 구성된 튜플 co_kwonlyargcount 키워드기반 매개변수의 수 co_name 해당 코드객체가 정의한 것의 이름 co_names 지역변수의 이름들로 구성된 튜플 co_nlocals 지역 변수의 이름들의 숫자 co_stacksize 가상머신 스텍 공간이 요구됨 co_varnames 매개변수와 지역변수의 이름들로 구성된 튜플 dis모듈을_사용하여_객체_분석 123456789101112131415161718192021222324252627282930import disimport pprintdef sample_func(a,b=3,msg=None): \"\"\" &gt;&gt;&gt; sample_func(2,msg=\"expected!\") (6, \"expected! 6\") \"\"\" return (a*b, f\"&#123;msg&#125; &#123;a*b&#125;\")pprint.pprint((dis.code_info(sample)))\"\"\"( 'Name: sample\\n' 'Filename: &lt;ipython-input...&gt;\\n' 'Argument count: 3\\n' 'nKw-only arguments: 0\\n' 'Number of locals: 3\\n' 'Stack size: 5\\n' 'Flags: OPTIMIZED, NEWLOCALS, NOFREE\\n' 'Contants: \\n' ' 0: \\'\\\\n &gt;&gt;&gt; sample_func(2,msg=\"expected!\")\\\\n (6, \"expected! ' 6\")\\\\n \\'\\n' 1: ' '\\n\" Variable names:\\n' 0: a\\n' 1: b\\n' ' 2: msg'))\"\"\" Function_module Type Attribute Description function doc 문서화 문자 func 매서드의 실행부가 포함된 함수객체 self instance to which method is bound or None Generator_module Type Attribute Description generator gi_frame frame gi_running 제너레이터가 실행 중 인가? gi_code code gi_yieldfrom yield from에서부터 생산되는 객체, or None Coroutine_module Type Attribute Description coroutine cr_await 대기중인 객체, or None cr_frame frame cr_running 코루틴이 진행 중인가? cr_code code cr_origin 코루틴이 생성된 곳, or None index 유용한_활용 지원_타입의_소스받아오기 123456789def sample(): return 'sample!'def getCode(any_obj): \"\"\" &gt;&gt;&gt; inspect.getsource(sample) \"def sample():\\n return 'sample!'\\n\" \"\"\" return inspect.getsource(any_obj) python_code_tracing inspect자원을 래핑하여 활용하는 기법을 보자 123456789101112131415161718import sysdef traceit(frame, event, arg): if event == \"line\": lineno = frame.f_lineno print(\"line\", lineno) return traceitdef main(): print(\"In main\") for i in range(5): print(i, i*3) print(\"Done.\") sys.settrace(traceit)main() sys.settrace()에 설정한 함수에 frame, event, arg를 넣고 해당 함수객체를 반환한다. 예시_크롤링_동작_코드 마치며","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"python module","slug":"python-module","permalink":"https://codenamenadja.github.io/tags/python-module/"}]},{"title":"fabric 기본적인 스크립트 작성하기","slug":"python/library/fabric","date":"2019-06-19T07:17:52.000Z","updated":"2021-01-25T08:36:06.079Z","comments":true,"path":"2019/06/19/python/library/fabric/","link":"","permalink":"https://codenamenadja.github.io/2019/06/19/python/library/fabric/","excerpt":"","text":"Index 공식문서 정의 sample 출력기능 간단한 구현 another 출력기능 나아가서 구현 마치며 sample Header with custom ID 기본 설치 Fabric3 pycrpyto: for secure connection paramiko: for secure connection either. fabfile.py 생성 manage.py와 같은 위치에 생성해 주고, deploy.json은 deploy_tool 폴더 아래로 생성해준다. _로 시작하는 함수는 fab 명령에서 제외 일반적인 함수 명은 fab deploy, test_connection등으로 실행가능 12345678# where directory fabfile.py with manage.pyPROJECT_DIR = os.path.dirname(os.path.abspath(__file__))# one step over fabfile direcotryBASE_DIR = os.path.dirname(PROJECT_DIR)json_config_file = os.path.join(PROJECT_DIR, 'deploy_tools/deploy.json')with open(json_config_file) as f: envs = json.loads(f.read())","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"fabric","slug":"fabric","permalink":"https://codenamenadja.github.io/tags/fabric/"}]},{"title":"장고에서 admin페이지 커스터마이징하기","slug":"python/library/django/admin_customize","date":"2019-06-14T04:41:47.000Z","updated":"2021-01-25T08:36:06.079Z","comments":true,"path":"2019/06/14/python/library/django/admin_customize/","link":"","permalink":"https://codenamenadja.github.io/2019/06/14/python/library/django/admin_customize/","excerpt":"","text":"order.admin수정해서 action에 행동 추가하기 123456789101112131415161718192021222324252627282930313233343536from django.utils import timezonefrom django.http import HttpResponseimport datetimeimport csvdef export_to_csv(modeladmin, request, queryset): # modeladmin.model - 선택된 객체의 모델 정보 # request - request정보 # queryset - 선택된 row들의 쿼리 # 선택된 주문 정보를 CSV파일로 다운로드 하게 만드는 기능 opts = modeladmin.model_meta response =HttpResponse(content_type='text/csv') current_time = timezone.now().strftime('%Y-%m-%d %H:%M:%S') response['Content-Disposition'] = f'attachment;filename=&#123;opts.verbose_name&#125;-&#123;current_time&#125;.csv' writer = csv.writer(response) # 필드 값 작성 fields = [field for field in opts.get_fields() if (not field.many_to_many) and (not field.one_to_many)] writer.writerow([field.verbose_name for field in fields]) # 데이터 작성 for obj in queryset: # 선택된 요소들에 대한 내용을 출력 ordered_items = getattr(obj, 'items').all() values = [getattr(obj, field.name).strftime('%Y-%m-%d') if field.datetime else getattr(obj, field.name) for field in fields] for item in ordered_items: order_detail = [item.product.id, item.price, item.quantity] writer.writerow(values+order_detail)export_to_csv.short_description = 'Order Export to CSV' # 여class OrderOption(admin.ModelAdmin): \"\"\" ... \"\"\" actions = [export_to_csv] Page 추가하기 1234567891011121314from django.utils.safestring import mark_safedef order_detail(obj): href = reverse('order:admin_detail') return mark_safe(f'&lt;a href=\"&#123;href&#125;\"&gt;to Detail&lt;/a&gt;')order_detail.short_description = 'details'def order_pdf(obj): return mark_safe('PDF')order_pdf.short_description = 'to_pdf'OrderOption.list_display += [order_pdf, order_detail] 1234567891011121314# order.views.pyfrom django.shorcuts import get_object_or_404from django.contrib.admin.views.decorators import staff_member_required@staff_member_requireddef admin_order_detail(req, order_id): order = get_object_or_404(Order, id=order_id) return render(req, 'order/admin/order_detail.html', &#123;'order':order&#125;)# order.urls.pyapp_name = 'order'urlpatterns += [ path('admin/detail/&lt;int:order_id&gt;', admin_order_detail, name='order_detail')] 1234567891011121314&#123;%extends 'admin/base_site.html'%&#125;&#123;%block breadcrumbs%&#125;&lt;div class='breadcrumbs'&gt; &lt;a href='&#123;url \"admin:index\"&#125;'&gt;&lt;/a&gt; &lt;a href='&#123;url \"admin:order_order_changelist\"&#125;'&gt;Orders&lt;/a&gt; &lt;a href='&#123;url \"admin:order_order_change\" order.id&#125;'&gt;Order &#123;&#123;order.id&#125;&#125;&lt;/a&gt; Detail &lt;/div&gt;&#123;%endblock%&#125;&#123;%block content%&#125;// order객체 가지고 표시&#123;%endblock%&#125; weasyprint로 PDF출력하기 pip install weasyprint 123456789101112from django.template.loader import render_to_stringfrom django.http import HttpResponseimport weasyprint@staff_member_requireddef admin_order_pdf(req, order_id): order = get_object_or_404(Order, id=order_id) html = render_to_string('order/admin/pdf.html', &#123;'order':order&#125;) response = HttpResponse(content_type='application/pdf') response['Content-Disposition'] = f'filename=invoice_&#123;order.id&#125;.pdf' weasyprint.HTML(string=html).write_pdf(response) return response 다운로드 Disposition = attachment 웹 뷰어로 보기 Disposition = invoice","categories":[],"tags":[{"name":"django","slug":"django","permalink":"https://codenamenadja.github.io/tags/django/"}]},{"title":"pytest 이해와 사용","slug":"tdd/python/pytest","date":"2019-06-12T12:02:11.000Z","updated":"2021-01-25T08:36:06.087Z","comments":true,"path":"2019/06/12/tdd/python/pytest/","link":"","permalink":"https://codenamenadja.github.io/2019/06/12/tdd/python/pytest/","excerpt":"","text":"major deminority of Default TestCase Specific test method names (ie, ‘assertIsNone’, ‘assertDictEqual’… elses) no database access? then may be to use ‘SimpleTestCase’ Test in the same class diff methods uses identical test database in order to properly user setup method. differences between pure django test and pytest default TestCase 1234567891011from django.test import TestCasedef add(a,b): return a+bclass TestAdd(TestCase): def test_add_positive_numbers(self): self.assertEqual(add(2, 2), 4) def test_add_negative_numbers(self): self.assertEqual(add(-2, -2), 4) Pytest 123456789import pytestfrom upper import add@pytest.mark.parametrize('a, b, expected', [(2, 2, 4), (-2, -2, -4)])def test_add(a, b, expected): assert add(a, b) == expected pytest idioms and features 간단한 assert문을 사용한다. pytest는 무엇이 assert되었는지에 따라 적절한 차이를 아웃풋 할것이다. 테스트를 클래스기반으로 그룹 짓지 않고 작성할 수 있다. 재사용 가능한 'fixtures’를 통해서 상태를 셋업 할 수있다. 픽스쳐는 다양한 스코프에서 사용이 가능하다. __Markers__를 통하여 테스트를 카테고리화 하고, 그들의 실행 자원을 변경할 수 있다. 서드파티 라이브러리의, 픽스쳐, 마커, 그리고 테스트 실행 기능성을 위한 플러그인 시스템 어떻게 테스트주도개발을 할 것인가? 고비용의 통합테스트를 조금 작성한다. 많은 분기의 핸들링을 가지고 있는 부분을 결정한다(ie: error handling). 우리는 그들을 다수의 유닛 테스트로 커버할 수 있다. 어플리케이션 단에서 제어할 수 없는 부분을 정의한다(ie: 외부 API) 앱이 모양을 갖추고 나면, 사이에 생략된 테스트들을 작성한다.","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"testing","slug":"testing","permalink":"https://codenamenadja.github.io/tags/testing/"}]},{"title":"boto3로 sqs자원 사용하기","slug":"python/custom_module/using_sqs","date":"2019-05-27T06:39:22.000Z","updated":"2021-01-25T08:36:06.079Z","comments":true,"path":"2019/05/27/python/custom_module/using_sqs/","link":"","permalink":"https://codenamenadja.github.io/2019/05/27/python/custom_module/using_sqs/","excerpt":"","text":"que네임을 기반으로 url받아오기 1234567def helper_get_que(key): sqs = boto3.client('sqs') response = sqs.list_queues() urlLists = response[\"QueueUrls\"] for url in urlLists: if key in url: return url url을 엔트포인트로 메세지 발송 12345678910111213def helper_send_message(url, title, author, body): sqs = boto3.client('sqs') reponse = sqs.send_message( QueueUrl=url, DelaySeconds=10, MessageAttributes=&#123; 'Title': &#123;'DataType': 'String', 'StringValue': title&#125;, 'Author': &#123;'DataType': 'String', 'StringValue': author&#125;, 'WeeksOn': &#123;'DataType': 'Number', 'StringValue': '6'&#125;, &#125;, MessageBody=(f\"&#123;body&#125;\") ) return reponse queue가 존재하지 않을때 만들기 123456789def helper_create_queue(name): sqs = boto3.client('sqs') response = sqs.create_queue( QueueName=f\"&#123;name&#125;\", Attributes=&#123;'DelaySeconds': '60', 'MessageRetentionPeriod': '86400' &#125; ) return response 메세지 받아오기 1234567891011121314151617def helper_get_message(endpoint): sqs = boto3.client('sqs') queue_url = helper_get_que(endpoint) response = sqs.receive_message( QueueUrl=queue_url, AttributeNames=[ 'SentTimestamp' ], MaxNumberOfMessages=1, MessageAttributeNames=[ 'All' ], VisibilityTimeout=0, WaitTimeSeconds=0 ) return response['Messages'][0]","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"custom module","slug":"custom-module","permalink":"https://codenamenadja.github.io/tags/custom-module/"}]},{"title":"ec2에서 nginx uwsgi로 장고앱 세팅","slug":"server/webserver","date":"2019-05-22T01:17:35.000Z","updated":"2021-01-25T08:36:06.087Z","comments":true,"path":"2019/05/22/server/webserver/","link":"","permalink":"https://codenamenadja.github.io/2019/05/22/server/webserver/","excerpt":"","text":"webserver 웹서버가 하는 일은 사용자가 네트워크를 통해 요청을 보내면, 요청에 대한 해석과 응답을 돌려주는 역할. 요청을 해석할때, 어떤 리소스를 찾느냐에 대한 처리를 진행, WAS와 통신하요 사용자 요텅을 해성하는 어플에 대한 호출과 콜백을 처리 APACHE 메인 쓰레드가 있고, 하위에 워커등의 방식으로 서브 프로레스를 만들어서 다량의 요청을 처리 NGINX 이벤트 처리방식, 요청이 들어오면 어플리케이션에 요청처리를 부탁하고 본인은 계속 들어온 요청에 대한 대응을 수행한다. Apache에 비해서 적은 리소스를 사용하고 빠른 처리가 가능 nginx설치 sudo apt install nginx service nginx status or systemctl status nginx ec2-52-78-46-217.ap-northeast-2.compute.amazonaws.com:80 웹 브라우저 동작 테스트 설정파일로 프로그램 설정 sudo cp /etc/nginx/site-available/default /etc/nginx/sites-available vim /etc/nginx/site-available/staticweb 123456789server 80;listen [::]:80;root /var/www/staticweb;server_name ec2-52-78-46-217.ap-northeast-2.compute.amazonaws.com; #EC2 인스턴스의 퍼블릭 인스턴스 DNSlocation / &#123;&#125; 설정파일을 제대로 했다면, Nginx를 재시작 해여한다. 설정파일이 올바른지 확인하는 법 12$:sudo nginx -ttest is successful 해당 설정파일을 Nginx로 연결시켜야 한다. sudo ln -s /etc/nginx/sites-available/staticweb /etc/nginx/sites-enabled/ test sudo vim etc/nginx/nginx.conf 124 server_name_hash_size 128 고정 IP가 필요한 이유는 도메인 연결을 하기 위해서 route53에서 ip주소에 따른 주소 생성하고 IP를 바인딩 하는 순간 기존 퍼블릭 도메인네임 으로 접속하는 것은 끝남 ssh -i ~/.ssh/aws-ec2-junehan.pen ubuntu@52.78.68.43 서비스 한번 리스타트 해주고 접속이 잘 된다 staticweb의 servername을 route53에 매칭한 주소로 바꿔준다. 백업, 재활용을 위한 이미지 생성 인스턴스 탭에서 이미지 생성 images AMIs에 있음 웹서버가 웹 어플을 관리하기 위한 계정 생성 sudo useradd -g www-data -b /home -m -s /bin/bash django www-data: nginx설치시 생성된 그룹 -b : 루트 디렉터리(유저의 홈폴터 -m : 자신의 계정 명의 -s : 로그인 기본쉘 sudo mkdir -p /var/www/django sudo chown django:www-data /var/www/django sudo usermod -a -G www-data ubuntu 루트권한 유저 그룹 변경 a :add G :group sudo chmod g+w /var/www/django python manage.py runserver 0:8000 제대로 출력되지 않으면 ALLOWED_HOST 설정 += ‘*’ 80번 포트에 nginx가 req를 받으면, uwsgi를 통해서 8000번 포트로 인계하라는 명령을? pip install uwsgi nginx랑 uwsgi 붙어 있는가? uwsgi랑 django랑 잘 붙어 있는가? uwsgi --http :8000 --home /var/www/django/venv/ --chdir /var/www/django/ --module config.uwsgi uwsgi는 run을 통해 소켓 파일을 생성한다. mkdir run logs chown django: uwsgi의 실행 프로세스 자동화, 컴퓨터 재시작시 자동 시작 프로세스에 등록하기, 그 시작에 해당 자동화 설정을 포함하기 mkdir uswgi.ini vacuum = true : 원래 만들어져 있던 파일을 지운다. uswgi.ini외에도 [Service]의 ExecStart에서는 폴더를 기준으로 실행하기 때문에, 설정 파일이 복수로 있다면, 다수의 장고앱을 실행한다. staticweb 일때만 root가 필요하다i. 흐름 정리 nginx&lt;-&gt;uwsgi site-available/django restart nginx uWSGI uwsgi.service systemctl deamon-reload, restart uwsgi uWSGI &lt;-&gt; djnago uwsgi.ini or django code diff? uwsgi restart step 24. 장고 소스코드가 있는 폴더의 소유자와 권한을 변경 sudo chown -R django:www-data /var/www/django sudo chmod -R g+w /var/www/django 위지윅 에디터 자바스크립트 라이브러리를 가지고 설정","categories":[],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://codenamenadja.github.io/tags/nginx/"},{"name":"django","slug":"django","permalink":"https://codenamenadja.github.io/tags/django/"}]},{"title":"server and client","slug":"python/library/django/server_basic","date":"2019-05-20T01:19:37.000Z","updated":"2021-01-25T08:36:06.079Z","comments":true,"path":"2019/05/20/python/library/django/server_basic/","link":"","permalink":"https://codenamenadja.github.io/2019/05/20/python/library/django/server_basic/","excerpt":"","text":"client Request 브라우저 동작을 통해 보내는 요청 링크를 클릭해서 해당 주소로 이동하면서 생기는 요청 주소를 브라우저 주소창에 입력해서 접속 폼의 제출 버튼을 눌러서 접속 page reload first -&gt; also req 브라우저의 요청을 흉내내서 보내는 요청 Ajax(Asyncronous javascript and xml request) 123456$.ajax(&#123; url: \"somepage.com/...\", &#125; ).done((data)=&gt;&#123; &#125;) Request header req target req method Put : 전체 수정 Delete : 삭제 요청 Fetch : 일부 혹은 전체 데이터의 수정 HEAD : GET으로 받는 정보 중에 헤더 정보만 req OPTIONS : 해당 URL에 대해 허용하는 메서드 확인 req http version header HOST User-Agent Accept connection content-type Body Django Settings.py middleware URLCONF동작 : url을 파싱하고 동작시켜야 할 뷰 연결 123456789path('admin/', func),path('update/&lt;int:pk&gt;/',func), # int:pk : namedGroupnamedGroup은 뷰에 매개변수로 전달def update(req, pk): passclass DUpdate(UpdateView): def get(self, req, *args, **kwargs): pk = kwargs['pk'] view with model: 해당 뷰에 유효한, 모델에 대해서 ORM을 이용해 추상화하고 데이터베이스 종류에 맞춰서 쿼리문 전달 ORM을 사용하면 데이터베이스 서비스 종속성이 없다.(SQL Query -&gt; mysql, ms-sql, postgresql …) QuerySet: django에서 ORM을 다루기 위한 스킬 with Template : bootstrap settings.py ROOT_URLCONF = config.urls WSGI_APPLICATION = ‘config.wsgi.application’ 12os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings')application = get_wsgi_application() Infra Tier 몇 개층으로 서버 인프라를 구성 했느냐? 1: 로컬 프로그램 2: 클라이언트 - 데이터 3: 클라이언트 - 비지니스 로직 - 데이터 (웹앱까지 유효하게 구성했을때) 스케일링 요청이나 수행의 분담을 분할 Up-scaling : Out-scaling : 데이터의 동기화 문제 발생(racecondition) 동기화 문제에 대한 해결 데이터베이스 중앙화 파일 서버 중앙화 스케일링은 자동화도 가능하다. 하지만 반 자동화가 일반적 WarmUP : 사용자가 늘거라고 예상되는 시점이 있을 때, 미리 스케일링 GIB : 20GB라고 하면 실제 OS에서 운영 가능한 양은 그것보다 적다. GIB는 그것보다 크게 잡아서 실제 20GB를 보장하는 것을 얘기한다. RDS region : 실제 서비스하는 지역 RDS Available zone : 2대 정도는 둔다.(전환 가능한) 성능을 위해서 READONLYDB(Read replica)를 둔다. 멀티 리전 : 실제 다른 국가에 인프라 구축, CDN database RDS instance 생성 settings.DATABASES 규정 123456789101112# backends는 장고와 데이터베이스 어뎁터 사이를 연결# psycopg2-binary: 어댑터 - 드라이버 -&gt; 실제로 데이터베이스에 접속해서 명령을 수행DATABASES = &#123; 'default' = &#123; 'ENGINE' : 'djnago.db.backends.postgresql_psycopg2', 'NAME' : 'oh_mon_lesiles_db', 'USER' : 'junehan', 'PASSWORD' : '0000', 'HOST' : 'wps10-dbsetting.cuddiucrfzmn.ap-northeast-2.rds.amazonaws.com', 'POST' : '5432', &#125;&#125; migrate python manage.py makemigrations : 변경사항이 있을때, python manage.py migrate : 총 제작한 쿼리로 실제 DB에 반영 python manage.py createsuperuser : 장고에서 DB에 대한 최종관리자 권한 생성 스케일링이 필요한 시점을 대비한 초기세팅 웹 서비스의 입출력 병목이 발생 했을때, 스케일링. 스케일링은 동기화 문제를 발생시킬 수 있기 때문에, DB의 경우 특이한 구조로 스케일링 진행 (Master/Slave방식) master DB : 기존에 저장만 가능하고, 저장되면 Read서버로 변경내용을 전달 Readonly DB : 읽기만 가능. Master/slave 구조를 구현하기 위해 DB Routing을 설정한다. 특정 모델이 별도 DB를 이용하는 경우를 설정한다. DB Scaling AWS RDS &gt; DATABSE &gt; ACTION &gt; 읽기전용DB생성 MASTER에 대한 아웃 스케일링이 필요할 때, AURORADB Storage 개요 파일 중앙화: 파일의 동기화 문제를 해결 스토리지 용량 무제한: 미디어 중심 서비스의 경우 1파일당 용량 2TB: 대형 미디어 파일의 안정성: 파일이 깨지지 않음을 보장하는 편 django 3rd-party라이브러리와 호환성이 높다 Static and media bucket media url을 직접 사용하여 유저가 업로드 버킷을 생성(리소스 인프라)을 완료하였다면, AWS API에 맞춰야 함 IAM S3에 맞춰서 생성 settings.py 12345AWS_S3_OBJECT_PARAMETERS = &#123; # 브라우저가 해당 파일에 접속 했을 때 나타나는 파라미터 값 'CacheControl' : '' # 자주 바꾸는게 아니라면, 한번 다운 받으면 계속 유지되도록 &#125;AWS_DEFAULT_ACL = 'public-read'AWS_S3_SECURE_URLS = False 12345# cofnig.s3media.pyclass MedaiStorage(S3Boto3Storage): location = '' bucket_name = 'media.sample.actingprogrammer.io' custom_domain = '' CDN을 사용하는 이유, https에서 문제가…","categories":[],"tags":[{"name":"django","slug":"django","permalink":"https://codenamenadja.github.io/tags/django/"}]},{"title":"connect s3, RDS in django","slug":"python/library/django/s3_in_django","date":"2019-05-15T07:47:58.000Z","updated":"2021-01-25T08:36:06.079Z","comments":true,"path":"2019/05/15/python/library/django/s3_in_django/","link":"","permalink":"https://codenamenadja.github.io/2019/05/15/python/library/django/s3_in_django/","excerpt":"","text":"awscli설정 awscli 설치하기 1pip install awscli awscli 설정하기 1awscli --configure ~/.aws/credentials 에 자동으로 키값이 들어간 파일을 만들어 준다. 123456[default]aws_access_key_id = *************aws_secret_access_key = *****************[guest]aws_access_key_id = ****************aws_secret_access_key = ************** boto3와 awscli를 이용한, 기본 접근키 비밀 키 설정 1234# settings.pyimport botocore.sessionAWS_ACCESS_KEY_ID = botocore.session.get_credentials().access_keyAWS_SECRET_ACCESS_KEY = botocore.session.get_credentials().secret_key django-storages의 폴더 자동 업로딩을 위한 설정 최소 업로드: ACCESS_KEY, SECRET_KEY, 버킷이름, Boto3 STATICFILES_DIRS: Folder dirs to upload as static 12345#settings.pySTATICFILES_DIRS = [ os.path.join(BASE_DIR, 'static'),]#한 폴더에 static을 모은다 STATICFILES_STORAGE: meaning S3 resouce on AWS_ACCESS_KEY_ID 12STATICFILES_STORAGE = \"storages.backends.s3boto3.S3Boto3Storage\"\"\"\"to allow collectstatic to automatically put file to bucket\"\"\" AWS_STORAGE_BUCKET_NAME: Pure S3 bucket name 123AWS_STORAGE_BUCKET_NAME = 'static.oh-mon-lesiles.shop'# s3 bucket name (Usually 'static' or 'sources',etc..)# but used DNS mapped as bucket name in this case STATIC_URL: used in static 'images/sample.jpg' 1STATIC_URL = f'https://&#123;AWS_STORAGE_BUCKET_NAME&#125;/ OPTIONALS 12345 AWS_LOCATION = '' # prefix prepended to all uploads AWS_S3_OBJECT_PARAMETERS = &#123;'CacheControl': 'max-age=86400', &#125; STATICFILES_STORAGE가 부트스트래핑 되는 자원에 대한 고유한 저장소라면 DEFAULT_FILE_STORAGE는 모델에서 저장되는 파일에 대한 기본적인 저장소로 연결되도록 django-storages가 처리해준다. DEFAULT_FILE_STORAGE: settings.py의 StaticStorage를 오버라이딩한 기본 모델의 파일 저장소 세팅 1234567from storages.backends.s3boto3 import S3Boto3Storageclass MediaStorage(S3Boto3Storage): location = '' bucket_name = 'media.oh-mon-lesiles.shop' file_overwrite = False custom_domain = f'http://&#123;bucket_name&#125;/' STATIC_URL이 settings.py 에서 Static_storage의 엔트포인트라면, custom_domain은 static을 제외한 것들이 statuc_url대신에 가질 수 있는 엔드포인트","categories":[],"tags":[{"name":"django","slug":"django","permalink":"https://codenamenadja.github.io/tags/django/"}]},{"title":"how to use pipenv 100%","slug":"python/library/pipenv","date":"2019-05-14T17:19:43.000Z","updated":"2021-01-25T08:36:06.079Z","comments":true,"path":"2019/05/15/python/library/pipenv/","link":"","permalink":"https://codenamenadja.github.io/2019/05/15/python/library/pipenv/","excerpt":"","text":"Index 개요 목표 2 3 4 마치며 개요 Pipenv는 모든 최고의 패키징(bundler, composer, npm, cargo, yarn)을 파이썬으로 가져오기 위한 툴입니다. 이것은 가상환경을 자동으로 생성하고 관리합니다. 또한 Pipfile에서 패키지는 추가하고 삭제함으로써 자동으로 가상환경을 생성하고 관리합니다. Pipenv가 해결하는 복합적인 면 문제는 이러합니다. 당신은 더 이상 pip와 virtualenv 개별적으로 사용하지 않아도 된다. reqirements.txt를 관리하는 것은 문제를 발생시킬 여지가 있습니다. 그래서 우리는 Pipfile, Pipfile.lock을 사용하여 최근에 테스트된 조합에서부터 추상적인 의존성 선언을 분리합니다. 해쉬들은 보안상 어디에든 쓰입니다. 자동적으로 취약접을 드러냅니다. 업데이트가 된 요소에 대해서 강하게 최근 버전을 권유함으로써 위험성을 최소화합니다. pipenv graph를 통해 의존성에 대한 통찰을 드립니다. .env파일들을 불러옴으로써 개발의 워크플로우를 연결해줍니다. Basic_concepts 가상환경이 자동으로 생성됩니다. 어떤 파라메터도 install 명령어에 전달되지 않아도, 명시된 [packages]는 설치될 것입니다. python3 가상환경을 초기화 하려면 pipenv --three 목표 Pipfile과 Pipfile.lock을 이해한다. dev-dependancy와 그렇지 않은 것을 구분한다. 자유롭게 다룬다. Pipfile Example Pipfile 12345678910111213[[source]]url = &quot;https://pypi.python.org/simple&quot;verify_ssl = truename = &quot;pypi&quot;[packages]requests = &quot;*&quot;[dev-packages]pytest = &quot;*&quot;[requires]python_version = &apos;&apos; 일반적인 권유, 버전관리 일반적으로 Pipfile, Pipfile.lock은 버전컨트롤에 포함하십시오. 만약 복수의 파이썬 버전이 타겟이라면 Pipfile.lock은 포함하지 마십시오. 당신이 요구하는 파이썬 버전을 Pipfile의 [requires] 섹션에 명시하십시오. 이상적으로라면 하나의 파이썬 버전만 가져야합니다. pipenv install은 pip install과 완벽히 호환됩니다. 기본_워크플로우 당신의 프로젝트 저장소를 생성하고 클론하십시오: cd my project Pipfile로부터 설치한다면: pipenv install 새로운 패키지를 추가한다면: pipenv install &lt;package&gt; Pipfile이 초기화 되었다면: pipenv shell &amp;&amp; python --version 이것은 새로운 shell subprocess를 생성합니다. exit를 통해 deactivate할 수 있습니다. 업데이트_워크플로우 무엇이 변했는지 확인할 때: pipenv update --outdated 패키지를 업그레이드할 때 2가지 옵션: 모든 것을 업그레이드: pipenv update 하나씩 업그레이드: pipenv update &lt;pkg&gt;i requirements.txt를_통해 requirements.txt파일이 존재할 때 pipenv install을 실행하면, pipenv는 자동적으로 파일의 내용을 가져와 Pipfile을 생성해 줄 것입니다. 아래처럼 명시적으로 처리하는 것도 가능합니다. pipenv install -r path/to/requirements.txt 가상환경관리 Pipenv환경을 관리하기 위한 3가지 주요 커맨드가 있습니다. pipenv install, pipenv uninstall, pipenv lock pipenv_install pipenv install [package names] options --two --three --python [version number] --dev: Pipfile에 명시된 develop와 default모두 설치합니다. --system: 가상환경이 아닌 글로벌 pip커맨드를 사용합니다. --ignore-pipfile: Pipfile을 무시하고 Pipfile.lock으로 설치합니다. --skip-lock: Pipfile.lock을 무시하고 Pipfile로 설치합니다. Pipfile에 생기는 변화가 Pipfile.lock에 반영되지 않습니다. install via vcs pipenv install -e git+https://github.com/requests/requests.git@v2.20.1#egg=requests pipenv_uninstall pipenv uninstall은 pipenv install의 모든 커맨드를 지원합니다. addtional options --all: 가상환경의 모든 것을 지우지만 Pipfile을 남깁니다. --all-dev: Pipfile까지 삭제합니다. Advanced_usage 패키지 유효성 이슈 검사 1234$pipenv check&lt;!-- vulnerable issue로 버전업데이트가 필요하면 Pipfile에 버전명을 수정 --&gt;$pipenv install$pipenv check lock이 잘 안된다? black을 설치할때, lock파일 해시 생성하는 곳에서 문제가 생기는데 해결법은 2가지가 있다. pipenv lock --pre: Pre release버전에 대해서 lock을 처리해준다 pipenv install --dev --skip-lock: 만약 마치며 간단한 시행착오를 통해 최종적으로 이렇게 다루면 Dev depency와 분리해서 사용하기 좋다. 일반 의존모듈은 pre release가 아니어야한다. dev모듈은 lock을 하지 말자 배포 환경에서는, pipfile을 가지고 설치하되 dev는 제외하고 설치한다. 1234pipenv --three# 초기화pipenv install --dev --skip-lock pylint selenium black fabric mypy pytestpipenv install","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://codenamenadja.github.io/tags/python/"},{"name":"pipenv","slug":"pipenv","permalink":"https://codenamenadja.github.io/tags/pipenv/"}]},{"title":"postgresql 접속하기","slug":"sql/postgresql_basic","date":"2019-05-14T10:03:54.000Z","updated":"2021-01-25T08:36:06.087Z","comments":true,"path":"2019/05/14/sql/postgresql_basic/","link":"","permalink":"https://codenamenadja.github.io/2019/05/14/sql/postgresql_basic/","excerpt":"","text":"1.초기 실행 명세 지정된 유저로 로그인 12$ psql -h localhost -d testdb -U testuser -Wpostgres=# 최초 슈퍼계정으로 로그인 (비밀번호 미설정?) 1$ sudo -u postgres psql 슈퍼계정 비밀번호 변경 1ALTER USER postgres with encrypted password '******'; 로그인 후 기본 명령어 \\q : 종료 \\du : 유저 목록 확인 \\l : DB확인 \\dt : 테이블 확인 \\c : 접속 정보 확인 1234&lt;!-- 현재 로그인 유저 확인 --&gt;$ SELECT current_user;&lt;!-- 현재 데이터베이스 확인 --&gt;$ SELECT current_database(); postgreql서비스 제어 12345$ service postgreqlUsage: /etc/init.d/postgresql &#123;start|stop|restart|reload|force-reload|status&#125; [verision ..]$ /etc/init.d/postgresql status$ sudo service postgresql [status, restart, stop, start] 2.DB User and DATABASE CREATE and DROP DATABASE 12345678910CREATE DATABASE test;DROP DATABASE test;CREATE TABLE test( id int, name varchar(20));DROP TABLE test; add user and DROP user 1234$ DROP ROLE testuser;DROP ROLE$ CREATE ROLE testuser;CREATE ROLE Change ROLE password 1$ ALTER ROLE guest LOGIN password 0000; give PRIVILEGE 1$ GRANT ALL PRIVILEGES ON dbname TO testuser 3.Connect to aws-rds psql connect through psql client 12345678$ psql--host=oh-my-lesiles-db.cuddiucrfzmn.ap-northeast-2.rds.amazonaws.com--port=5432--username=junehan--password--dbname=oh_my_lesiles_dbPassword for user junehan: connect and log through sqlarchemy 12import logginglogging.getLogger('sqlalchemy.dialects.postgresql').setLevel(logging.INFO) create orm engine 1234engine = create_engine('postgresql+psycopg2://&#123;id&#125;:&#123;password&#125;@&#123;host&#125;/&#123;dbname&#125;',use_native_hstore=False,) psycopg2 psycopg2 DBAPI는 내부적으로 serialization을 HSTORE타입을 사용하는 extension을 포함한다.","categories":[],"tags":[{"name":"sql","slug":"sql","permalink":"https://codenamenadja.github.io/tags/sql/"},{"name":"postgresql","slug":"postgresql","permalink":"https://codenamenadja.github.io/tags/postgresql/"}]}]}